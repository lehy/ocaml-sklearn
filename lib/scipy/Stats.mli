(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module PearsonRConstantInputWarning : sig
type tag = [`PearsonRConstantInputWarning]
type t = [`BaseException | `Object | `PearsonRConstantInputWarning] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_exception : t -> [`BaseException] Obj.t
val create : ?msg:Py.Object.t -> unit -> t
(**
Warning generated by `pearsonr` when an input is constant.
*)

val with_traceback : tb:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Exception.with_traceback(tb) --
set self.__traceback__ to tb and return self.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module PearsonRNearConstantInputWarning : sig
type tag = [`PearsonRNearConstantInputWarning]
type t = [`BaseException | `Object | `PearsonRNearConstantInputWarning] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_exception : t -> [`BaseException] Obj.t
val create : ?msg:Py.Object.t -> unit -> t
(**
Warning generated by `pearsonr` when an input is nearly constant.
*)

val with_traceback : tb:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Exception.with_traceback(tb) --
set self.__traceback__ to tb and return self.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gaussian_kde : sig
type tag = [`Gaussian_kde]
type t = [`Gaussian_kde | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : ?bw_method:[`S of string | `F of float | `I of int | `Bool of bool | `Callable of Py.Object.t] -> ?weights:[>`Ndarray] Np.Obj.t -> dataset:[>`Ndarray] Np.Obj.t -> unit -> t
(**
Representation of a kernel-density estimate using Gaussian kernels.

Kernel density estimation is a way to estimate the probability density
function (PDF) of a random variable in a non-parametric way.
`gaussian_kde` works for both uni-variate and multi-variate data.   It
includes automatic bandwidth determination.  The estimation works best for
a unimodal distribution; bimodal or multi-modal distributions tend to be
oversmoothed.

Parameters
----------
dataset : array_like
    Datapoints to estimate from. In case of univariate data this is a 1-D
    array, otherwise a 2-D array with shape (# of dims, # of data).
bw_method : str, scalar or callable, optional
    The method used to calculate the estimator bandwidth.  This can be
    'scott', 'silverman', a scalar constant or a callable.  If a scalar,
    this will be used directly as `kde.factor`.  If a callable, it should
    take a `gaussian_kde` instance as only parameter and return a scalar.
    If None (default), 'scott' is used.  See Notes for more details.
weights : array_like, optional
    weights of datapoints. This must be the same shape as dataset.
    If None (default), the samples are assumed to be equally weighted

Attributes
----------
dataset : ndarray
    The dataset with which `gaussian_kde` was initialized.
d : int
    Number of dimensions.
n : int
    Number of datapoints.
neff : int
    Effective number of datapoints.

    .. versionadded:: 1.2.0
factor : float
    The bandwidth factor, obtained from `kde.covariance_factor`, with which
    the covariance matrix is multiplied.
covariance : ndarray
    The covariance matrix of `dataset`, scaled by the calculated bandwidth
    (`kde.factor`).
inv_cov : ndarray
    The inverse of `covariance`.

Methods
-------
evaluate
__call__
integrate_gaussian
integrate_box_1d
integrate_box
integrate_kde
pdf
logpdf
resample
set_bandwidth
covariance_factor

Notes
-----
Bandwidth selection strongly influences the estimate obtained from the KDE
(much more so than the actual shape of the kernel).  Bandwidth selection
can be done by a 'rule of thumb', by cross-validation, by 'plug-in
methods' or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`
uses a rule of thumb, the default is Scott's Rule.

Scott's Rule [1]_, implemented as `scotts_factor`, is::

    n**(-1./(d+4)),

with ``n`` the number of data points and ``d`` the number of dimensions.
In the case of unequally weighted points, `scotts_factor` becomes::

    neff**(-1./(d+4)),

with ``neff`` the effective number of datapoints.
Silverman's Rule [2]_, implemented as `silverman_factor`, is::

    (n * (d + 2) / 4.)**(-1. / (d + 4)).

or in the case of unequally weighted points::

    (neff * (d + 2) / 4.)**(-1. / (d + 4)).

Good general descriptions of kernel density estimation can be found in [1]_
and [2]_, the mathematics for this multi-dimensional implementation can be
found in [1]_.

With a set of weighted samples, the effective number of datapoints ``neff``
is defined by::

    neff = sum(weights)^2 / sum(weights^2)

as detailed in [5]_.

References
----------
.. [1] D.W. Scott, 'Multivariate Density Estimation: Theory, Practice, and
       Visualization', John Wiley & Sons, New York, Chicester, 1992.
.. [2] B.W. Silverman, 'Density Estimation for Statistics and Data
       Analysis', Vol. 26, Monographs on Statistics and Applied Probability,
       Chapman and Hall, London, 1986.
.. [3] B.A. Turlach, 'Bandwidth Selection in Kernel Density Estimation: A
       Review', CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.
.. [4] D.M. Bashtannyk and R.J. Hyndman, 'Bandwidth selection for kernel
       conditional density estimation', Computational Statistics & Data
       Analysis, Vol. 36, pp. 279-298, 2001.
.. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.
       Series A (General), 132, 272

Examples
--------
Generate some random two-dimensional data:

>>> from scipy import stats
>>> def measure(n):
...     'Measurement model, return two coupled measurements.'
...     m1 = np.random.normal(size=n)
...     m2 = np.random.normal(scale=0.5, size=n)
...     return m1+m2, m1-m2

>>> m1, m2 = measure(2000)
>>> xmin = m1.min()
>>> xmax = m1.max()
>>> ymin = m2.min()
>>> ymax = m2.max()

Perform a kernel density estimate on the data:

>>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
>>> positions = np.vstack([X.ravel(), Y.ravel()])
>>> values = np.vstack([m1, m2])
>>> kernel = stats.gaussian_kde(values)
>>> Z = np.reshape(kernel(positions).T, X.shape)

Plot the results:

>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots()
>>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,
...           extent=[xmin, xmax, ymin, ymax])
>>> ax.plot(m1, m2, 'k.', markersize=2)
>>> ax.set_xlim([xmin, xmax])
>>> ax.set_ylim([ymin, ymax])
>>> plt.show()
*)

val covariance_factor : [> tag] Obj.t -> Py.Object.t
(**
Computes the coefficient (`kde.factor`) that
multiplies the data covariance matrix to obtain the kernel covariance
matrix. The default is `scotts_factor`.  A subclass can overwrite this
method to provide a different method, or set it through a call to
`kde.set_bandwidth`.
*)

val evaluate : points:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Evaluate the estimated pdf on a set of points.

Parameters
----------
points : (# of dimensions, # of points)-array
    Alternatively, a (# of dimensions,) vector can be passed in and
    treated as a single point.

Returns
-------
values : (# of points,)-array
    The values at each point.

Raises
------
ValueError : if the dimensionality of the input points is different than
             the dimensionality of the KDE.
*)

val integrate_box : ?maxpts:int -> low_bounds:[>`Ndarray] Np.Obj.t -> high_bounds:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Computes the integral of a pdf over a rectangular interval.

Parameters
----------
low_bounds : array_like
    A 1-D array containing the lower bounds of integration.
high_bounds : array_like
    A 1-D array containing the upper bounds of integration.
maxpts : int, optional
    The maximum number of points to use for integration.

Returns
-------
value : scalar
    The result of the integral.
*)

val integrate_box_1d : low:[`F of float | `I of int | `Bool of bool | `S of string] -> high:[`F of float | `I of int | `Bool of bool | `S of string] -> [> tag] Obj.t -> Py.Object.t
(**
Computes the integral of a 1D pdf between two bounds.

Parameters
----------
low : scalar
    Lower bound of integration.
high : scalar
    Upper bound of integration.

Returns
-------
value : scalar
    The result of the integral.

Raises
------
ValueError
    If the KDE is over more than one dimension.
*)

val integrate_gaussian : mean:Py.Object.t -> cov:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Multiply estimated density by a multivariate Gaussian and integrate
over the whole space.

Parameters
----------
mean : aray_like
    A 1-D array, specifying the mean of the Gaussian.
cov : array_like
    A 2-D array, specifying the covariance matrix of the Gaussian.

Returns
-------
result : scalar
    The value of the integral.

Raises
------
ValueError
    If the mean or covariance of the input Gaussian differs from
    the KDE's dimensionality.
*)

val integrate_kde : other:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Computes the integral of the product of this  kernel density estimate
with another.

Parameters
----------
other : gaussian_kde instance
    The other kde.

Returns
-------
value : scalar
    The result of the integral.

Raises
------
ValueError
    If the KDEs have different dimensionality.
*)

val logpdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Evaluate the log of the estimated pdf on a provided set of points.
*)

val pdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Evaluate the estimated pdf on a provided set of points.

Notes
-----
This is an alias for `gaussian_kde.evaluate`.  See the ``evaluate``
docstring for more details.
*)

val resample : ?size:int -> ?seed:[`I of int | `T_np_random_RandomState_ of Py.Object.t] -> [> tag] Obj.t -> Py.Object.t
(**
Randomly sample a dataset from the estimated pdf.

Parameters
----------
size : int, optional
    The number of samples to draw.  If not provided, then the size is
    the same as the effective number of samples in the underlying
    dataset.
seed : None or int or `np.random.RandomState`, optional
    If `seed` is None, random variates are drawn by the RandomState
    singleton used by np.random.
    If `seed` is an int, a new `np.random.RandomState` instance is used,
    seeded with seed.
    If `seed` is already a `np.random.RandomState instance`, then that
    `np.random.RandomState` instance is used.
    Specify `seed` for reproducible drawing of random variates.

Returns
-------
resample : (self.d, `size`) ndarray
    The sampled dataset.
*)

val set_bandwidth : ?bw_method:[`S of string | `F of float | `I of int | `Bool of bool | `Callable of Py.Object.t] -> [> tag] Obj.t -> Py.Object.t
(**
Compute the estimator bandwidth with given method.

The new bandwidth calculated after a call to `set_bandwidth` is used
for subsequent evaluations of the estimated density.

Parameters
----------
bw_method : str, scalar or callable, optional
    The method used to calculate the estimator bandwidth.  This can be
    'scott', 'silverman', a scalar constant or a callable.  If a
    scalar, this will be used directly as `kde.factor`.  If a callable,
    it should take a `gaussian_kde` instance as only parameter and
    return a scalar.  If None (default), nothing happens; the current
    `kde.covariance_factor` method is kept.

Notes
-----
.. versionadded:: 0.11

Examples
--------
>>> import scipy.stats as stats
>>> x1 = np.array([-7, -5, 1, 4, 5.])
>>> kde = stats.gaussian_kde(x1)
>>> xs = np.linspace(-10, 10, num=50)
>>> y1 = kde(xs)
>>> kde.set_bandwidth(bw_method='silverman')
>>> y2 = kde(xs)
>>> kde.set_bandwidth(bw_method=kde.factor / 3.)
>>> y3 = kde(xs)

>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots()
>>> ax.plot(x1, np.full(x1.shape, 1 / (4. * x1.size)), 'bo',
...         label='Data points (rescaled)')
>>> ax.plot(xs, y1, label='Scott (default)')
>>> ax.plot(xs, y2, label='Silverman')
>>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')
>>> ax.legend()
>>> plt.show()
*)

val silverman_factor : [> tag] Obj.t -> float
(**
Compute the Silverman factor.

Returns
-------
s : float
    The silverman factor.
*)


(** Attribute dataset: get value or raise Not_found if None.*)
val dataset : t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t

(** Attribute dataset: get value as an option. *)
val dataset_opt : t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t) option


(** Attribute d: get value or raise Not_found if None.*)
val d : t -> int

(** Attribute d: get value as an option. *)
val d_opt : t -> (int) option


(** Attribute n: get value or raise Not_found if None.*)
val n : t -> int

(** Attribute n: get value as an option. *)
val n_opt : t -> (int) option


(** Attribute neff: get value or raise Not_found if None.*)
val neff : t -> int

(** Attribute neff: get value as an option. *)
val neff_opt : t -> (int) option


(** Attribute factor: get value or raise Not_found if None.*)
val factor : t -> float

(** Attribute factor: get value as an option. *)
val factor_opt : t -> (float) option


(** Attribute covariance: get value or raise Not_found if None.*)
val covariance : t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t

(** Attribute covariance: get value as an option. *)
val covariance_opt : t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t) option


(** Attribute inv_cov: get value or raise Not_found if None.*)
val inv_cov : t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t

(** Attribute inv_cov: get value as an option. *)
val inv_cov_opt : t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t) option


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rv_continuous : sig
type tag = [`Rv_continuous]
type t = [`Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?momtype:int -> ?a:float -> ?b:float -> ?xtol:float -> ?badvalue:float -> ?name:string -> ?longname:string -> ?shapes:string -> ?extradoc:[`Deprecated of Py.Object.t | `S of string] -> ?seed:[`I of int | `T_numpy_random_RandomState_instance of Py.Object.t] -> unit -> t
(**
A generic continuous random variable class meant for subclassing.

`rv_continuous` is a base class to construct specific distribution classes
and instances for continuous random variables. It cannot be used
directly as a distribution.

Parameters
----------
momtype : int, optional
    The type of generic moment calculation to use: 0 for pdf, 1 (default)
    for ppf.
a : float, optional
    Lower bound of the support of the distribution, default is minus
    infinity.
b : float, optional
    Upper bound of the support of the distribution, default is plus
    infinity.
xtol : float, optional
    The tolerance for fixed point calculation for generic ppf.
badvalue : float, optional
    The value in a result arrays that indicates a value that for which
    some argument restriction is violated, default is np.nan.
name : str, optional
    The name of the instance. This string is used to construct the default
    example for distributions.
longname : str, optional
    This string is used as part of the first line of the docstring returned
    when a subclass has no docstring of its own. Note: `longname` exists
    for backwards compatibility, do not use for new subclasses.
shapes : str, optional
    The shape of the distribution. For example ``'m, n'`` for a
    distribution that takes two integers as the two shape arguments for all
    its methods. If not provided, shape parameters will be inferred from
    the signature of the private methods, ``_pdf`` and ``_cdf`` of the
    instance.
extradoc :  str, optional, deprecated
    This string is used as the last part of the docstring returned when a
    subclass has no docstring of its own. Note: `extradoc` exists for
    backwards compatibility, do not use for new subclasses.
seed : None or int or ``numpy.random.RandomState`` instance, optional
    This parameter defines the RandomState object to use for drawing
    random variates.
    If None (or np.random), the global np.random state is used.
    If integer, it is used to seed the local RandomState instance.
    Default is None.

Methods
-------
rvs
pdf
logpdf
cdf
logcdf
sf
logsf
ppf
isf
moment
stats
entropy
expect
median
mean
std
var
interval
__call__
fit
fit_loc_scale
nnlf
support

Notes
-----
Public methods of an instance of a distribution class (e.g., ``pdf``,
``cdf``) check their arguments and pass valid arguments to private,
computational methods (``_pdf``, ``_cdf``). For ``pdf(x)``, ``x`` is valid
if it is within the support of the distribution.
Whether a shape parameter is valid is decided by an ``_argcheck`` method
(which defaults to checking that its arguments are strictly positive.)

**Subclassing**

New random variables can be defined by subclassing the `rv_continuous` class
and re-defining at least the ``_pdf`` or the ``_cdf`` method (normalized
to location 0 and scale 1).

If positive argument checking is not correct for your RV
then you will also need to re-define the ``_argcheck`` method.

For most of the scipy.stats distributions, the support interval doesn't
depend on the shape parameters. ``x`` being in the support interval is
equivalent to ``self.a <= x <= self.b``.  If either of the endpoints of
the support do depend on the shape parameters, then
i) the distribution must implement the ``_get_support`` method; and
ii) those dependent endpoints must be omitted from the distribution's
call to the ``rv_continuous`` initializer.

Correct, but potentially slow defaults exist for the remaining
methods but for speed and/or accuracy you can over-ride::

  _logpdf, _cdf, _logcdf, _ppf, _rvs, _isf, _sf, _logsf

The default method ``_rvs`` relies on the inverse of the cdf, ``_ppf``,
applied to a uniform random variate. In order to generate random variates
efficiently, either the default ``_ppf`` needs to be overwritten (e.g.
if the inverse cdf can expressed in an explicit form) or a sampling
method needs to be implemented in a custom ``_rvs`` method.

If possible, you should override ``_isf``, ``_sf`` or ``_logsf``.
The main reason would be to improve numerical accuracy: for example,
the survival function ``_sf`` is computed as ``1 - _cdf`` which can
result in loss of precision if ``_cdf(x)`` is close to one.

**Methods that can be overwritten by subclasses**
::

  _rvs
  _pdf
  _cdf
  _sf
  _ppf
  _isf
  _stats
  _munp
  _entropy
  _argcheck
  _get_support

There are additional (internal and private) generic methods that can
be useful for cross-checking and for debugging, but might work in all
cases when directly called.

A note on ``shapes``: subclasses need not specify them explicitly. In this
case, `shapes` will be automatically deduced from the signatures of the
overridden methods (`pdf`, `cdf` etc).
If, for some reason, you prefer to avoid relying on introspection, you can
specify ``shapes`` explicitly as an argument to the instance constructor.


**Frozen Distributions**

Normally, you must provide shape parameters (and, optionally, location and
scale parameters to each call of a method of a distribution.

Alternatively, the object may be called (as a function) to fix the shape,
location, and scale parameters returning a 'frozen' continuous RV object:

rv = generic(<shape(s)>, loc=0, scale=1)
    `rv_frozen` object with the same methods but holding the given shape,
    location, and scale fixed

**Statistics**

Statistics are computed using numerical integration by default.
For speed you can redefine this using ``_stats``:

 - take shape parameters and return mu, mu2, g1, g2
 - If you can't compute one of these, return it as None
 - Can also be defined with a keyword argument ``moments``, which is a
   string composed of 'm', 'v', 's', and/or 'k'.
   Only the components appearing in string should be computed and
   returned in the order 'm', 'v', 's', or 'k'  with missing values
   returned as None.

Alternatively, you can override ``_munp``, which takes ``n`` and shape
parameters and returns the n-th non-central moment of the distribution.

Examples
--------
To create a new Gaussian distribution, we would do the following:

>>> from scipy.stats import rv_continuous
>>> class gaussian_gen(rv_continuous):
...     'Gaussian distribution'
...     def _pdf(self, x):
...         return np.exp(-x**2 / 2.) / np.sqrt(2.0 * np.pi)
>>> gaussian = gaussian_gen(name='gaussian')

``scipy.stats`` distributions are *instances*, so here we subclass
`rv_continuous` and create an instance. With this, we now have
a fully functional distribution with all relevant methods automagically
generated by the framework.

Note that above we defined a standard normal distribution, with zero mean
and unit variance. Shifting and scaling of the distribution can be done
by using ``loc`` and ``scale`` parameters: ``gaussian.pdf(x, loc, scale)``
essentially computes ``y = (x - loc) / scale`` and
``gaussian._pdf(y) / scale``.
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rv_discrete : sig
type tag = [`Rv_discrete]
type t = [`Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:float -> ?b:float -> ?name:string -> ?badvalue:float -> ?moment_tol:float -> ?values:Py.Object.t -> ?inc:int -> ?longname:string -> ?shapes:string -> ?extradoc:string -> ?seed:[`I of int | `T_numpy_random_RandomState_instance of Py.Object.t] -> unit -> t
(**
A generic discrete random variable class meant for subclassing.

`rv_discrete` is a base class to construct specific distribution classes
and instances for discrete random variables. It can also be used
to construct an arbitrary distribution defined by a list of support
points and corresponding probabilities.

Parameters
----------
a : float, optional
    Lower bound of the support of the distribution, default: 0
b : float, optional
    Upper bound of the support of the distribution, default: plus infinity
moment_tol : float, optional
    The tolerance for the generic calculation of moments.
values : tuple of two array_like, optional
    ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero
    probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``
    and ``pk`` must have the same shape.
inc : integer, optional
    Increment for the support of the distribution.
    Default is 1. (other values have not been tested)
badvalue : float, optional
    The value in a result arrays that indicates a value that for which
    some argument restriction is violated, default is np.nan.
name : str, optional
    The name of the instance. This string is used to construct the default
    example for distributions.
longname : str, optional
    This string is used as part of the first line of the docstring returned
    when a subclass has no docstring of its own. Note: `longname` exists
    for backwards compatibility, do not use for new subclasses.
shapes : str, optional
    The shape of the distribution. For example 'm, n' for a distribution
    that takes two integers as the two shape arguments for all its methods
    If not provided, shape parameters will be inferred from
    the signatures of the private methods, ``_pmf`` and ``_cdf`` of
    the instance.
extradoc :  str, optional
    This string is used as the last part of the docstring returned when a
    subclass has no docstring of its own. Note: `extradoc` exists for
    backwards compatibility, do not use for new subclasses.
seed : None or int or ``numpy.random.RandomState`` instance, optional
    This parameter defines the RandomState object to use for drawing
    random variates.
    If None, the global np.random state is used.
    If integer, it is used to seed the local RandomState instance.
    Default is None.

Methods
-------
rvs
pmf
logpmf
cdf
logcdf
sf
logsf
ppf
isf
moment
stats
entropy
expect
median
mean
std
var
interval
__call__
support


Notes
-----

This class is similar to `rv_continuous`. Whether a shape parameter is
valid is decided by an ``_argcheck`` method (which defaults to checking
that its arguments are strictly positive.)
The main differences are:

- the support of the distribution is a set of integers
- instead of the probability density function, ``pdf`` (and the
  corresponding private ``_pdf``), this class defines the
  *probability mass function*, `pmf` (and the corresponding
  private ``_pmf``.)
- scale parameter is not defined.

To create a new discrete distribution, we would do the following:

>>> from scipy.stats import rv_discrete
>>> class poisson_gen(rv_discrete):
...     'Poisson distribution'
...     def _pmf(self, k, mu):
...         return exp(-mu) * mu**k / factorial(k)

and create an instance::

>>> poisson = poisson_gen(name='poisson')

Note that above we defined the Poisson distribution in the standard form.
Shifting the distribution can be done by providing the ``loc`` parameter
to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``
delegates the work to ``poisson._pmf(x-loc, mu)``.

**Discrete distributions from a list of probabilities**

Alternatively, you can construct an arbitrary discrete rv defined
on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the
``values`` keyword argument to the `rv_discrete` constructor.

Examples
--------

Custom made discrete distribution:

>>> from scipy import stats
>>> xk = np.arange(7)
>>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)
>>> custm = stats.rv_discrete(name='custm', values=(xk, pk))
>>>
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)
>>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')
>>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)
>>> plt.show()

Random number generation:

>>> R = custm.rvs(size=100)
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rv_histogram : sig
type tag = [`Rv_histogram]
type t = [`Object | `Rv_continuous | `Rv_generic | `Rv_histogram] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?kwargs:(string * Py.Object.t) list -> histogram:Py.Object.t -> Py.Object.t list -> t
(**
Generates a distribution given by a histogram.
This is useful to generate a template distribution from a binned
datasample.

As a subclass of the `rv_continuous` class, `rv_histogram` inherits from it
a collection of generic methods (see `rv_continuous` for the full list),
and implements them based on the properties of the provided binned
datasample.

Parameters
----------
histogram : tuple of array_like
  Tuple containing two array_like objects
  The first containing the content of n bins
  The second containing the (n+1) bin boundaries
  In particular the return value np.histogram is accepted

Notes
-----
There are no additional shape parameters except for the loc and scale.
The pdf is defined as a stepwise function from the provided histogram
The cdf is a linear interpolation of the pdf.

.. versionadded:: 0.19.0

Examples
--------

Create a scipy.stats distribution from a numpy histogram

>>> import scipy.stats
>>> import numpy as np
>>> data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)
>>> hist = np.histogram(data, bins=100)
>>> hist_dist = scipy.stats.rv_histogram(hist)

Behaves like an ordinary scipy rv_continuous distribution

>>> hist_dist.pdf(1.0)
0.20538577847618705
>>> hist_dist.cdf(2.0)
0.90818568543056499

PDF is zero above (below) the highest (lowest) bin of the histogram,
defined by the max (min) of the original dataset

>>> hist_dist.pdf(np.max(data))
0.0
>>> hist_dist.cdf(np.max(data))
1.0
>>> hist_dist.pdf(np.min(data))
7.7591907244498314e-05
>>> hist_dist.cdf(np.min(data))
0.0

PDF and CDF follow the histogram

>>> import matplotlib.pyplot as plt
>>> X = np.linspace(-5.0, 5.0, 100)
>>> plt.title('PDF from Template')
>>> plt.hist(data, density=True, bins=100)
>>> plt.plot(X, hist_dist.pdf(X), label='PDF')
>>> plt.plot(X, hist_dist.cdf(X), label='CDF')
>>> plt.show()
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Contingency : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

val chi2_contingency : ?correction:bool -> ?lambda_:[`S of string | `F of float] -> observed:[>`Ndarray] Np.Obj.t -> unit -> (float * float * int * Py.Object.t)
(**
Chi-square test of independence of variables in a contingency table.

This function computes the chi-square statistic and p-value for the
hypothesis test of independence of the observed frequencies in the
contingency table [1]_ `observed`.  The expected frequencies are computed
based on the marginal sums under the assumption of independence; see
`scipy.stats.contingency.expected_freq`.  The number of degrees of
freedom is (expressed using numpy functions and attributes)::

    dof = observed.size - sum(observed.shape) + observed.ndim - 1


Parameters
----------
observed : array_like
    The contingency table. The table contains the observed frequencies
    (i.e. number of occurrences) in each category.  In the two-dimensional
    case, the table is often described as an 'R x C table'.
correction : bool, optional
    If True, *and* the degrees of freedom is 1, apply Yates' correction
    for continuity.  The effect of the correction is to adjust each
    observed value by 0.5 towards the corresponding expected value.
lambda_ : float or str, optional.
    By default, the statistic computed in this test is Pearson's
    chi-squared statistic [2]_.  `lambda_` allows a statistic from the
    Cressie-Read power divergence family [3]_ to be used instead.  See
    `power_divergence` for details.

Returns
-------
chi2 : float
    The test statistic.
p : float
    The p-value of the test
dof : int
    Degrees of freedom
expected : ndarray, same shape as `observed`
    The expected frequencies, based on the marginal sums of the table.

See Also
--------
contingency.expected_freq
fisher_exact
chisquare
power_divergence

Notes
-----
An often quoted guideline for the validity of this calculation is that
the test should be used only if the observed and expected frequencies
in each cell are at least 5.

This is a test for the independence of different categories of a
population. The test is only meaningful when the dimension of
`observed` is two or more.  Applying the test to a one-dimensional
table will always result in `expected` equal to `observed` and a
chi-square statistic equal to 0.

This function does not handle masked arrays, because the calculation
does not make sense with missing values.

Like stats.chisquare, this function computes a chi-square statistic;
the convenience this function provides is to figure out the expected
frequencies and degrees of freedom from the given contingency table.
If these were already known, and if the Yates' correction was not
required, one could use stats.chisquare.  That is, if one calls::

    chi2, p, dof, ex = chi2_contingency(obs, correction=False)

then the following is true::

    (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),
                                 ddof=obs.size - 1 - dof)

The `lambda_` argument was added in version 0.13.0 of scipy.

References
----------
.. [1] 'Contingency table',
       https://en.wikipedia.org/wiki/Contingency_table
.. [2] 'Pearson's chi-squared test',
       https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test
.. [3] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
A two-way example (2 x 3):

>>> from scipy.stats import chi2_contingency
>>> obs = np.array([[10, 10, 20], [20, 20, 20]])
>>> chi2_contingency(obs)
(2.7777777777777777,
 0.24935220877729619,
 2,
 array([[ 12.,  12.,  16.],
        [ 18.,  18.,  24.]]))

Perform the test using the log-likelihood ratio (i.e. the 'G-test')
instead of Pearson's chi-squared statistic.

>>> g, p, dof, expctd = chi2_contingency(obs, lambda_='log-likelihood')
>>> g, p
(2.7688587616781319, 0.25046668010954165)

A four-way example (2 x 2 x 2 x 2):

>>> obs = np.array(
...     [[[[12, 17],
...        [11, 16]],
...       [[11, 12],
...        [15, 16]]],
...      [[[23, 15],
...        [30, 22]],
...       [[14, 17],
...        [15, 16]]]])
>>> chi2_contingency(obs)
(8.7584514426741897,
 0.64417725029295503,
 11,
 array([[[[ 14.15462386,  14.15462386],
          [ 16.49423111,  16.49423111]],
         [[ 11.2461395 ,  11.2461395 ],
          [ 13.10500554,  13.10500554]]],
        [[[ 19.5591166 ,  19.5591166 ],
          [ 22.79202844,  22.79202844]],
         [[ 15.54012004,  15.54012004],
          [ 18.10873492,  18.10873492]]]]))
*)

val expected_freq : [>`Ndarray] Np.Obj.t -> Py.Object.t
(**
Compute the expected frequencies from a contingency table.

Given an n-dimensional contingency table of observed frequencies,
compute the expected frequencies for the table based on the marginal
sums under the assumption that the groups associated with each
dimension are independent.

Parameters
----------
observed : array_like
    The table of observed frequencies.  (While this function can handle
    a 1-D array, that case is trivial.  Generally `observed` is at
    least 2-D.)

Returns
-------
expected : ndarray of float64
    The expected frequencies, based on the marginal sums of the table.
    Same shape as `observed`.

Examples
--------
>>> observed = np.array([[10, 10, 20],[20, 20, 20]])
>>> from scipy.stats.contingency import expected_freq
>>> expected_freq(observed)
array([[ 12.,  12.,  16.],
       [ 18.,  18.,  24.]])
*)

val margins : [>`Ndarray] Np.Obj.t -> Py.Object.t
(**
Return a list of the marginal sums of the array `a`.

Parameters
----------
a : ndarray
    The array for which to compute the marginal sums.

Returns
-------
margsums : list of ndarrays
    A list of length `a.ndim`.  `margsums[k]` is the result
    of summing `a` over all axes except `k`; it has the same
    number of dimensions as `a`, but the length of each axis
    except axis `k` will be 1.

Examples
--------
>>> a = np.arange(12).reshape(2, 6)
>>> a
array([[ 0,  1,  2,  3,  4,  5],
       [ 6,  7,  8,  9, 10, 11]])
>>> from scipy.stats.contingency import margins
>>> m0, m1 = margins(a)
>>> m0
array([[15],
       [51]])
>>> m1
array([[ 6,  8, 10, 12, 14, 16]])

>>> b = np.arange(24).reshape(2,3,4)
>>> m0, m1, m2 = margins(b)
>>> m0
array([[[ 66]],
       [[210]]])
>>> m1
array([[[ 60],
        [ 92],
        [124]]])
>>> m2
array([[[60, 66, 72, 78]]])
*)

val power_divergence : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> ?lambda_:[`S of string | `F of float] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Cressie-Read power divergence statistic and goodness of fit test.

This function tests the null hypothesis that the categorical data
has the given frequencies, using the Cressie-Read power divergence
statistic.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.
lambda_ : float or str, optional
    The power in the Cressie-Read power divergence statistic.  The default
    is 1.  For convenience, `lambda_` may be assigned one of the following
    strings, in which case the corresponding numerical value is used::

        String              Value   Description
        'pearson'             1     Pearson's chi-squared statistic.
                                    In this case, the function is
                                    equivalent to `stats.chisquare`.
        'log-likelihood'      0     Log-likelihood ratio. Also known as
                                    the G-test [3]_.
        'freeman-tukey'      -1/2   Freeman-Tukey statistic.
        'mod-log-likelihood' -1     Modified log-likelihood ratio.
        'neyman'             -2     Neyman's statistic.
        'cressie-read'        2/3   The power recommended in [5]_.

Returns
-------
statistic : float or ndarray
    The Cressie-Read power divergence test statistic.  The value is
    a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.
pvalue : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `stat` are scalars.

See Also
--------
chisquare

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

When `lambda_` is less than zero, the formula for the statistic involves
dividing by `f_obs`, so a warning or error may be generated if any value
in `f_obs` is 0.

Similarly, a warning or error may be generated if any value in `f_exp` is
zero when `lambda_` >= 0.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not a chisquare, in which case this
test is not appropriate.

This function handles masked arrays.  If an element of `f_obs` or `f_exp`
is masked, then data at that position is ignored, and does not count
towards the size of the data set.

.. versionadded:: 0.13.0

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test
.. [3] 'G-test', https://en.wikipedia.org/wiki/G-test
.. [4] Sokal, R. R. and Rohlf, F. J. 'Biometry: the principles and
       practice of statistics in biological research', New York: Freeman
       (1981)
.. [5] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
(See `chisquare` for more examples.)

When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.  Here we
perform a G-test (i.e. use the log-likelihood ratio statistic):

>>> from scipy.stats import power_divergence
>>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')
(2.006573162632538, 0.84823476779463769)

The expected frequencies can be given with the `f_exp` argument:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[16, 16, 16, 16, 16, 8],
...                  lambda_='log-likelihood')
(3.3281031458963746, 0.6495419288047497)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> power_divergence(obs, lambda_='log-likelihood')
(array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> power_divergence(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> power_divergence(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
test statistic with `ddof`.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we must use ``axis=1``:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[[16, 16, 16, 16, 16, 8],
...                         [8, 20, 20, 16, 12, 12]],
...                  axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val reduce : ?initial:Py.Object.t -> function_:Py.Object.t -> sequence:Py.Object.t -> unit -> Py.Object.t
(**
reduce(function, sequence[, initial]) -> value

Apply a function of two arguments cumulatively to the items of a sequence,
from left to right, so as to reduce the sequence to a single value.
For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates
((((1+2)+3)+4)+5).  If initial is present, it is placed before the items
of the sequence in the calculation, and serves as a default when the
sequence is empty.
*)


end

module Distributions : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module Alpha_gen : sig
type tag = [`Alpha_gen]
type t = [`Alpha_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An alpha continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `alpha` ([1]_, [2]_) is:

.. math::

    f(x, a) = \frac{1}{x^2 \Phi(a) \sqrt{2\pi}} *
              \exp(-\frac{1}{2} (a-1/x)^2)

where :math:`\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.

`alpha` takes ``a`` as a shape parameter.

%(after_notes)s

References
----------
.. [1] Johnson, Kotz, and Balakrishnan, 'Continuous Univariate
       Distributions, Volume 1', Second Edition, John Wiley and Sons,
       p. 173 (1994).
.. [2] Anthony A. Salvia, 'Reliability applications of the Alpha
       Distribution', IEEE Transactions on Reliability, Vol. R-34,
       No. 3, pp. 251-252 (1985).

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Anglit_gen : sig
type tag = [`Anglit_gen]
type t = [`Anglit_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An anglit continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `anglit` is:

.. math::

    f(x) = \sin(2x + \pi/2) = \cos(2x)

for :math:`-\pi/4 \le x \le \pi/4`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Arcsine_gen : sig
type tag = [`Arcsine_gen]
type t = [`Arcsine_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An arcsine continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `arcsine` is:

.. math::

    f(x) = \frac{1}{\pi \sqrt{x (1-x)}}

for :math:`0 < x < 1`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Argus_gen : sig
type tag = [`Argus_gen]
type t = [`Argus_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Argus distribution

%(before_notes)s

Notes
-----
The probability density function for `argus` is:

.. math::

    f(x, \chi) = \frac{\chi^3}{\sqrt{2\pi} \Psi(\chi)} x \sqrt{1-x^2}
                 \exp(-\chi^2 (1 - x^2)/2)

for :math:`0 < x < 1`, where

.. math::

    \Psi(\chi) = \Phi(\chi) - \chi \phi(\chi) - 1/2

with :math:`\Phi` and :math:`\phi` being the CDF and PDF of a standard
normal distribution, respectively.

`argus` takes :math:`\chi` as shape a parameter.

References
----------

.. [1] 'ARGUS distribution',
       https://en.wikipedia.org/wiki/ARGUS_distribution

%(after_notes)s

.. versionadded:: 0.19.0

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Bernoulli_gen : sig
type tag = [`Bernoulli_gen]
type t = [`Bernoulli_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Bernoulli discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `bernoulli` is:

.. math::

   f(k) = \begin{cases}1-p  &\text{if } k = 0\\
                       p    &\text{if } k = 1\end{cases}

for :math:`k` in :math:`\{0, 1\}`.

`bernoulli` takes :math:`p` as shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Beta_gen : sig
type tag = [`Beta_gen]
type t = [`Beta_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A beta continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `beta` is:

.. math::

    f(x, a, b) = \frac{\Gamma(a+b) x^{a-1} (1-x)^{b-1}}
                      {\Gamma(a) \Gamma(b)}

for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`beta` takes :math:`a` and :math:`b` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

In the special case where both `floc` and `fscale` are given, a
`ValueError` is raised if any value `x` in `data` does not satisfy
`floc < x < floc + fscale`.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Betabinom_gen : sig
type tag = [`Betabinom_gen]
type t = [`Betabinom_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A beta-binomial discrete random variable.

%(before_notes)s

Notes
-----
The beta-binomial distribution is a binomial distribution with a
probability of success `p` that follows a beta distribution.

The probability mass function for `betabinom` is:

.. math::

   f(k) = \binom{n}{k} \frac{B(k + a, n - k + b)}{B(a, b)}

for ``k`` in ``{0, 1,..., n}``, :math:`n \geq 0`, :math:`a > 0`,
:math:`b > 0`, where :math:`B(a, b)` is the beta function.

`betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.

References
----------
.. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution

%(after_notes)s

.. versionadded:: 1.4.0

See Also
--------
beta, binom

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Betaprime_gen : sig
type tag = [`Betaprime_gen]
type t = [`Betaprime_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A beta prime continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `betaprime` is:

.. math::

    f(x, a, b) = \frac{x^{a-1} (1+x)^{-a-b}}{\beta(a, b)}

for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where
:math:`\beta(a, b)` is the beta function (see `scipy.special.beta`).

`betaprime` takes ``a`` and ``b`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Binom_gen : sig
type tag = [`Binom_gen]
type t = [`Binom_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A binomial discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `binom` is:

.. math::

   f(k) = \binom{n}{k} p^k (1-p)^{n-k}

for ``k`` in ``{0, 1,..., n}``.

`binom` takes ``n`` and ``p`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Boltzmann_gen : sig
type tag = [`Boltzmann_gen]
type t = [`Boltzmann_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Boltzmann (Truncated Discrete Exponential) random variable.

%(before_notes)s

Notes
-----
The probability mass function for `boltzmann` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k) / (1-\exp(-\lambda N))

for :math:`k = 0,..., N-1`.

`boltzmann` takes :math:`\lambda > 0` and :math:`N > 0` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Bradford_gen : sig
type tag = [`Bradford_gen]
type t = [`Bradford_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Bradford continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `bradford` is:

.. math::

    f(x, c) = \frac{c}{\log(1+c) (1+cx)}

for :math:`0 <= x <= 1` and :math:`c > 0`.

`bradford` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Burr12_gen : sig
type tag = [`Burr12_gen]
type t = [`Burr12_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Burr (Type XII) continuous random variable.

%(before_notes)s

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr : Burr Type III distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{c-1} / (1 + x^c)^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`
and :math:`d`.

This is the PDF corresponding to the twelfth CDF given in Burr's list;
specifically, it is equation (20) in Burr's paper [1]_.

%(after_notes)s

The Burr type 12 distribution is also sometimes referred to as
the Singh-Maddala distribution from NIST [2]_.

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).

.. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm

.. [3] 'Burr distribution',
   https://en.wikipedia.org/wiki/Burr_distribution

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Burr_gen : sig
type tag = [`Burr_gen]
type t = [`Burr_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Burr (Type III) continuous random variable.

%(before_notes)s

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr12 : Burr Type XII distribution
mielke : Mielke Beta-Kappa / Dagum distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{-c - 1} / (1 + x^{-c})^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr` takes :math:`c` and :math:`d` as shape parameters.

This is the PDF corresponding to the third CDF given in Burr's list;
specifically, it is equation (11) in Burr's paper [1]_. The distribution
is also commonly referred to as the Dagum distribution [2]_. If the
parameter :math:`c < 1` then the mean of the distribution does not
exist and if :math:`c < 2` the variance does not exist [2]_.
The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.

%(after_notes)s

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).
.. [2] https://en.wikipedia.org/wiki/Dagum_distribution
.. [3] Kleiber, Christian. 'A guide to the Dagum distributions.'
   Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Cauchy_gen : sig
type tag = [`Cauchy_gen]
type t = [`Cauchy_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Cauchy continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `cauchy` is

.. math::

    f(x) = \frac{1}{\pi (1 + x^2)}

for a real number :math:`x`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Chi2_gen : sig
type tag = [`Chi2_gen]
type t = [`Chi2_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A chi-squared continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `chi2` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2} \Gamma \left( k/2 \right)}
               x^{k/2-1} \exp \left( -x/2 \right)

for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation).

`chi2` takes ``df`` as a shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Chi_gen : sig
type tag = [`Chi_gen]
type t = [`Chi_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A chi continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `chi` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2-1} \Gamma \left( k/2 \right)}
               x^{k-1} \exp \left( -x^2/2 \right)

for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation). :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

Special cases of `chi` are:

    - ``chi(1, loc, scale)`` is equivalent to `halfnorm`
    - ``chi(2, 0, scale)`` is equivalent to `rayleigh`
    - ``chi(3, 0, scale)`` is equivalent to `maxwell`

`chi` takes ``df`` as a shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Cosine_gen : sig
type tag = [`Cosine_gen]
type t = [`Cosine_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A cosine continuous random variable.

%(before_notes)s

Notes
-----
The cosine distribution is an approximation to the normal distribution.
The probability density function for `cosine` is:

.. math::

    f(x) = \frac{1}{2\pi} (1+\cos(x))

for :math:`-\pi \le x \le \pi`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Crystalball_gen : sig
type tag = [`Crystalball_gen]
type t = [`Crystalball_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Crystalball distribution

%(before_notes)s

Notes
-----
The probability density function for `crystalball` is:

.. math::

    f(x, \beta, m) =  \begin{cases}
                        N \exp(-x^2 / 2),  &\text{for } x > -\beta\\
                        N A (B - x)^{-m}  &\text{for } x \le -\beta
                      \end{cases}

where :math:`A = (m / |\beta|)^n  \exp(-\beta^2 / 2)`,
:math:`B = m/|\beta| - |\beta|` and :math:`N` is a normalisation constant.

`crystalball` takes :math:`\beta > 0` and :math:`m > 1` as shape
parameters.  :math:`\beta` defines the point where the pdf changes
from a power-law to a Gaussian distribution.  :math:`m` is the power
of the power-law tail.

References
----------
.. [1] 'Crystal Ball Function',
       https://en.wikipedia.org/wiki/Crystal_Ball_function

%(after_notes)s

.. versionadded:: 0.19.0

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Dgamma_gen : sig
type tag = [`Dgamma_gen]
type t = [`Dgamma_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A double gamma continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `dgamma` is:

.. math::

    f(x, a) = \frac{1}{2\Gamma(a)} |x|^{a-1} \exp(-|x|)

for a real number :math:`x` and :math:`a > 0`. :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`dgamma` takes ``a`` as a shape parameter for :math:`a`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Dlaplace_gen : sig
type tag = [`Dlaplace_gen]
type t = [`Dlaplace_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A  Laplacian discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `dlaplace` is:

.. math::

    f(k) = \tanh(a/2) \exp(-a |k|)

for integers :math:`k` and :math:`a > 0`.

`dlaplace` takes :math:`a` as shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Dweibull_gen : sig
type tag = [`Dweibull_gen]
type t = [`Dweibull_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A double Weibull continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `dweibull` is given by

.. math::

    f(x, c) = c / 2 |x|^{c-1} \exp(-|x|^c)

for a real number :math:`x` and :math:`c > 0`.

`dweibull` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Erlang_gen : sig
type tag = [`Erlang_gen]
type t = [`Erlang_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An Erlang continuous random variable.

%(before_notes)s

See Also
--------
gamma

Notes
-----
The Erlang distribution is a special case of the Gamma distribution, with
the shape parameter `a` an integer.  Note that this restriction is not
enforced by `erlang`. It will, however, generate a warning the first time
a non-integer value is used for the shape parameter.

Refer to `gamma` for examples.
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:Py.Object.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

When the location is fixed by using the argument `floc`, this
function uses explicit formulas or solves a simpler numerical
problem than the full ML optimization problem.  So in that case,
the `optimizer`, `loc` and `scale` arguments are ignored.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Expon_gen : sig
type tag = [`Expon_gen]
type t = [`Expon_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An exponential continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `expon` is:

.. math::

    f(x) = \exp(-x)

for :math:`x \ge 0`.

%(after_notes)s

A common parameterization for `expon` is in terms of the rate parameter
``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This
parameterization corresponds to using ``scale = 1 / lambda``.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This function uses explicit formulas for the maximum likelihood
estimation of the exponential distribution parameters, so the
`optimizer`, `loc` and `scale` keyword arguments are ignored.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Exponnorm_gen : sig
type tag = [`Exponnorm_gen]
type t = [`Exponnorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An exponentially modified Normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `exponnorm` is:

.. math::

    f(x, K) = \frac{1}{2K} \exp\left(\frac{1}{2 K^2} - x / K \right)
              \text{erfc}\left(-\frac{x - 1/K}{\sqrt{2}}\right)

where :math:`x` is a real number and :math:`K > 0`.

It can be thought of as the sum of a standard normal random variable
and an independent exponentially distributed random variable with rate
``1/K``.

%(after_notes)s

An alternative parameterization of this distribution (for example, in
`Wikipedia <https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution>`_)
involves three parameters, :math:`\mu`, :math:`\lambda` and
:math:`\sigma`.
In the present parameterization this corresponds to having ``loc`` and
``scale`` equal to :math:`\mu` and :math:`\sigma`, respectively, and
shape parameter :math:`K = 1/(\sigma\lambda)`.

.. versionadded:: 0.16.0

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Exponpow_gen : sig
type tag = [`Exponpow_gen]
type t = [`Exponpow_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An exponential power continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `exponpow` is:

.. math::

    f(x, b) = b x^{b-1} \exp(1 + x^b - \exp(x^b))

for :math:`x \ge 0`, :math:`b > 0`.  Note that this is a different
distribution from the exponential power distribution that is also known
under the names 'generalized normal' or 'generalized Gaussian'.

`exponpow` takes ``b`` as a shape parameter for :math:`b`.

%(after_notes)s

References
----------
http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Exponweib_gen : sig
type tag = [`Exponweib_gen]
type t = [`Exponweib_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An exponentiated Weibull continuous random variable.

%(before_notes)s

See Also
--------
weibull_min, numpy.random.mtrand.RandomState.weibull

Notes
-----
The probability density function for `exponweib` is:

.. math::

    f(x, a, c) = a c [1-\exp(-x^c)]^{a-1} \exp(-x^c) x^{c-1}

and its cumulative distribution function is:

.. math::

    F(x, a, c) = [1-\exp(-x^c)]^a

for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.

`exponweib` takes :math:`a` and :math:`c` as shape parameters:

* :math:`a` is the exponentiation parameter,
  with the special case :math:`a=1` corresponding to the
  (non-exponentiated) Weibull distribution `weibull_min`.
* :math:`c` is the shape parameter of the non-exponentiated Weibull law.

%(after_notes)s

References
----------
https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module F_gen : sig
type tag = [`F_gen]
type t = [`F_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An F continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `f` is:

.. math::

    f(x, df_1, df_2) = \frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}
                            {(df_2+df_1 x)^{(df_1+df_2)/2}
                             B(df_1/2, df_2/2)}

for :math:`x > 0`.

`f` takes ``dfn`` and ``dfd`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Fatiguelife_gen : sig
type tag = [`Fatiguelife_gen]
type t = [`Fatiguelife_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A fatigue-life (Birnbaum-Saunders) continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `fatiguelife` is:

.. math::

    f(x, c) = \frac{x+1}{2c\sqrt{2\pi x^3}} \exp(-\frac{(x-1)^2}{2x c^2})

for :math:`x >= 0` and :math:`c > 0`.

`fatiguelife` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

References
----------
.. [1] 'Birnbaum-Saunders distribution',
       https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Fisk_gen : sig
type tag = [`Fisk_gen]
type t = [`Fisk_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Fisk continuous random variable.

The Fisk distribution is also known as the log-logistic distribution.

%(before_notes)s

Notes
-----
The probability density function for `fisk` is:

.. math::

    f(x, c) = c x^{-c-1} (1 + x^{-c})^{-2}

for :math:`x >= 0` and :math:`c > 0`.

`fisk` takes ``c`` as a shape parameter for :math:`c`.

`fisk` is a special case of `burr` or `burr12` with ``d=1``.

%(after_notes)s

See Also
--------
burr

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Foldcauchy_gen : sig
type tag = [`Foldcauchy_gen]
type t = [`Foldcauchy_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A folded Cauchy continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `foldcauchy` is:

.. math::

    f(x, c) = \frac{1}{\pi (1+(x-c)^2)} + \frac{1}{\pi (1+(x+c)^2)}

for :math:`x \ge 0`.

`foldcauchy` takes ``c`` as a shape parameter for :math:`c`.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Foldnorm_gen : sig
type tag = [`Foldnorm_gen]
type t = [`Foldnorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A folded normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `foldnorm` is:

.. math::

    f(x, c) = \sqrt{2/\pi} cosh(c x) \exp(-\frac{x^2+c^2}{2})

for :math:`c \ge 0`.

`foldnorm` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Frechet_l_gen : sig
type tag = [`Frechet_l_gen]
type t = [`Frechet_l_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Frechet left (or Weibull maximum) continuous random variable.

%(before_notes)s

See Also
--------
weibull_max : The same distribution as `frechet_l`.

Notes
-----
%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val expect : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val fit : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val fit_loc_scale : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val interval : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val isf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val moment : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val nnlf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val pdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val ppf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_l` is deprecated!
The distribution `frechet_l` is a synonym for `weibull_max`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_max`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Frechet_r_gen : sig
type tag = [`Frechet_r_gen]
type t = [`Frechet_r_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Frechet right (or Weibull minimum) continuous random variable.

%(before_notes)s

See Also
--------
weibull_min : The same distribution as `frechet_r`.

Notes
-----
%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val expect : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val fit : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val fit_loc_scale : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val interval : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val isf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val moment : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val nnlf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val pdf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val ppf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
`frechet_r` is deprecated!
The distribution `frechet_r` is a synonym for `weibull_min`; this historical
usage is deprecated because of possible confusion with the (quite different)
Frechet distribution.  To preserve the existing behavior of the program, use
`scipy.stats.weibull_min`.  For the Frechet distribution (i.e. the Type II
extreme value distribution), use `scipy.stats.invweibull`.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gamma_gen : sig
type tag = [`Gamma_gen]
type t = [`Gamma_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A gamma continuous random variable.

%(before_notes)s

See Also
--------
erlang, expon

Notes
-----
The probability density function for `gamma` is:

.. math::

    f(x, a) = \frac{x^{a-1} \exp(-x)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`. Here :math:`\Gamma(a)` refers to the
gamma function.

`gamma` takes ``a`` as a shape parameter for :math:`a`.

When :math:`a` is an integer, `gamma` reduces to the Erlang
distribution, and when :math:`a=1` to the exponential distribution.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

When the location is fixed by using the argument `floc`, this
function uses explicit formulas or solves a simpler numerical
problem than the full ML optimization problem.  So in that case,
the `optimizer`, `loc` and `scale` arguments are ignored.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gausshyper_gen : sig
type tag = [`Gausshyper_gen]
type t = [`Gausshyper_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Gauss hypergeometric continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `gausshyper` is:

.. math::

    f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}

for :math:`0 \le x \le 1`, :math:`a > 0`, :math:`b > 0`, and
:math:`C = \frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.
:math:`F[2, 1]` is the Gauss hypergeometric function
`scipy.special.hyp2f1`.

`gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape
parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Genexpon_gen : sig
type tag = [`Genexpon_gen]
type t = [`Genexpon_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized exponential continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `genexpon` is:

.. math::

    f(x, a, b, c) = (a + b (1 - \exp(-c x)))
                    \exp(-a x - b x + \frac{b}{c}  (1-\exp(-c x)))

for :math:`x \ge 0`, :math:`a, b, c > 0`.

`genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.

%(after_notes)s

References
----------
H.K. Ryu, 'An Extension of Marshall and Olkin's Bivariate Exponential
Distribution', Journal of the American Statistical Association, 1993.

N. Balakrishnan, 'The Exponential Distribution: Theory, Methods and
Applications', Asit P. Basu.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Genextreme_gen : sig
type tag = [`Genextreme_gen]
type t = [`Genextreme_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized extreme value continuous random variable.

%(before_notes)s

See Also
--------
gumbel_r

Notes
-----
For :math:`c=0`, `genextreme` is equal to `gumbel_r`.
The probability density function for `genextreme` is:

.. math::

    f(x, c) = \begin{cases}
                \exp(-\exp(-x)) \exp(-x)              &\text{for } c = 0\\
                \exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1}  &\text{for }
                                                        x \le 1/c, c > 0
              \end{cases}


Note that several sources and software packages use the opposite
convention for the sign of the shape parameter :math:`c`.

`genextreme` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gengamma_gen : sig
type tag = [`Gengamma_gen]
type t = [`Gengamma_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized gamma continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `gengamma` is:

.. math::

    f(x, a, c) = \frac{ |c| x^{c a-1} \exp(-x^c)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`, and :math:`c \ne 0`.
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gengamma` takes :math:`a` and :math:`c` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Genhalflogistic_gen : sig
type tag = [`Genhalflogistic_gen]
type t = [`Genhalflogistic_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized half-logistic continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `genhalflogistic` is:

.. math::

    f(x, c) = \frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}

for :math:`0 \le x \le 1/c`, and :math:`c > 0`.

`genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Geninvgauss_gen : sig
type tag = [`Geninvgauss_gen]
type t = [`Geninvgauss_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Generalized Inverse Gaussian continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `geninvgauss` is:

.. math::

    f(x, p, b) = x^{p-1} \exp(-b (x + 1/x) / 2) / (2 K_p(b))

where `x > 0`, and the parameters `p, b` satisfy `b > 0` ([1]_).
:math:`K_p` is the modified Bessel function of second kind of order `p`
(`scipy.special.kv`).

%(after_notes)s

The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of
`geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.

Generating random variates is challenging for this distribution. The
implementation is based on [2]_.

References
----------
.. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, 'First hitting time
   models for the generalized inverse gaussian distribution',
   Stochastic Processes and their Applications 7, pp. 49--54, 1978.

.. [2] W. Hoermann and J. Leydold, 'Generating generalized inverse Gaussian
   random variates', Statistics and Computing, 24(4), p. 547--557, 2014.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Genlogistic_gen : sig
type tag = [`Genlogistic_gen]
type t = [`Genlogistic_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized logistic continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `genlogistic` is:

.. math::

    f(x, c) = c \frac{\exp(-x)}
                     {(1 + \exp(-x))^{c+1}}

for :math:`x >= 0`, :math:`c > 0`.

`genlogistic` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gennorm_gen : sig
type tag = [`Gennorm_gen]
type t = [`Gennorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `gennorm` is [1]_:

.. math::

    f(x, \beta) = \frac{\beta}{2 \Gamma(1/\beta)} \exp(-|x|^\beta)

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to a Laplace distribution.
For :math:`\beta = 2`, it is identical to a normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
laplace : Laplace distribution
norm : normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Genpareto_gen : sig
type tag = [`Genpareto_gen]
type t = [`Genpareto_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A generalized Pareto continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `genpareto` is:

.. math::

    f(x, c) = (1 + c x)^{-1 - 1/c}

defined for :math:`x \ge 0` if :math:`c \ge 0`, and for
:math:`0 \le x \le -1/c` if :math:`c < 0`.

`genpareto` takes ``c`` as a shape parameter for :math:`c`.

For :math:`c=0`, `genpareto` reduces to the exponential
distribution, `expon`:

.. math::

    f(x, 0) = \exp(-x)

For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:

.. math::

    f(x, -1) = 1

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Geom_gen : sig
type tag = [`Geom_gen]
type t = [`Geom_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A geometric discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `geom` is:

.. math::

    f(k) = (1-p)^{k-1} p

for :math:`k \ge 1`.

`geom` takes :math:`p` as shape parameter.

%(after_notes)s

See Also
--------
planck

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gilbrat_gen : sig
type tag = [`Gilbrat_gen]
type t = [`Gilbrat_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Gilbrat continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `gilbrat` is:

.. math::

    f(x) = \frac{1}{x \sqrt{2\pi}} \exp(-\frac{1}{2} (\log(x))^2)

`gilbrat` is a special case of `lognorm` with ``s=1``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gompertz_gen : sig
type tag = [`Gompertz_gen]
type t = [`Gompertz_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Gompertz (or truncated Gumbel) continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `gompertz` is:

.. math::

    f(x, c) = c \exp(x) \exp(-c (e^x-1))

for :math:`x \ge 0`, :math:`c > 0`.

`gompertz` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gumbel_l_gen : sig
type tag = [`Gumbel_l_gen]
type t = [`Gumbel_l_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A left-skewed Gumbel continuous random variable.

%(before_notes)s

See Also
--------
gumbel_r, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_l` is:

.. math::

    f(x) = \exp(x - e^x)

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Gumbel_r_gen : sig
type tag = [`Gumbel_r_gen]
type t = [`Gumbel_r_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A right-skewed Gumbel continuous random variable.

%(before_notes)s

See Also
--------
gumbel_l, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_r` is:

.. math::

    f(x) = \exp(-(x + e^{-x}))

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Halfcauchy_gen : sig
type tag = [`Halfcauchy_gen]
type t = [`Halfcauchy_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Half-Cauchy continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `halfcauchy` is:

.. math::

    f(x) = \frac{2}{\pi (1 + x^2)}

for :math:`x \ge 0`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Halfgennorm_gen : sig
type tag = [`Halfgennorm_gen]
type t = [`Halfgennorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
The upper half of a generalized normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `halfgennorm` is:

.. math::

    f(x, \beta) = \frac{\beta}{\Gamma(1/\beta)} \exp(-|x|^\beta)

for :math:`x > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to an exponential distribution.
For :math:`\beta = 2`, it is identical to a half normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
gennorm : generalized normal distribution
expon : exponential distribution
halfnorm : half normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Halflogistic_gen : sig
type tag = [`Halflogistic_gen]
type t = [`Halflogistic_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A half-logistic continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `halflogistic` is:

.. math::

    f(x) = \frac{ 2 e^{-x} }{ (1+e^{-x})^2 }
         = \frac{1}{2} \text{sech}(x/2)^2

for :math:`x \ge 0`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Halfnorm_gen : sig
type tag = [`Halfnorm_gen]
type t = [`Halfnorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A half-normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `halfnorm` is:

.. math::

    f(x) = \sqrt{2/\pi} \exp(-x^2 / 2)

for :math:`x >= 0`.

`halfnorm` is a special case of `chi` with ``df=1``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Hypergeom_gen : sig
type tag = [`Hypergeom_gen]
type t = [`Hypergeom_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A hypergeometric discrete random variable.

The hypergeometric distribution models drawing objects from a bin.
`M` is the total number of objects, `n` is total number of Type I objects.
The random variate represents the number of Type I objects in `N` drawn
without replacement from the total population.

%(before_notes)s

Notes
-----
The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not
universally accepted.  See the Examples for a clarification of the
definitions used here.

The probability mass function is defined as,

.. math:: p(k, M, n, N) = \frac{\binom{n}{k} \binom{M - n}{N - k}}
                               {\binom{M}{N}}

for :math:`k \in [\max(0, N - M + n), \min(n, N)]`, where the binomial
coefficients are defined as,

.. math:: \binom{n}{k} \equiv \frac{n!}{k! (n - k)!}.

%(after_notes)s

Examples
--------
>>> from scipy.stats import hypergeom
>>> import matplotlib.pyplot as plt

Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if
we want to know the probability of finding a given number of dogs if we
choose at random 12 of the 20 animals, we can initialize a frozen
distribution and plot the probability mass function:

>>> [M, n, N] = [20, 7, 12]
>>> rv = hypergeom(M, n, N)
>>> x = np.arange(0, n+1)
>>> pmf_dogs = rv.pmf(x)

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, pmf_dogs, 'bo')
>>> ax.vlines(x, 0, pmf_dogs, lw=2)
>>> ax.set_xlabel('# of dogs in our group of chosen animals')
>>> ax.set_ylabel('hypergeom PMF')
>>> plt.show()

Instead of using a frozen distribution we can also use `hypergeom`
methods directly.  To for example obtain the cumulative distribution
function, use:

>>> prb = hypergeom.cdf(x, M, n, N)

And to generate random numbers:

>>> R = hypergeom.rvs(M, n, N, size=10)
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Hypsecant_gen : sig
type tag = [`Hypsecant_gen]
type t = [`Hypsecant_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A hyperbolic secant continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `hypsecant` is:

.. math::

    f(x) = \frac{1}{\pi} \text{sech}(x)

for a real number :math:`x`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Invgamma_gen : sig
type tag = [`Invgamma_gen]
type t = [`Invgamma_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An inverted gamma continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `invgamma` is:

.. math::

    f(x, a) = \frac{x^{-a-1}}{\Gamma(a)} \exp(-\frac{1}{x})

for :math:`x >= 0`, :math:`a > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`invgamma` takes ``a`` as a shape parameter for :math:`a`.

`invgamma` is a special case of `gengamma` with ``c=-1``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Invgauss_gen : sig
type tag = [`Invgauss_gen]
type t = [`Invgauss_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An inverse Gaussian continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `invgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2 \pi x^3}}
                \exp(-\frac{(x-\mu)^2}{2 x \mu^2})

for :math:`x >= 0` and :math:`\mu > 0`.

`invgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

%(after_notes)s

When :math:`\mu` is too small, evaluating the cumulative distribution
function will be inaccurate due to ``cdf(mu -> 0) = inf * 0``.
NaNs are returned for :math:`\mu \le 0.0028`.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Invweibull_gen : sig
type tag = [`Invweibull_gen]
type t = [`Invweibull_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An inverted Weibull continuous random variable.

This distribution is also known as the Frchet distribution or the
type II extreme value distribution.

%(before_notes)s

Notes
-----
The probability density function for `invweibull` is:

.. math::

    f(x, c) = c x^{-c-1} \exp(-x^{-c})

for :math:`x > 0`, :math:`c > 0`.

`invweibull` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

References
----------
F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, 'The generalized inverse
Weibull distribution', Stat. Papers, vol. 52, pp. 591-619, 2011.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Johnsonsb_gen : sig
type tag = [`Johnsonsb_gen]
type t = [`Johnsonsb_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Johnson SB continuous random variable.

%(before_notes)s

See Also
--------
johnsonsu

Notes
-----
The probability density function for `johnsonsb` is:

.. math::

    f(x, a, b) = \frac{b}{x(1-x)}  \phi(a + b \log \frac{x}{1-x} )

for :math:`0 <= x < =1` and :math:`a, b > 0`, and :math:`\phi` is the normal
pdf.

`johnsonsb` takes :math:`a` and :math:`b` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Johnsonsu_gen : sig
type tag = [`Johnsonsu_gen]
type t = [`Johnsonsu_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Johnson SU continuous random variable.

%(before_notes)s

See Also
--------
johnsonsb

Notes
-----
The probability density function for `johnsonsu` is:

.. math::

    f(x, a, b) = \frac{b}{\sqrt{x^2 + 1}}
                 \phi(a + b \log(x + \sqrt{x^2 + 1}))

for all :math:`x, a, b > 0`, and :math:`\phi` is the normal pdf.

`johnsonsu` takes :math:`a` and :math:`b` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Kappa3_gen : sig
type tag = [`Kappa3_gen]
type t = [`Kappa3_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Kappa 3 parameter distribution.

%(before_notes)s

Notes
-----
The probability density function for `kappa3` is:

.. math::

    f(x, a) = a (a + x^a)^{-(a + 1)/a}

for :math:`x > 0` and :math:`a > 0`.

`kappa3` takes ``a`` as a shape parameter for :math:`a`.

References
----------
P.W. Mielke and E.S. Johnson, 'Three-Parameter Kappa Distribution Maximum
Likelihood and Likelihood Ratio Tests', Methods in Weather Research,
701-707, (September, 1973),
https://doi.org/10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2

B. Kumphon, 'Maximum Entropy and Maximum Likelihood Estimation for the
Three-Parameter Kappa Distribution', Open Journal of Statistics, vol 2,
415-419 (2012), https://doi.org/10.4236/ojs.2012.24050

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Kappa4_gen : sig
type tag = [`Kappa4_gen]
type t = [`Kappa4_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Kappa 4 parameter distribution.

%(before_notes)s

Notes
-----
The probability density function for kappa4 is:

.. math::

    f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}

if :math:`h` and :math:`k` are not equal to 0.

If :math:`h` or :math:`k` are zero then the pdf can be simplified:

h = 0 and k != 0::

    kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*
                          exp(-(1.0 - k*x)**(1.0/k))

h != 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)

h = 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))

kappa4 takes :math:`h` and :math:`k` as shape parameters.

The kappa4 distribution returns other distributions when certain
:math:`h` and :math:`k` values are used.

+------+-------------+----------------+------------------+
| h    | k=0.0       | k=1.0          | -inf<=k<=inf     |
+======+=============+================+==================+
| -1.0 | Logistic    |                | Generalized      |
|      |             |                | Logistic(1)      |
|      |             |                |                  |
|      | logistic(x) |                |                  |
+------+-------------+----------------+------------------+
|  0.0 | Gumbel      | Reverse        | Generalized      |
|      |             | Exponential(2) | Extreme Value    |
|      |             |                |                  |
|      | gumbel_r(x) |                | genextreme(x, k) |
+------+-------------+----------------+------------------+
|  1.0 | Exponential | Uniform        | Generalized      |
|      |             |                | Pareto           |
|      |             |                |                  |
|      | expon(x)    | uniform(x)     | genpareto(x, -k) |
+------+-------------+----------------+------------------+

(1) There are at least five generalized logistic distributions.
    Four are described here:
    https://en.wikipedia.org/wiki/Generalized_logistic_distribution
    The 'fifth' one is the one kappa4 should match which currently
    isn't implemented in scipy:
    https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution
    https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html
(2) This distribution is currently not in scipy.

References
----------
J.C. Finney, 'Optimization of a Skewed Logistic Distribution With Respect
to the Kolmogorov-Smirnov Test', A Dissertation Submitted to the Graduate
Faculty of the Louisiana State University and Agricultural and Mechanical
College, (August, 2004),
https://digitalcommons.lsu.edu/gradschool_dissertations/3672

J.R.M. Hosking, 'The four-parameter kappa distribution'. IBM J. Res.
Develop. 38 (3), 25 1-258 (1994).

B. Kumphon, A. Kaew-Man, P. Seenoi, 'A Rainfall Distribution for the Lampao
Site in the Chi River Basin, Thailand', Journal of Water Resource and
Protection, vol. 4, 866-869, (2012).
https://doi.org/10.4236/jwarp.2012.410101

C. Winchester, 'On Estimation of the Four-Parameter Kappa Distribution', A
Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March
2000).
http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ksone_gen : sig
type tag = [`Ksone_gen]
type t = [`Ksone_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
General Kolmogorov-Smirnov one-sided test.

This is the distribution of the one-sided Kolmogorov-Smirnov (KS)
statistics :math:`D_n^+` and :math:`D_n^-`
for a finite sample size ``n`` (the shape parameter).

%(before_notes)s

Notes
-----
:math:`D_n^+` and :math:`D_n^-` are given by

.. math::

    D_n^+ &= \text{sup}_x (F_n(x) - F(x)),\\
    D_n^- &= \text{sup}_x (F(x) - F_n(x)),\\

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `ksone`
describes the distribution under the null hypothesis of the KS test
that the empirical CDF corresponds to :math:`n` i.i.d. random variates
with CDF :math:`F`.

%(after_notes)s

See Also
--------
kstwobign, kstest

References
----------
.. [1] Birnbaum, Z. W. and Tingey, F.H. 'One-sided confidence contours
   for probability distribution functions', The Annals of Mathematical
   Statistics, 22(4), pp 592-596 (1951).

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Kstwobign_gen : sig
type tag = [`Kstwobign_gen]
type t = [`Kstwobign_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Kolmogorov-Smirnov two-sided test for large N.

This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov
statistic :math:`\sqrt{n} D_n` that measures the maximum absolute
distance of the theoretical CDF from the empirical CDF (see `kstest`).

%(before_notes)s

Notes
-----
:math:`\sqrt{n} D_n` is given by

.. math::

    D_n = \text{sup}_x |F_n(x) - F(x)|

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `kstwobign`
describes the asymptotic distribution (i.e. the limit of
:math:`\sqrt{n} D_n`) under the null hypothesis of the KS test that the
empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.

%(after_notes)s

See Also
--------
ksone, kstest

References
----------
.. [1] Marsaglia, G. et al. 'Evaluating Kolmogorov's distribution',
   Journal of Statistical Software, 8(18), 2003.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Laplace_gen : sig
type tag = [`Laplace_gen]
type t = [`Laplace_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Laplace continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `laplace` is

.. math::

    f(x) = \frac{1}{2} \exp(-|x|)

for a real number :math:`x`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Levy_gen : sig
type tag = [`Levy_gen]
type t = [`Levy_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Levy continuous random variable.

%(before_notes)s

See Also
--------
levy_stable, levy_l

Notes
-----
The probability density function for `levy` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp\left(-\frac{1}{2x}\right)

for :math:`x >= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=1`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Levy_l_gen : sig
type tag = [`Levy_l_gen]
type t = [`Levy_l_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A left-skewed Levy continuous random variable.

%(before_notes)s

See Also
--------
levy, levy_stable

Notes
-----
The probability density function for `levy_l` is:

.. math::
    f(x) = \frac{1}{ |x| \sqrt{2\pi |x| }} \exp{ \left(-\frac{1}{2|x| } \right)}

for :math:`x <= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=-1`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Levy_stable_gen : sig
type tag = [`Levy_stable_gen]
type t = [`Levy_stable_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Levy-stable continuous random variable.

%(before_notes)s

See Also
--------
levy, levy_l

Notes
-----
The distribution for `levy_stable` has characteristic function:

.. math::

    \varphi(t, \alpha, \beta, c, \mu) =
    e^{it\mu -|ct|^{\alpha}(1-i\beta \operatorname{sign}(t)\Phi(\alpha, t))}

where:

.. math::

    \Phi = \begin{cases}
            \tan \left({\frac {\pi \alpha }{2}}\right)&\alpha \neq 1\\
            -{\frac {2}{\pi }}\log |t|&\alpha =1
            \end{cases}

The probability density function for `levy_stable` is:

.. math::

    f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \varphi(t)e^{-ixt}\,dt

where :math:`-\infty < t < \infty`. This integral does not have a known closed form.

For evaluation of pdf we use either Zolotarev :math:`S_0` parameterization with integration,
direct integration of standard parameterization of characteristic function or FFT of
characteristic function. If set to other than None and if number of points is greater than
``levy_stable.pdf_fft_min_points_threshold`` (defaults to None) we use FFT otherwise we use one
of the other methods.

The default method is 'best' which uses Zolotarev's method if alpha = 1 and integration of
characteristic function otherwise. The default method can be changed by setting
``levy_stable.pdf_default_method`` to either 'zolotarev', 'quadrature' or 'best'.

To increase accuracy of FFT calculation one can specify ``levy_stable.pdf_fft_grid_spacing``
(defaults to 0.001) and ``pdf_fft_n_points_two_power`` (defaults to a value that covers the
input range * 4). Setting ``pdf_fft_n_points_two_power`` to 16 should be sufficiently accurate
in most cases at the expense of CPU time.

For evaluation of cdf we use Zolatarev :math:`S_0` parameterization with integration or integral of
the pdf FFT interpolated spline. The settings affecting FFT calculation are the same as
for pdf calculation. Setting the threshold to ``None`` (default) will disable FFT. For cdf
calculations the Zolatarev method is superior in accuracy, so FFT is disabled by default.

Fitting estimate uses quantile estimation method in [MC]. MLE estimation of parameters in
fit method uses this quantile estimate initially. Note that MLE doesn't always converge if
using FFT for pdf calculations; so it's best that ``pdf_fft_min_points_threshold`` is left unset.

.. warning::

    For pdf calculations implementation of Zolatarev is unstable for values where alpha = 1 and
    beta != 0. In this case the quadrature method is recommended. FFT calculation is also
    considered experimental.

    For cdf calculations FFT calculation is considered experimental. Use Zolatarev's method
    instead (default).

%(after_notes)s

References
----------
.. [MC] McCulloch, J., 1986. Simple consistent estimators of stable distribution parameters.
   Communications in Statistics - Simulation and Computation 15, 11091136.
.. [MS] Mittnik, S.T. Rachev, T. Doganoglu, D. Chenyao, 1999. Maximum likelihood estimation
   of stable Paretian models, Mathematical and Computer Modelling, Volume 29, Issue 10,
   1999, Pages 275-293.
.. [BS] Borak, S., Hardle, W., Rafal, W. 2005. Stable distributions, Economic Risk.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Loggamma_gen : sig
type tag = [`Loggamma_gen]
type t = [`Loggamma_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A log gamma continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `loggamma` is:

.. math::

    f(x, c) = \frac{\exp(c x - \exp(x))}
                   {\Gamma(c)}

for all :math:`x, c > 0`. Here, :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`loggamma` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Logistic_gen : sig
type tag = [`Logistic_gen]
type t = [`Logistic_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A logistic (or Sech-squared) continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `logistic` is:

.. math::

    f(x) = \frac{\exp(-x)}
                {(1+\exp(-x))^2}

`logistic` is a special case of `genlogistic` with ``c=1``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Loglaplace_gen : sig
type tag = [`Loglaplace_gen]
type t = [`Loglaplace_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A log-Laplace continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `loglaplace` is:

.. math::

    f(x, c) = \begin{cases}\frac{c}{2} x^{ c-1}  &\text{for } 0 < x < 1\\
                           \frac{c}{2} x^{-c-1}  &\text{for } x \ge 1
              \end{cases}

for :math:`c > 0`.

`loglaplace` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

References
----------
T.J. Kozubowski and K. Podgorski, 'A log-Laplace growth rate model',
The Mathematical Scientist, vol. 28, pp. 49-60, 2003.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Lognorm_gen : sig
type tag = [`Lognorm_gen]
type t = [`Lognorm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A lognormal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `lognorm` is:

.. math::

    f(x, s) = \frac{1}{s x \sqrt{2\pi}}
              \exp\left(-\frac{\log^2(x)}{2s^2}\right)

for :math:`x > 0`, :math:`s > 0`.

`lognorm` takes ``s`` as a shape parameter for :math:`s`.

%(after_notes)s

A common parametrization for a lognormal random variable ``Y`` is in
terms of the mean, ``mu``, and standard deviation, ``sigma``, of the
unique normally distributed random variable ``X`` such that exp(X) = Y.
This parametrization corresponds to setting ``s = sigma`` and ``scale =
exp(mu)``.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

When the location parameter is fixed by using the `floc` argument,
this function uses explicit formulas for the maximum likelihood
estimation of the log-normal shape and scale parameters, so the
`optimizer`, `loc` and `scale` keyword arguments are ignored.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Logser_gen : sig
type tag = [`Logser_gen]
type t = [`Logser_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Logarithmic (Log-Series, Series) discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `logser` is:

.. math::

    f(k) = - \frac{p^k}{k \log(1-p)}

for :math:`k \ge 1`.

`logser` takes :math:`p` as shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Lomax_gen : sig
type tag = [`Lomax_gen]
type t = [`Lomax_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Lomax (Pareto of the second kind) continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `lomax` is:

.. math::

    f(x, c) = \frac{c}{(1+x)^{c+1}}

for :math:`x \ge 0`, :math:`c > 0`.

`lomax` takes ``c`` as a shape parameter for :math:`c`.

`lomax` is a special case of `pareto` with ``loc=-1.0``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Maxwell_gen : sig
type tag = [`Maxwell_gen]
type t = [`Maxwell_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Maxwell continuous random variable.

%(before_notes)s

Notes
-----
A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,
and given ``scale = a``, where ``a`` is the parameter used in the
Mathworld description [1]_.

The probability density function for `maxwell` is:

.. math::

    f(x) = \sqrt{2/\pi}x^2 \exp(-x^2/2)

for :math:`x >= 0`.

%(after_notes)s

References
----------
.. [1] http://mathworld.wolfram.com/MaxwellDistribution.html

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Mielke_gen : sig
type tag = [`Mielke_gen]
type t = [`Mielke_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Mielke Beta-Kappa / Dagum continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `mielke` is:

.. math::

    f(x, k, s) = \frac{k x^{k-1}}{(1+x^s)^{1+k/s}}

for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes
called Dagum distribution ([2]_). It was already defined in [3]_, called
a Burr Type III distribution (`burr` with parameters ``c=s`` and
``d=k/s``).

`mielke` takes ``k`` and ``s`` as shape parameters.

%(after_notes)s

References
----------
.. [1] Mielke, P.W., 1973 'Another Family of Distributions for Describing
       and Analyzing Precipitation Data.' J. Appl. Meteor., 12, 275-280
.. [2] Dagum, C., 1977 'A new model for personal income distribution.'
       Economie Appliquee, 33, 327-367.
.. [3] Burr, I. W. 'Cumulative frequency functions', Annals of
       Mathematical Statistics, 13(2), pp 215-232 (1942).

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Moyal_gen : sig
type tag = [`Moyal_gen]
type t = [`Moyal_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Moyal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `moyal` is:

.. math::

    f(x) = \exp(-(x + \exp(-x))/2) / \sqrt{2\pi}

for a real number :math:`x`.

%(after_notes)s

This distribution has utility in high-energy physics and radiation
detection. It describes the energy loss of a charged relativistic
particle due to ionization of the medium [1]_. It also provides an
approximation for the Landau distribution. For an in depth description
see [2]_. For additional description, see [3]_.

References
----------
.. [1] J.E. Moyal, 'XXX. Theory of ionization fluctuations',
       The London, Edinburgh, and Dublin Philosophical Magazine
       and Journal of Science, vol 46, 263-280, (1955).
       :doi:`10.1080/14786440308521076` (gated)
.. [2] G. Cordeiro et al., 'The beta Moyal: a useful skew distribution',
       International Journal of Research and Reviews in Applied Sciences,
       vol 10, 171-192, (2012).
       http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf
.. [3] C. Walck, 'Handbook on Statistical Distributions for
       Experimentalists; International Report SUF-PFY/96-01', Chapter 26,
       University of Stockholm: Stockholm, Sweden, (2007).
       http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf

.. versionadded:: 1.1.0

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Nakagami_gen : sig
type tag = [`Nakagami_gen]
type t = [`Nakagami_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Nakagami continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `nakagami` is:

.. math::

    f(x, \nu) = \frac{2 \nu^\nu}{\Gamma(\nu)} x^{2\nu-1} \exp(-\nu x^2)

for :math:`x >= 0`, :math:`\nu > 0`.

`nakagami` takes ``nu`` as a shape parameter for :math:`\nu`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Nbinom_gen : sig
type tag = [`Nbinom_gen]
type t = [`Nbinom_gen | `Object | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A negative binomial discrete random variable.

%(before_notes)s

Notes
-----
Negative binomial distribution describes a sequence of i.i.d. Bernoulli
trials, repeated until a predefined, non-random number of successes occurs.

The probability mass function of the number of failures for `nbinom` is:

.. math::

   f(k) = \binom{k+n-1}{n-1} p^n (1-p)^k

for :math:`k \ge 0`.

`nbinom` takes :math:`n` and :math:`p` as shape parameters where n is the
number of successes, whereas p is the probability of a single success.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ncf_gen : sig
type tag = [`Ncf_gen]
type t = [`Ncf_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A non-central F distribution continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `ncf` is:

.. math::

    f(x, n_1, n_2, \lambda) =
                      \exp(\frac{\lambda}{2} + \lambda n_1 \frac{x}{2(n_1 x+n_2)})
                      n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\
                      (n_2+n_1 x)^{-(n_1+n_2)/2}
                      \gamma(n_1/2) \gamma(1+n_2/2) \\
                     \frac{L^{\frac{v_1}{2}-1}_{v_2/2}
                           (-\lambda v_1 \frac{x}{2(v_1 x+v_2)})}
                          {B(v_1/2, v_2/2)  \gamma(\frac{v_1+v_2}{2})}

for :math:`n_1 > 1`, :math:`n_2, \lambda > 0`.  Here :math:`n_1` is the
degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in
the denominator, :math:`\lambda` the non-centrality parameter,
:math:`\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a
generalized Laguerre polynomial and :math:`B` is the beta function.

`ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Nct_gen : sig
type tag = [`Nct_gen]
type t = [`Nct_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A non-central Student's t continuous random variable.

%(before_notes)s

Notes
-----
If :math:`Y` is a standard normal random variable and :math:`V` is
an independent chi-square random variable (`chi2`) with :math:`k` degrees
of freedom, then

.. math::

    X = \frac{Y + c}{\sqrt{V/k}}

has a non-central Student's t distribution on the real line.
The degrees of freedom parameter :math:`k` (denoted ``df`` in the
implementation) satisfies :math:`k > 0` and the noncentrality parameter
:math:`c` (denoted ``nc`` in the implementation) is a real number.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ncx2_gen : sig
type tag = [`Ncx2_gen]
type t = [`Ncx2_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A non-central chi-squared continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `ncx2` is:

.. math::

    f(x, k, \lambda) = \frac{1}{2} \exp(-(\lambda+x)/2)
        (x/\lambda)^{(k-2)/4}  I_{(k-2)/2}(\sqrt{\lambda x})

for :math:`x >= 0` and :math:`k, \lambda > 0`. :math:`k` specifies the
degrees of freedom (denoted ``df`` in the implementation) and
:math:`\lambda` is the non-centrality parameter (denoted ``nc`` in the
implementation). :math:`I_\nu` denotes the modified Bessel function of
first order of degree :math:`\nu` (`scipy.special.iv`).

`ncx2` takes ``df`` and ``nc`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Norm_gen : sig
type tag = [`Norm_gen]
type t = [`Norm_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A normal continuous random variable.

The location (``loc``) keyword specifies the mean.
The scale (``scale``) keyword specifies the standard deviation.

%(before_notes)s

Notes
-----
The probability density function for `norm` is:

.. math::

    f(x) = \frac{\exp(-x^2/2)}{\sqrt{2\pi}}

for a real number :math:`x`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This function uses explicit formulas for the maximum likelihood
estimation of the normal distribution parameters, so the
`optimizer` argument is ignored.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Norminvgauss_gen : sig
type tag = [`Norminvgauss_gen]
type t = [`Norminvgauss_gen | `Object | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Normal Inverse Gaussian continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `norminvgauss` is:

.. math::

    f(x, a, b) = (a \exp(\sqrt{a^2 - b^2} + b x)) /
                 (\pi \sqrt{1 + x^2} \, K_1(a \sqrt{1 + x^2}))

where :math:`x` is a real number, the parameter :math:`a` is the tail
heaviness and :math:`b` is the asymmetry parameter satisfying
:math:`a > 0` and :math:`|b| <= a`.
:math:`K_1` is the modified Bessel function of second kind
(`scipy.special.k1`).

%(after_notes)s

A normal inverse Gaussian random variable `Y` with parameters `a` and `b`
can be expressed as a normal mean-variance mixture:
`Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is
`invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used
to generate random variates.

References
----------
O. Barndorff-Nielsen, 'Hyperbolic Distributions and Distributions on
Hyperbolae', Scandinavian Journal of Statistics, Vol. 5(3),
pp. 151-157, 1978.

O. Barndorff-Nielsen, 'Normal Inverse Gaussian Distributions and Stochastic
Volatility Modelling', Scandinavian Journal of Statistics, Vol. 24,
pp. 1-13, 1997.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Pareto_gen : sig
type tag = [`Pareto_gen]
type t = [`Object | `Pareto_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Pareto continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `pareto` is:

.. math::

    f(x, b) = \frac{b}{x^{b+1}}

for :math:`x \ge 1`, :math:`b > 0`.

`pareto` takes ``b`` as a shape parameter for :math:`b`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Pearson3_gen : sig
type tag = [`Pearson3_gen]
type t = [`Object | `Pearson3_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A pearson type III continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `pearson3` is:

.. math::

    f(x, skew) = \frac{ |\beta| }{\Gamma(\alpha)}
                 (\beta (x - \zeta))^{\alpha - 1}
                 \exp(-\beta (x - \zeta))

where:

.. math::

        \beta = \frac{2}{skew  stddev}
        \alpha = (stddev \beta)^2
        \zeta = loc - \frac{\alpha}{\beta}

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).
`pearson3` takes ``skew`` as a shape parameter for :math:`skew`.

%(after_notes)s

%(example)s

References
----------
R.W. Vogel and D.E. McMartin, 'Probability Plot Goodness-of-Fit and
Skewness Estimation Procedures for the Pearson Type 3 Distribution', Water
Resources Research, Vol.27, 3149-3158 (1991).

L.R. Salvosa, 'Tables of Pearson's Type III Function', Ann. Math. Statist.,
Vol.1, 191-198 (1930).

'Using Modern Computing Tools to Fit the Pearson Type III Distribution to
Aviation Loads Data', Office of Aviation Research (2003).
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Planck_gen : sig
type tag = [`Planck_gen]
type t = [`Object | `Planck_gen | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Planck discrete exponential random variable.

%(before_notes)s

Notes
-----
The probability mass function for `planck` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k)

for :math:`k \ge 0` and :math:`\lambda > 0`.

`planck` takes :math:`\lambda` as shape parameter. The Planck distribution
can be written as a geometric distribution (`geom`) with
:math:`p = 1 - \exp(-\lambda)` shifted by `loc = -1`.

%(after_notes)s

See Also
--------
geom

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Poisson_gen : sig
type tag = [`Poisson_gen]
type t = [`Object | `Poisson_gen | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Poisson discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `poisson` is:

.. math::

    f(k) = \exp(-\mu) \frac{\mu^k}{k!}

for :math:`k \ge 0`.

`poisson` takes :math:`\mu` as shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Powerlaw_gen : sig
type tag = [`Powerlaw_gen]
type t = [`Object | `Powerlaw_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A power-function continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `powerlaw` is:

.. math::

    f(x, a) = a x^{a-1}

for :math:`0 \le x \le 1`, :math:`a > 0`.

`powerlaw` takes ``a`` as a shape parameter for :math:`a`.

%(after_notes)s

`powerlaw` is a special case of `beta` with ``b=1``.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Powerlognorm_gen : sig
type tag = [`Powerlognorm_gen]
type t = [`Object | `Powerlognorm_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A power log-normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `powerlognorm` is:

.. math::

    f(x, c, s) = \frac{c}{x s} \phi(\log(x)/s)
                 (\Phi(-\log(x)/s))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x > 0`, :math:`s, c > 0`.

`powerlognorm` takes :math:`c` and :math:`s` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Powernorm_gen : sig
type tag = [`Powernorm_gen]
type t = [`Object | `Powernorm_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A power normal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `powernorm` is:

.. math::

    f(x, c) = c \phi(x) (\Phi(-x))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x >= 0`, :math:`c > 0`.

`powernorm` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Randint_gen : sig
type tag = [`Randint_gen]
type t = [`Object | `Randint_gen | `Rv_discrete | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A uniform discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `randint` is:

.. math::

    f(k) = \frac{1}{high - low}

for ``k = low, ..., high - 1``.

`randint` takes ``low`` and ``high`` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rayleigh_gen : sig
type tag = [`Rayleigh_gen]
type t = [`Object | `Rayleigh_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Rayleigh continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `rayleigh` is:

.. math::

    f(x) = x \exp(-x^2/2)

for :math:`x \ge 0`.

`rayleigh` is a special case of `chi` with ``df=2``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rdist_gen : sig
type tag = [`Rdist_gen]
type t = [`Object | `Rdist_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
An R-distributed (symmetric beta) continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `rdist` is:

.. math::

    f(x, c) = \frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}

for :math:`-1 \le x \le 1`, :math:`c > 0`. `rdist` is also called the
symmetric beta distribution: if B has a `beta` distribution with
parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with
parameter c.

`rdist` takes ``c`` as a shape parameter for :math:`c`.

This distribution includes the following distribution kernels as
special cases::

    c = 2:  uniform
    c = 3:  `semicircular`
    c = 4:  Epanechnikov (parabolic)
    c = 6:  quartic (biweight)
    c = 8:  triweight

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Recipinvgauss_gen : sig
type tag = [`Recipinvgauss_gen]
type t = [`Object | `Recipinvgauss_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A reciprocal inverse Gaussian continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `recipinvgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2\pi x}}
                \exp\left(\frac{-(1-\mu x)^2}{2\mu^2x}\right)

for :math:`x \ge 0`.

`recipinvgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Reciprocal_gen : sig
type tag = [`Reciprocal_gen]
type t = [`Object | `Reciprocal_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A loguniform or reciprocal continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for this class is:

.. math::

    f(x, a, b) = \frac{1}{x \log(b/a)}

for :math:`a \le x \le b`, :math:`b > a > 0`. This class takes
:math:`a` and :math:`b` as shape parameters. %(after_notes)s

%(example)s

This doesn't show the equal probability of ``0.01``, ``0.1`` and
``1``. This is best when the x-axis is log-scaled:

>>> import numpy as np
>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log10(r))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$10^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()

This random variable will be log-uniform regardless of the base chosen for
``a`` and ``b``. Let's specify with base ``2`` instead:

>>> rvs = %(name)s(2**-2, 2**0).rvs(size=1000)

Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random
variable.  Here's the histogram:

>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log2(rvs))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$2^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rice_gen : sig
type tag = [`Rice_gen]
type t = [`Object | `Rice_gen | `Rv_continuous | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Rice continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `rice` is:

.. math::

    f(x, b) = x \exp(- \frac{x^2 + b^2}{2}) I_0(x b)

for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel
function of order zero (`scipy.special.i0`).

`rice` takes ``b`` as a shape parameter for :math:`b`.

%(after_notes)s

The Rice distribution describes the length, :math:`r`, of a 2-D vector with
components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,
v` are independent Gaussian random variables with standard deviation
:math:`s`.  Let :math:`R = \sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is
``rice.pdf(x, R/s, scale=s)``.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rv_frozen : sig
type tag = [`Rv_frozen]
type t = [`Object | `Rv_frozen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : ?kwds:(string * Py.Object.t) list -> dist:Py.Object.t -> Py.Object.t list -> t
(**
None
*)

val cdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val entropy : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val expect : ?func:Py.Object.t -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:Py.Object.t -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val interval : alpha:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val isf : q:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val logcdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val logpdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val logpmf : k:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val logsf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val mean : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val median : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val moment : n:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val pdf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val pmf : k:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val ppf : q:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val rvs : ?size:Py.Object.t -> ?random_state:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val sf : x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val stats : ?moments:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
None
*)

val std : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val support : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val var : [> tag] Obj.t -> Py.Object.t
(**
None
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Semicircular_gen : sig
type tag = [`Semicircular_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Semicircular_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A semicircular continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `semicircular` is:

.. math::

    f(x) = \frac{2}{\pi} \sqrt{1-x^2}

for :math:`-1 \le x \le 1`.

The distribution is a special case of `rdist` with `c = 3`.

%(after_notes)s

See Also
--------
rdist

References
----------
.. [1] 'Wigner semicircle distribution',
       https://en.wikipedia.org/wiki/Wigner_semicircle_distribution

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Skellam_gen : sig
type tag = [`Skellam_gen]
type t = [`Object | `Rv_discrete | `Rv_generic | `Skellam_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A  Skellam discrete random variable.

%(before_notes)s

Notes
-----
Probability distribution of the difference of two correlated or
uncorrelated Poisson random variables.

Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with
expected values :math:`\lambda_1` and :math:`\lambda_2`. Then,
:math:`k_1 - k_2` follows a Skellam distribution with parameters
:math:`\mu_1 = \lambda_1 - \rho \sqrt{\lambda_1 \lambda_2}` and
:math:`\mu_2 = \lambda_2 - \rho \sqrt{\lambda_1 \lambda_2}`, where
:math:`\rho` is the correlation coefficient between :math:`k_1` and
:math:`k_2`. If the two Poisson-distributed r.v. are independent then
:math:`\rho = 0`.

Parameters :math:`\mu_1` and :math:`\mu_2` must be strictly positive.

For details see: https://en.wikipedia.org/wiki/Skellam_distribution

`skellam` takes :math:`\mu_1` and :math:`\mu_2` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Skew_norm_gen : sig
type tag = [`Skew_norm_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Skew_norm_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A skew-normal random variable.

%(before_notes)s

Notes
-----
The pdf is::

    skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)

`skewnorm` takes a real number :math:`a` as a skewness parameter
When ``a = 0`` the distribution is identical to a normal distribution
(`norm`). `rvs` implements the method of [1]_.

%(after_notes)s

%(example)s

References
----------
.. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of the
    multivariate skew-normal distribution. J. Roy. Statist. Soc., B 61, 579-602.
    https://arxiv.org/abs/0911.2093
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module T_gen : sig
type tag = [`T_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `T_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Student's t continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `t` is:

.. math::

    f(x, \nu) = \frac{\Gamma((\nu+1)/2)}
                    {\sqrt{\pi \nu} \Gamma(\nu/2)}
                (1+x^2/\nu)^{-(\nu+1)/2}

where :math:`x` is a real number and the degrees of freedom parameter
:math:`\nu` (denoted ``df`` in the implementation) satisfies
:math:`\nu > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Trapz_gen : sig
type tag = [`Trapz_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Trapz_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A trapezoidal continuous random variable.

%(before_notes)s

Notes
-----
The trapezoidal distribution can be represented with an up-sloping line
from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``
and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.

`trapz` takes :math:`c` and :math:`d` as shape parameters.

%(after_notes)s

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Triang_gen : sig
type tag = [`Triang_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Triang_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A triangular continuous random variable.

%(before_notes)s

Notes
-----
The triangular distribution can be represented with an up-sloping line from
``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``
to ``(loc + scale)``.

`triang` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Truncexpon_gen : sig
type tag = [`Truncexpon_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Truncexpon_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A truncated exponential continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `truncexpon` is:

.. math::

    f(x, b) = \frac{\exp(-x)}{1 - \exp(-b)}

for :math:`0 <= x <= b`.

`truncexpon` takes ``b`` as a shape parameter for :math:`b`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Truncnorm_gen : sig
type tag = [`Truncnorm_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Truncnorm_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A truncated normal continuous random variable.

%(before_notes)s

Notes
-----
The standard form of this distribution is a standard normal truncated to
the range [a, b] --- notice that a and b are defined over the domain of the
standard normal.  To convert clip values for a specific mean and standard
deviation, use::

    a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std

`truncnorm` takes :math:`a` and :math:`b` as shape parameters.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Tukeylambda_gen : sig
type tag = [`Tukeylambda_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Tukeylambda_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Tukey-Lamdba continuous random variable.

%(before_notes)s

Notes
-----
A flexible distribution, able to represent and interpolate between the
following distributions:

- Cauchy                (:math:`lambda = -1`)
- logistic              (:math:`lambda = 0`)
- approx Normal         (:math:`lambda = 0.14`)
- uniform from -1 to 1  (:math:`lambda = 1`)

`tukeylambda` takes a real number :math:`lambda` (denoted ``lam``
in the implementation) as a shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Uniform_gen : sig
type tag = [`Uniform_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Uniform_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A uniform continuous random variable.

In the standard form, the distribution is uniform on ``[0, 1]``. Using
the parameters ``loc`` and ``scale``, one obtains the uniform distribution
on ``[loc, loc + scale]``.

%(before_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Maximum likelihood estimate for the location and scale parameters.

`uniform.fit` uses only the following parameters.  Because exact
formulas are used, the parameters related to optimization that are
available in the `fit` method of other distributions are ignored
here.  The only positional argument accepted is `data`.

Parameters
----------
data : array_like
    Data to use in calculating the maximum likelihood estimate.
floc : float, optional
    Hold the location parameter fixed to the specified value.
fscale : float, optional
    Hold the scale parameter fixed to the specified value.

Returns
-------
loc, scale : float
    Maximum likelihood estimates for the location and scale.

Notes
-----
An error is raised if `floc` is given and any values in `data` are
less than `floc`, or if `fscale` is given and `fscale` is less
than ``data.max() - data.min()``.  An error is also raised if both
`floc` and `fscale` are given.

Examples
--------
>>> from scipy.stats import uniform

We'll fit the uniform distribution to `x`:

>>> x = np.array([2, 2.5, 3.1, 9.5, 13.0])

For a uniform distribution MLE, the location is the minimum of the
data, and the scale is the maximum minus the minimum.

>>> loc, scale = uniform.fit(x)
>>> loc
2.0
>>> scale
11.0

If we know the data comes from a uniform distribution where the support
starts at 0, we can use `floc=0`:

>>> loc, scale = uniform.fit(x, floc=0)
>>> loc
0.0
>>> scale
13.0

Alternatively, if we know the length of the support is 12, we can use
`fscale=12`:

>>> loc, scale = uniform.fit(x, fscale=12)
>>> loc
1.5
>>> scale
12.0

In that last example, the support interval is [1.5, 13.5].  This
solution is not unique.  For example, the distribution with ``loc=2``
and ``scale=12`` has the same likelihood as the one above.  When
`fscale` is given and it is larger than ``data.max() - data.min()``,
the parameters returned by the `fit` method center the support over
the interval ``[data.min(), data.max()]``.
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Vonmises_gen : sig
type tag = [`Vonmises_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Vonmises_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Von Mises continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `vonmises` and `vonmises_line` is:

.. math::

    f(x, \kappa) = \frac{ \exp(\kappa \cos(x)) }{ 2 \pi I_0(\kappa) }

for :math:`-\pi \le x \le \pi`, :math:`\kappa > 0`. :math:`I_0` is the
modified Bessel function of order zero (`scipy.special.i0`).

`vonmises` is a circular distribution which does not restrict the
distribution to a fixed interval. Currently, there is no circular
distribution framework in scipy. The ``cdf`` is implemented such that
``cdf(x + 2*np.pi) == cdf(x) + 1``.

`vonmises_line` is the same distribution, defined on :math:`[-\pi, \pi]`
on the real line. This is a regular (i.e. non-circular) distribution.

`vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Wald_gen : sig
type tag = [`Wald_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Wald_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Wald continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `wald` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp(- \frac{ (x-1)^2 }{ 2x })

for :math:`x >= 0`.

`wald` is a special case of `invgauss` with ``mu=1``.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Weibull_max_gen : sig
type tag = [`Weibull_max_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Weibull_max_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Weibull maximum continuous random variable.

%(before_notes)s

See Also
--------
weibull_min

Notes
-----
The probability density function for `weibull_max` is:

.. math::

    f(x, c) = c (-x)^{c-1} \exp(-(-x)^c)

for :math:`x < 0`, :math:`c > 0`.

`weibull_max` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Weibull_min_gen : sig
type tag = [`Weibull_min_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Weibull_min_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
Weibull minimum continuous random variable.

The Weibull Minimum Extreme Value distribution, from extreme value theory,
is also often simply called the Weibull distribution.

%(before_notes)s

See Also
--------
weibull_max, numpy.random.mtrand.RandomState.weibull, exponweib

Notes
-----
The probability density function for `weibull_min` is:

.. math::

    f(x, c) = c x^{c-1} \exp(-x^c)

for :math:`x >= 0`, :math:`c > 0`.

`weibull_min` takes ``c`` as a shape parameter for :math:`c`.
(named :math:`k` in Wikipedia article and :math:`a` in
``numpy.random.weibull``).  Special shape values are :math:`c=1` and
:math:`c=2` where Weibull distribution reduces to the `expon` and
`rayleigh` distributions respectively.

%(after_notes)s

References
----------
https://en.wikipedia.org/wiki/Weibull_distribution

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Wrapcauchy_gen : sig
type tag = [`Wrapcauchy_gen]
type t = [`Object | `Rv_continuous | `Rv_generic | `Wrapcauchy_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_generic : t -> [`Rv_generic] Obj.t
val as_rv_continuous : t -> [`Rv_continuous] Obj.t
val create : ?momtype:Py.Object.t -> ?a:Py.Object.t -> ?b:Py.Object.t -> ?xtol:Py.Object.t -> ?badvalue:Py.Object.t -> ?name:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A wrapped Cauchy continuous random variable.

%(before_notes)s

Notes
-----
The probability density function for `wrapcauchy` is:

.. math::

    f(x, c) = \frac{1-c^2}{2\pi (1+c^2 - 2c \cos(x))}

for :math:`0 \le x \le 2\pi`, :math:`0 < c < 1`.

`wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `x`
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?scale:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?kwds:(string * Py.Object.t) list -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the
distribution by numerical integration.

The expected value of a function ``f(x)`` with respect to a
distribution ``dist`` is defined as::

            ub
    E[f(x)] = Integral(f(x) * dist.pdf(x)),
            lb

where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``
distribution. If the bounds ``lb`` and ``ub`` correspond to the
support of the distribution, e.g. ``[-inf, inf]`` in the default
case, then the integral is the unrestricted expectation of ``f(x)``.
Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``
outside a finite interval in which case the expectation is
calculated within the finite range ``[lb, ub]``.

Parameters
----------
func : callable, optional
    Function for which integral is calculated. Takes only one argument.
    The default is the identity mapping f(x) = x.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter (default=0).
scale : float, optional
    Scale parameter (default=1).
lb, ub : scalar, optional
    Lower and upper bound for integration. Default is set to the
    support of the distribution.
conditional : bool, optional
    If True, the integral is corrected by the conditional probability
    of the integration interval.  The return value is the expectation
    of the function, conditional on being in the given interval.
    Default is False.

Additional keyword arguments are passed to the integration routine.

Returns
-------
expect : float
    The calculated expected value.

Notes
-----
The integration behavior of this function is inherited from
`scipy.integrate.quad`. Neither this function nor
`scipy.integrate.quad` can verify whether the integral exists or is
finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and
``cauchy(0).expect()`` returns ``0.0``.

Examples
--------

To understand the effect of the bounds of integration consider

>>> from scipy.stats import expon
>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)
0.6321205588285578

This is close to

>>> expon(1).cdf(2.0) - expon(1).cdf(0.0)
0.6321205588285577

If ``conditional=True``

>>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)
1.0000000000000002

The slight deviation from 1 is due to numerical integration.
*)

val fit : ?kwds:(string * Py.Object.t) list -> data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return MLEs for shape (if applicable), location, and scale
parameters from data.

MLE stands for Maximum Likelihood Estimate.  Starting estimates for
the fit are given by input arguments; for any arguments not provided
with starting estimates, ``self._fitstart(data)`` is called to generate
such.

One can hold some parameters fixed to specific values by passing in
keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)
and ``floc`` and ``fscale`` (for location and scale parameters,
respectively).

Parameters
----------
data : array_like
    Data to use in calculating the MLEs.
args : floats, optional
    Starting value(s) for any shape-characterizing arguments (those not
    provided will be determined by a call to ``_fitstart(data)``).
    No default value.
kwds : floats, optional
    Starting values for the location and scale parameters; no default.
    Special keyword arguments are recognized as holding certain
    parameters fixed:

    - f0...fn : hold respective shape parameters fixed.
      Alternatively, shape parameters to fix can be specified by name.
      For example, if ``self.shapes == 'a, b'``, ``fa``and ``fix_a``
      are equivalent to ``f0``, and ``fb`` and ``fix_b`` are
      equivalent to ``f1``.

    - floc : hold location parameter fixed to specified value.

    - fscale : hold scale parameter fixed to specified value.

    - optimizer : The optimizer to use.  The optimizer must take ``func``,
      and starting position as the first two arguments,
      plus ``args`` (for extra arguments to pass to the
      function to be optimized) and ``disp=0`` to suppress
      output as keyword arguments.

Returns
-------
mle_tuple : tuple of floats
    MLEs for any shape parameters (if applicable), followed by those
    for location and scale. For most random variables, shape statistics
    will be returned, but there are exceptions (e.g. ``norm``).

Notes
-----
This fit is computed by maximizing a log-likelihood function, with
penalty applied for samples outside of range of the distribution. The
returned answer is not guaranteed to be the globally optimal MLE, it
may only be locally optimal, or the optimization may fail altogether.
If the data contain any of np.nan, np.inf, or -np.inf, the fit routine
will throw a RuntimeError.

Examples
--------

Generate some data to fit: draw random variates from the `beta`
distribution

>>> from scipy.stats import beta
>>> a, b = 1., 2.
>>> x = beta.rvs(a, b, size=1000)

Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):

>>> a1, b1, loc1, scale1 = beta.fit(x)

We can also use some prior knowledge about the dataset: let's keep
``loc`` and ``scale`` fixed:

>>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)
>>> loc1, scale1
(0, 1)

We can also keep shape parameters fixed by using ``f``-keywords. To
keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,
equivalently, ``fa=1``:

>>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)
>>> a1
1

Not all distributions return estimates for the shape parameters.
``norm`` for example just returns estimates for location and scale:

>>> from scipy.stats import norm
>>> x = norm.rvs(a, b, size=1000, random_state=123)
>>> loc1, scale1 = norm.fit(x)
>>> loc1, scale1
(0.92087172783841631, 2.0015750750324668)
*)

val fit_loc_scale : data:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> (float * float)
(**
Estimate loc and scale parameters from data using 1st and 2nd moments.

Parameters
----------
data : array_like
    Data to fit.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).

Returns
-------
Lhat : float
    Estimated location parameter for the data.
Shat : float
    Estimated scale parameter for the data.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    upper tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : ndarray or scalar
    Quantile corresponding to the upper tail probability q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at x
*)

val logpdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability density function at x of the given RV.

This uses a more numerically accurate calculation if available.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logpdf : array_like
    Log of the probability density function evaluated at x
*)

val logsf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as (1 - `cdf`),
evaluated at `x`.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `x`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val nnlf : theta:Py.Object.t -> x:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return negative loglikelihood function.

Notes
-----
This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the
parameters (including loc and scale).
*)

val pdf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability density function at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
pdf : ndarray
    Probability density function evaluated at x
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    lower tail probability
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
x : array_like
    quantile corresponding to the lower tail probability q.
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> x:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at x of the given RV.

Parameters
----------
x : array_like
    quantiles
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
sf : array_like
    Survival function evaluated at x
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Yulesimon_gen : sig
type tag = [`Yulesimon_gen]
type t = [`Object | `Rv_discrete | `Rv_generic | `Yulesimon_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Yule-Simon discrete random variable.

%(before_notes)s

Notes
-----

The probability mass function for the `yulesimon` is:

.. math::

    f(k) =  \alpha B(k, \alpha+1)

for :math:`k=1,2,3,...`, where :math:`\alpha>0`.
Here :math:`B` refers to the `scipy.special.beta` function.

The sampling of random variates is based on pg 553, Section 6.3 of [1]_.
Our notation maps to the referenced logic via :math:`\alpha=a-1`.

For details see the wikipedia entry [2]_.

References
----------
.. [1] Devroye, Luc. 'Non-uniform Random Variate Generation',
     (1986) Springer, New York.

.. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Zipf_gen : sig
type tag = [`Zipf_gen]
type t = [`Object | `Rv_discrete | `Rv_generic | `Zipf_gen] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val as_rv_discrete : t -> [`Rv_discrete] Obj.t
val as_rv_generic : t -> [`Rv_generic] Obj.t
val create : ?a:Py.Object.t -> ?b:Py.Object.t -> ?name:Py.Object.t -> ?badvalue:Py.Object.t -> ?moment_tol:Py.Object.t -> ?values:Py.Object.t -> ?inc:Py.Object.t -> ?longname:Py.Object.t -> ?shapes:Py.Object.t -> ?extradoc:Py.Object.t -> ?seed:Py.Object.t -> unit -> t
(**
A Zipf discrete random variable.

%(before_notes)s

Notes
-----
The probability mass function for `zipf` is:

.. math::

    f(k, a) = \frac{1}{\zeta(a) k^a}

for :math:`k \ge 1`.

`zipf` takes :math:`a` as shape parameter. :math:`\zeta` is the
Riemann zeta function (`scipy.special.zeta`)

%(after_notes)s

%(example)s
*)

val cdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Cumulative distribution function of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
cdf : ndarray
    Cumulative distribution function evaluated at `k`.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val expect : ?func:Py.Object.t -> ?args:Py.Object.t -> ?loc:float -> ?lb:Py.Object.t -> ?ub:Py.Object.t -> ?conditional:bool -> ?maxcount:int -> ?tolerance:float -> ?chunksize:int -> [> tag] Obj.t -> float
(**
Calculate expected value of a function with respect to the distribution
for discrete distribution by numerical summation.

Parameters
----------
func : callable, optional
    Function for which the expectation value is calculated.
    Takes only one argument.
    The default is the identity mapping f(k) = k.
args : tuple, optional
    Shape parameters of the distribution.
loc : float, optional
    Location parameter.
    Default is 0.
lb, ub : int, optional
    Lower and upper bound for the summation, default is set to the
    support of the distribution, inclusive (``ul <= k <= ub``).
conditional : bool, optional
    If true then the expectation is corrected by the conditional
    probability of the summation interval. The return value is the
    expectation of the function, `func`, conditional on being in
    the given interval (k such that ``ul <= k <= ub``).
    Default is False.
maxcount : int, optional
    Maximal number of terms to evaluate (to avoid an endless loop for
    an infinite sum). Default is 1000.
tolerance : float, optional
    Absolute tolerance for the summation. Default is 1e-10.
chunksize : int, optional
    Iterate over the support of a distributions in chunks of this size.
    Default is 32.

Returns
-------
expect : float
    Expected value.

Notes
-----
For heavy-tailed distributions, the expected value may or may not exist,
depending on the function, `func`. If it does exist, but the sum converges
slowly, the accuracy of the result may be rather low. For instance, for
``zipf(4)``, accuracy for mean, variance in example is only 1e-5.
increasing `maxcount` and/or `chunksize` may improve the result, but may
also make zipf very slow.

The function is not vectorized.
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val generic_moment : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Non-central moment of discrete distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val isf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Inverse survival function (inverse of `sf`) at q of the given RV.

Parameters
----------
q : array_like
    Upper tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : ndarray or scalar
    Quantile corresponding to the upper tail probability, q.
*)

val logcdf : ?kwds:(string * Py.Object.t) list -> k:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the cumulative distribution function at k of the given RV.

Parameters
----------
k : array_like, int
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logcdf : array_like
    Log of the cumulative distribution function evaluated at k.
*)

val logpmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter. Default is 0.

Returns
-------
logpmf : array_like
    Log of the probability mass function evaluated at k.
*)

val logsf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Log of the survival function of the given RV.

Returns the log of the 'survival function,' defined as 1 - `cdf`,
evaluated at `k`.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
logsf : ndarray
    Log of the survival function evaluated at `k`.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val pmf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Probability mass function at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
pmf : array_like
    Probability mass function evaluated at k
*)

val ppf : ?kwds:(string * Py.Object.t) list -> q:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Percent point function (inverse of `cdf`) at q of the given RV.

Parameters
----------
q : array_like
    Lower tail probability.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
k : array_like
    Quantile corresponding to the lower tail probability, q.
*)

val rvs : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
size : int or tuple of ints, optional
    Defining number of random variates (Default is 1).  Note that `size`
    has to be given as keyword, not as positional argument.
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val sf : ?kwds:(string * Py.Object.t) list -> k:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Survival function (1 - `cdf`) at k of the given RV.

Parameters
----------
k : array_like
    Quantiles.
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).

Returns
-------
sf : array_like
    Survival function evaluated at k.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

val alpha : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Alpha_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An alpha continuous random variable.

As an instance of the `rv_continuous` class, `alpha` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `alpha` ([1]_, [2]_) is:

.. math::

    f(x, a) = \frac{1}{x^2 \Phi(a) \sqrt{2\pi}} *
              \exp(-\frac{1}{2} (a-1/x)^2)

where :math:`\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.

`alpha` takes ``a`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``alpha.pdf(x, a, loc, scale)`` is identically
equivalent to ``alpha.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Johnson, Kotz, and Balakrishnan, 'Continuous Univariate
       Distributions, Volume 1', Second Edition, John Wiley and Sons,
       p. 173 (1994).
.. [2] Anthony A. Salvia, 'Reliability applications of the Alpha
       Distribution', IEEE Transactions on Reliability, Vol. R-34,
       No. 3, pp. 251-252 (1985).

Examples
--------
>>> from scipy.stats import alpha
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 3.57
>>> mean, var, skew, kurt = alpha.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(alpha.ppf(0.01, a),
...                 alpha.ppf(0.99, a), 100)
>>> ax.plot(x, alpha.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='alpha pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = alpha(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = alpha.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], alpha.cdf(vals, a))
True

Generate random numbers:

>>> r = alpha.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val anglit : ?loc:float -> ?scale:float -> unit -> [`Anglit_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An anglit continuous random variable.

As an instance of the `rv_continuous` class, `anglit` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `anglit` is:

.. math::

    f(x) = \sin(2x + \pi/2) = \cos(2x)

for :math:`-\pi/4 \le x \le \pi/4`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``anglit.pdf(x, loc, scale)`` is identically
equivalent to ``anglit.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import anglit
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = anglit.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(anglit.ppf(0.01),
...                 anglit.ppf(0.99), 100)
>>> ax.plot(x, anglit.pdf(x),
...        'r-', lw=5, alpha=0.6, label='anglit pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = anglit()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = anglit.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], anglit.cdf(vals))
True

Generate random numbers:

>>> r = anglit.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val arcsine : ?loc:float -> ?scale:float -> unit -> [`Arcsine_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An arcsine continuous random variable.

As an instance of the `rv_continuous` class, `arcsine` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `arcsine` is:

.. math::

    f(x) = \frac{1}{\pi \sqrt{x (1-x)}}

for :math:`0 < x < 1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``arcsine.pdf(x, loc, scale)`` is identically
equivalent to ``arcsine.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import arcsine
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = arcsine.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(arcsine.ppf(0.01),
...                 arcsine.ppf(0.99), 100)
>>> ax.plot(x, arcsine.pdf(x),
...        'r-', lw=5, alpha=0.6, label='arcsine pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = arcsine()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = arcsine.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], arcsine.cdf(vals))
True

Generate random numbers:

>>> r = arcsine.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val argus : ?loc:float -> ?scale:float -> chi:Py.Object.t -> unit -> [`Argus_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Argus distribution

As an instance of the `rv_continuous` class, `argus` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(chi, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, chi, loc=0, scale=1)
    Probability density function.
logpdf(x, chi, loc=0, scale=1)
    Log of the probability density function.
cdf(x, chi, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, chi, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, chi, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, chi, loc=0, scale=1)
    Log of the survival function.
ppf(q, chi, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, chi, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, chi, loc=0, scale=1)
    Non-central moment of order n
stats(chi, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(chi, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, chi, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(chi,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(chi, loc=0, scale=1)
    Median of the distribution.
mean(chi, loc=0, scale=1)
    Mean of the distribution.
var(chi, loc=0, scale=1)
    Variance of the distribution.
std(chi, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, chi, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `argus` is:

.. math::

    f(x, \chi) = \frac{\chi^3}{\sqrt{2\pi} \Psi(\chi)} x \sqrt{1-x^2}
                 \exp(-\chi^2 (1 - x^2)/2)

for :math:`0 < x < 1`, where

.. math::

    \Psi(\chi) = \Phi(\chi) - \chi \phi(\chi) - 1/2

with :math:`\Phi` and :math:`\phi` being the CDF and PDF of a standard
normal distribution, respectively.

`argus` takes :math:`\chi` as shape a parameter.

References
----------

.. [1] 'ARGUS distribution',
       https://en.wikipedia.org/wiki/ARGUS_distribution

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``argus.pdf(x, chi, loc, scale)`` is identically
equivalent to ``argus.pdf(y, chi) / scale`` with
``y = (x - loc) / scale``.

.. versionadded:: 0.19.0

Examples
--------
>>> from scipy.stats import argus
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> chi = 1
>>> mean, var, skew, kurt = argus.stats(chi, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(argus.ppf(0.01, chi),
...                 argus.ppf(0.99, chi), 100)
>>> ax.plot(x, argus.pdf(x, chi),
...        'r-', lw=5, alpha=0.6, label='argus pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = argus(chi)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = argus.ppf([0.001, 0.5, 0.999], chi)
>>> np.allclose([0.001, 0.5, 0.999], argus.cdf(vals, chi))
True

Generate random numbers:

>>> r = argus.rvs(chi, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val bernoulli : ?loc:float -> p:Py.Object.t -> unit -> [`Bernoulli_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Bernoulli discrete random variable.

As an instance of the `rv_discrete` class, `bernoulli` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `bernoulli` is:

.. math::

   f(k) = \begin{cases}1-p  &\text{if } k = 0\\
                       p    &\text{if } k = 1\end{cases}

for :math:`k` in :math:`\{0, 1\}`.

`bernoulli` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``bernoulli.pmf(k, p, loc)`` is identically
equivalent to ``bernoulli.pmf(k - loc, p)``.

Examples
--------
>>> from scipy.stats import bernoulli
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.3
>>> mean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(bernoulli.ppf(0.01, p),
...               bernoulli.ppf(0.99, p))
>>> ax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')
>>> ax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = bernoulli(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = bernoulli.cdf(x, p)
>>> np.allclose(x, bernoulli.ppf(prob, p))
True

Generate random numbers:

>>> r = bernoulli.rvs(p, size=1000)
*)

val beta : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Beta_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A beta continuous random variable.

As an instance of the `rv_continuous` class, `beta` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `beta` is:

.. math::

    f(x, a, b) = \frac{\Gamma(a+b) x^{a-1} (1-x)^{b-1}}
                      {\Gamma(a) \Gamma(b)}

for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`beta` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``beta.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import beta
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 2.31, 0.627
>>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(beta.ppf(0.01, a, b),
...                 beta.ppf(0.99, a, b), 100)
>>> ax.plot(x, beta.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='beta pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = beta(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))
True

Generate random numbers:

>>> r = beta.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val betabinom : ?loc:float -> n:Py.Object.t -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Betabinom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A beta-binomial discrete random variable.

As an instance of the `rv_discrete` class, `betabinom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, a, b, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, a, b, loc=0)
    Probability mass function.
logpmf(k, n, a, b, loc=0)
    Log of the probability mass function.
cdf(k, n, a, b, loc=0)
    Cumulative distribution function.
logcdf(k, n, a, b, loc=0)
    Log of the cumulative distribution function.
sf(k, n, a, b, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, a, b, loc=0)
    Log of the survival function.
ppf(q, n, a, b, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, a, b, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, a, b, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, a, b, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, a, b, loc=0)
    Median of the distribution.
mean(n, a, b, loc=0)
    Mean of the distribution.
var(n, a, b, loc=0)
    Variance of the distribution.
std(n, a, b, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, a, b, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The beta-binomial distribution is a binomial distribution with a
probability of success `p` that follows a beta distribution.

The probability mass function for `betabinom` is:

.. math::

   f(k) = \binom{n}{k} \frac{B(k + a, n - k + b)}{B(a, b)}

for ``k`` in ``{0, 1,..., n}``, :math:`n \geq 0`, :math:`a > 0`,
:math:`b > 0`, where :math:`B(a, b)` is the beta function.

`betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.

References
----------
.. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``betabinom.pmf(k, n, a, b, loc)`` is identically
equivalent to ``betabinom.pmf(k - loc, n, a, b)``.

.. versionadded:: 1.4.0

See Also
--------
beta, binom

Examples
--------
>>> from scipy.stats import betabinom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, a, b = 5, 2.3, 0.63
>>> mean, var, skew, kurt = betabinom.stats(n, a, b, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(betabinom.ppf(0.01, n, a, b),
...               betabinom.ppf(0.99, n, a, b))
>>> ax.plot(x, betabinom.pmf(x, n, a, b), 'bo', ms=8, label='betabinom pmf')
>>> ax.vlines(x, 0, betabinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = betabinom(n, a, b)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = betabinom.cdf(x, n, a, b)
>>> np.allclose(x, betabinom.ppf(prob, n, a, b))
True

Generate random numbers:

>>> r = betabinom.rvs(n, a, b, size=1000)
*)

val betaprime : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Betaprime_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A beta prime continuous random variable.

As an instance of the `rv_continuous` class, `betaprime` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `betaprime` is:

.. math::

    f(x, a, b) = \frac{x^{a-1} (1+x)^{-a-b}}{\beta(a, b)}

for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where
:math:`\beta(a, b)` is the beta function (see `scipy.special.beta`).

`betaprime` takes ``a`` and ``b`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``betaprime.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``betaprime.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import betaprime
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 5, 6
>>> mean, var, skew, kurt = betaprime.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(betaprime.ppf(0.01, a, b),
...                 betaprime.ppf(0.99, a, b), 100)
>>> ax.plot(x, betaprime.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='betaprime pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = betaprime(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = betaprime.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], betaprime.cdf(vals, a, b))
True

Generate random numbers:

>>> r = betaprime.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val binom : ?loc:float -> n:Py.Object.t -> p:Py.Object.t -> unit -> [`Binom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A binomial discrete random variable.

As an instance of the `rv_discrete` class, `binom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, p, loc=0)
    Probability mass function.
logpmf(k, n, p, loc=0)
    Log of the probability mass function.
cdf(k, n, p, loc=0)
    Cumulative distribution function.
logcdf(k, n, p, loc=0)
    Log of the cumulative distribution function.
sf(k, n, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, p, loc=0)
    Log of the survival function.
ppf(q, n, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, p, loc=0)
    Median of the distribution.
mean(n, p, loc=0)
    Mean of the distribution.
var(n, p, loc=0)
    Variance of the distribution.
std(n, p, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `binom` is:

.. math::

   f(k) = \binom{n}{k} p^k (1-p)^{n-k}

for ``k`` in ``{0, 1,..., n}``.

`binom` takes ``n`` and ``p`` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``binom.pmf(k, n, p, loc)`` is identically
equivalent to ``binom.pmf(k - loc, n, p)``.

Examples
--------
>>> from scipy.stats import binom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, p = 5, 0.4
>>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(binom.ppf(0.01, n, p),
...               binom.ppf(0.99, n, p))
>>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
>>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = binom(n, p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = binom.cdf(x, n, p)
>>> np.allclose(x, binom.ppf(prob, n, p))
True

Generate random numbers:

>>> r = binom.rvs(n, p, size=1000)
*)

val boltzmann : ?loc:float -> lambda_:Py.Object.t -> n:Py.Object.t -> unit -> [`Boltzmann_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Boltzmann (Truncated Discrete Exponential) random variable.

As an instance of the `rv_discrete` class, `boltzmann` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lambda_, N, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, lambda_, N, loc=0)
    Probability mass function.
logpmf(k, lambda_, N, loc=0)
    Log of the probability mass function.
cdf(k, lambda_, N, loc=0)
    Cumulative distribution function.
logcdf(k, lambda_, N, loc=0)
    Log of the cumulative distribution function.
sf(k, lambda_, N, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, lambda_, N, loc=0)
    Log of the survival function.
ppf(q, lambda_, N, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lambda_, N, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(lambda_, N, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lambda_, N, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(lambda_, N), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(lambda_, N, loc=0)
    Median of the distribution.
mean(lambda_, N, loc=0)
    Mean of the distribution.
var(lambda_, N, loc=0)
    Variance of the distribution.
std(lambda_, N, loc=0)
    Standard deviation of the distribution.
interval(alpha, lambda_, N, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `boltzmann` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k) / (1-\exp(-\lambda N))

for :math:`k = 0,..., N-1`.

`boltzmann` takes :math:`\lambda > 0` and :math:`N > 0` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``boltzmann.pmf(k, lambda_, N, loc)`` is identically
equivalent to ``boltzmann.pmf(k - loc, lambda_, N)``.

Examples
--------
>>> from scipy.stats import boltzmann
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lambda_, N = 1.4, 19
>>> mean, var, skew, kurt = boltzmann.stats(lambda_, N, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(boltzmann.ppf(0.01, lambda_, N),
...               boltzmann.ppf(0.99, lambda_, N))
>>> ax.plot(x, boltzmann.pmf(x, lambda_, N), 'bo', ms=8, label='boltzmann pmf')
>>> ax.vlines(x, 0, boltzmann.pmf(x, lambda_, N), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = boltzmann(lambda_, N)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = boltzmann.cdf(x, lambda_, N)
>>> np.allclose(x, boltzmann.ppf(prob, lambda_, N))
True

Generate random numbers:

>>> r = boltzmann.rvs(lambda_, N, size=1000)
*)

val bradford : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Bradford_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Bradford continuous random variable.

As an instance of the `rv_continuous` class, `bradford` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `bradford` is:

.. math::

    f(x, c) = \frac{c}{\log(1+c) (1+cx)}

for :math:`0 <= x <= 1` and :math:`c > 0`.

`bradford` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``bradford.pdf(x, c, loc, scale)`` is identically
equivalent to ``bradford.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import bradford
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.299
>>> mean, var, skew, kurt = bradford.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(bradford.ppf(0.01, c),
...                 bradford.ppf(0.99, c), 100)
>>> ax.plot(x, bradford.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='bradford pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = bradford(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = bradford.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], bradford.cdf(vals, c))
True

Generate random numbers:

>>> r = bradford.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val burr : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Burr_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Burr (Type III) continuous random variable.

As an instance of the `rv_continuous` class, `burr` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr12 : Burr Type XII distribution
mielke : Mielke Beta-Kappa / Dagum distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{-c - 1} / (1 + x^{-c})^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr` takes :math:`c` and :math:`d` as shape parameters.

This is the PDF corresponding to the third CDF given in Burr's list;
specifically, it is equation (11) in Burr's paper [1]_. The distribution
is also commonly referred to as the Dagum distribution [2]_. If the
parameter :math:`c < 1` then the mean of the distribution does not
exist and if :math:`c < 2` the variance does not exist [2]_.
The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``burr.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``burr.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).
.. [2] https://en.wikipedia.org/wiki/Dagum_distribution
.. [3] Kleiber, Christian. 'A guide to the Dagum distributions.'
   Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).

Examples
--------
>>> from scipy.stats import burr
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 10.5, 4.3
>>> mean, var, skew, kurt = burr.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(burr.ppf(0.01, c, d),
...                 burr.ppf(0.99, c, d), 100)
>>> ax.plot(x, burr.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='burr pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = burr(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = burr.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], burr.cdf(vals, c, d))
True

Generate random numbers:

>>> r = burr.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val burr12 : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Burr12_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Burr (Type XII) continuous random variable.

As an instance of the `rv_continuous` class, `burr12` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr : Burr Type III distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{c-1} / (1 + x^c)^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`
and :math:`d`.

This is the PDF corresponding to the twelfth CDF given in Burr's list;
specifically, it is equation (20) in Burr's paper [1]_.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``burr12.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``burr12.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

The Burr type 12 distribution is also sometimes referred to as
the Singh-Maddala distribution from NIST [2]_.

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).

.. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm

.. [3] 'Burr distribution',
   https://en.wikipedia.org/wiki/Burr_distribution

Examples
--------
>>> from scipy.stats import burr12
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 10, 4
>>> mean, var, skew, kurt = burr12.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(burr12.ppf(0.01, c, d),
...                 burr12.ppf(0.99, c, d), 100)
>>> ax.plot(x, burr12.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='burr12 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = burr12(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = burr12.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], burr12.cdf(vals, c, d))
True

Generate random numbers:

>>> r = burr12.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val cauchy : ?loc:float -> ?scale:float -> unit -> [`Cauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `cauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `cauchy` is

.. math::

    f(x) = \frac{1}{\pi (1 + x^2)}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``cauchy.pdf(x, loc, scale)`` is identically
equivalent to ``cauchy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import cauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = cauchy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(cauchy.ppf(0.01),
...                 cauchy.ppf(0.99), 100)
>>> ax.plot(x, cauchy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='cauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = cauchy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = cauchy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], cauchy.cdf(vals))
True

Generate random numbers:

>>> r = cauchy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val chi : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Chi_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A chi continuous random variable.

As an instance of the `rv_continuous` class, `chi` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `chi` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2-1} \Gamma \left( k/2 \right)}
               x^{k-1} \exp \left( -x^2/2 \right)

for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation). :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

Special cases of `chi` are:

    - ``chi(1, loc, scale)`` is equivalent to `halfnorm`
    - ``chi(2, 0, scale)`` is equivalent to `rayleigh`
    - ``chi(3, 0, scale)`` is equivalent to `maxwell`

`chi` takes ``df`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``chi.pdf(x, df, loc, scale)`` is identically
equivalent to ``chi.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import chi
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 78
>>> mean, var, skew, kurt = chi.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(chi.ppf(0.01, df),
...                 chi.ppf(0.99, df), 100)
>>> ax.plot(x, chi.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='chi pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = chi(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = chi.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], chi.cdf(vals, df))
True

Generate random numbers:

>>> r = chi.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val chi2 : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Chi2_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A chi-squared continuous random variable.

As an instance of the `rv_continuous` class, `chi2` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `chi2` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2} \Gamma \left( k/2 \right)}
               x^{k/2-1} \exp \left( -x/2 \right)

for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation).

`chi2` takes ``df`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``chi2.pdf(x, df, loc, scale)`` is identically
equivalent to ``chi2.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import chi2
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 55
>>> mean, var, skew, kurt = chi2.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(chi2.ppf(0.01, df),
...                 chi2.ppf(0.99, df), 100)
>>> ax.plot(x, chi2.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='chi2 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = chi2(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = chi2.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df))
True

Generate random numbers:

>>> r = chi2.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val cosine : ?loc:float -> ?scale:float -> unit -> [`Cosine_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A cosine continuous random variable.

As an instance of the `rv_continuous` class, `cosine` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The cosine distribution is an approximation to the normal distribution.
The probability density function for `cosine` is:

.. math::

    f(x) = \frac{1}{2\pi} (1+\cos(x))

for :math:`-\pi \le x \le \pi`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``cosine.pdf(x, loc, scale)`` is identically
equivalent to ``cosine.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import cosine
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = cosine.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(cosine.ppf(0.01),
...                 cosine.ppf(0.99), 100)
>>> ax.plot(x, cosine.pdf(x),
...        'r-', lw=5, alpha=0.6, label='cosine pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = cosine()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = cosine.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], cosine.cdf(vals))
True

Generate random numbers:

>>> r = cosine.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val crystalball : ?loc:float -> ?scale:float -> beta:Py.Object.t -> m:Py.Object.t -> unit -> [`Crystalball_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Crystalball distribution

As an instance of the `rv_continuous` class, `crystalball` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, m, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, m, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, m, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, m, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, m, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, m, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, m, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, m, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, m, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, m, loc=0, scale=1)
    Non-central moment of order n
stats(beta, m, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, m, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, m, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta, m), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, m, loc=0, scale=1)
    Median of the distribution.
mean(beta, m, loc=0, scale=1)
    Mean of the distribution.
var(beta, m, loc=0, scale=1)
    Variance of the distribution.
std(beta, m, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, m, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `crystalball` is:

.. math::

    f(x, \beta, m) =  \begin{cases}
                        N \exp(-x^2 / 2),  &\text{for } x > -\beta\\
                        N A (B - x)^{-m}  &\text{for } x \le -\beta
                      \end{cases}

where :math:`A = (m / |\beta|)^n  \exp(-\beta^2 / 2)`,
:math:`B = m/|\beta| - |\beta|` and :math:`N` is a normalisation constant.

`crystalball` takes :math:`\beta > 0` and :math:`m > 1` as shape
parameters.  :math:`\beta` defines the point where the pdf changes
from a power-law to a Gaussian distribution.  :math:`m` is the power
of the power-law tail.

References
----------
.. [1] 'Crystal Ball Function',
       https://en.wikipedia.org/wiki/Crystal_Ball_function

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``crystalball.pdf(x, beta, m, loc, scale)`` is identically
equivalent to ``crystalball.pdf(y, beta, m) / scale`` with
``y = (x - loc) / scale``.

.. versionadded:: 0.19.0

Examples
--------
>>> from scipy.stats import crystalball
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta, m = 2, 3
>>> mean, var, skew, kurt = crystalball.stats(beta, m, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(crystalball.ppf(0.01, beta, m),
...                 crystalball.ppf(0.99, beta, m), 100)
>>> ax.plot(x, crystalball.pdf(x, beta, m),
...        'r-', lw=5, alpha=0.6, label='crystalball pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = crystalball(beta, m)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = crystalball.ppf([0.001, 0.5, 0.999], beta, m)
>>> np.allclose([0.001, 0.5, 0.999], crystalball.cdf(vals, beta, m))
True

Generate random numbers:

>>> r = crystalball.rvs(beta, m, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val dgamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Dgamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A double gamma continuous random variable.

As an instance of the `rv_continuous` class, `dgamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `dgamma` is:

.. math::

    f(x, a) = \frac{1}{2\Gamma(a)} |x|^{a-1} \exp(-|x|)

for a real number :math:`x` and :math:`a > 0`. :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`dgamma` takes ``a`` as a shape parameter for :math:`a`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``dgamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``dgamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import dgamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.1
>>> mean, var, skew, kurt = dgamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(dgamma.ppf(0.01, a),
...                 dgamma.ppf(0.99, a), 100)
>>> ax.plot(x, dgamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='dgamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = dgamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = dgamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], dgamma.cdf(vals, a))
True

Generate random numbers:

>>> r = dgamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val dlaplace : ?loc:float -> a:Py.Object.t -> unit -> [`Dlaplace_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A  Laplacian discrete random variable.

As an instance of the `rv_discrete` class, `dlaplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, a, loc=0)
    Probability mass function.
logpmf(k, a, loc=0)
    Log of the probability mass function.
cdf(k, a, loc=0)
    Cumulative distribution function.
logcdf(k, a, loc=0)
    Log of the cumulative distribution function.
sf(k, a, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, a, loc=0)
    Log of the survival function.
ppf(q, a, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(a, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0)
    Median of the distribution.
mean(a, loc=0)
    Mean of the distribution.
var(a, loc=0)
    Variance of the distribution.
std(a, loc=0)
    Standard deviation of the distribution.
interval(alpha, a, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `dlaplace` is:

.. math::

    f(k) = \tanh(a/2) \exp(-a |k|)

for integers :math:`k` and :math:`a > 0`.

`dlaplace` takes :math:`a` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``dlaplace.pmf(k, a, loc)`` is identically
equivalent to ``dlaplace.pmf(k - loc, a)``.

Examples
--------
>>> from scipy.stats import dlaplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 0.8
>>> mean, var, skew, kurt = dlaplace.stats(a, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(dlaplace.ppf(0.01, a),
...               dlaplace.ppf(0.99, a))
>>> ax.plot(x, dlaplace.pmf(x, a), 'bo', ms=8, label='dlaplace pmf')
>>> ax.vlines(x, 0, dlaplace.pmf(x, a), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = dlaplace(a)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = dlaplace.cdf(x, a)
>>> np.allclose(x, dlaplace.ppf(prob, a))
True

Generate random numbers:

>>> r = dlaplace.rvs(a, size=1000)
*)

val dweibull : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Dweibull_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A double Weibull continuous random variable.

As an instance of the `rv_continuous` class, `dweibull` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `dweibull` is given by

.. math::

    f(x, c) = c / 2 |x|^{c-1} \exp(-|x|^c)

for a real number :math:`x` and :math:`c > 0`.

`dweibull` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``dweibull.pdf(x, c, loc, scale)`` is identically
equivalent to ``dweibull.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import dweibull
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 2.07
>>> mean, var, skew, kurt = dweibull.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(dweibull.ppf(0.01, c),
...                 dweibull.ppf(0.99, c), 100)
>>> ax.plot(x, dweibull.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='dweibull pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = dweibull(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = dweibull.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], dweibull.cdf(vals, c))
True

Generate random numbers:

>>> r = dweibull.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val entropy : ?qk:Py.Object.t -> ?base:float -> ?axis:int -> pk:Py.Object.t -> unit -> float
(**
Calculate the entropy of a distribution for given probability values.

If only probabilities `pk` are given, the entropy is calculated as
``S = -sum(pk * log(pk), axis=axis)``.

If `qk` is not None, then compute the Kullback-Leibler divergence
``S = sum(pk * log(pk / qk), axis=axis)``.

This routine will normalize `pk` and `qk` if they don't sum to 1.

Parameters
----------
pk : sequence
    Defines the (discrete) distribution. ``pk[i]`` is the (possibly
    unnormalized) probability of event ``i``.
qk : sequence, optional
    Sequence against which the relative entropy is computed. Should be in
    the same format as `pk`.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis: int, optional
    The axis along which the entropy is calculated. Default is 0.

Returns
-------
S : float
    The calculated entropy.

Examples
--------

>>> from scipy.stats import entropy

Bernoulli trial with different p.
The outcome of a fair coin is the most uncertain:

>>> entropy([1/2, 1/2], base=2)
1.0

The outcome of a biased coin is less uncertain:

>>> entropy([9/10, 1/10], base=2)
0.46899559358928117

Relative entropy:

>>> entropy([1/2, 1/2], qk=[9/10, 1/10])
0.5108256237659907
*)

val erlang : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Erlang_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An Erlang continuous random variable.

As an instance of the `rv_continuous` class, `erlang` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gamma

Notes
-----
The Erlang distribution is a special case of the Gamma distribution, with
the shape parameter `a` an integer.  Note that this restriction is not
enforced by `erlang`. It will, however, generate a warning the first time
a non-integer value is used for the shape parameter.

Refer to `gamma` for examples.
*)

val expon : ?loc:float -> ?scale:float -> unit -> [`Expon_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponential continuous random variable.

As an instance of the `rv_continuous` class, `expon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `expon` is:

.. math::

    f(x) = \exp(-x)

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``expon.pdf(x, loc, scale)`` is identically
equivalent to ``expon.pdf(y) / scale`` with
``y = (x - loc) / scale``.

A common parameterization for `expon` is in terms of the rate parameter
``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This
parameterization corresponds to using ``scale = 1 / lambda``.

Examples
--------
>>> from scipy.stats import expon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = expon.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(expon.ppf(0.01),
...                 expon.ppf(0.99), 100)
>>> ax.plot(x, expon.pdf(x),
...        'r-', lw=5, alpha=0.6, label='expon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = expon()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = expon.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], expon.cdf(vals))
True

Generate random numbers:

>>> r = expon.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponnorm : ?loc:float -> ?scale:float -> k:Py.Object.t -> unit -> [`Exponnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponentially modified Normal continuous random variable.

As an instance of the `rv_continuous` class, `exponnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(K, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, K, loc=0, scale=1)
    Probability density function.
logpdf(x, K, loc=0, scale=1)
    Log of the probability density function.
cdf(x, K, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, K, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, K, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, K, loc=0, scale=1)
    Log of the survival function.
ppf(q, K, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, K, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, K, loc=0, scale=1)
    Non-central moment of order n
stats(K, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(K, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, K, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(K,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(K, loc=0, scale=1)
    Median of the distribution.
mean(K, loc=0, scale=1)
    Mean of the distribution.
var(K, loc=0, scale=1)
    Variance of the distribution.
std(K, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, K, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `exponnorm` is:

.. math::

    f(x, K) = \frac{1}{2K} \exp\left(\frac{1}{2 K^2} - x / K \right)
              \text{erfc}\left(-\frac{x - 1/K}{\sqrt{2}}\right)

where :math:`x` is a real number and :math:`K > 0`.

It can be thought of as the sum of a standard normal random variable
and an independent exponentially distributed random variable with rate
``1/K``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponnorm.pdf(x, K, loc, scale)`` is identically
equivalent to ``exponnorm.pdf(y, K) / scale`` with
``y = (x - loc) / scale``.

An alternative parameterization of this distribution (for example, in
`Wikipedia <https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution>`_)
involves three parameters, :math:`\mu`, :math:`\lambda` and
:math:`\sigma`.
In the present parameterization this corresponds to having ``loc`` and
``scale`` equal to :math:`\mu` and :math:`\sigma`, respectively, and
shape parameter :math:`K = 1/(\sigma\lambda)`.

.. versionadded:: 0.16.0

Examples
--------
>>> from scipy.stats import exponnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> K = 1.5
>>> mean, var, skew, kurt = exponnorm.stats(K, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponnorm.ppf(0.01, K),
...                 exponnorm.ppf(0.99, K), 100)
>>> ax.plot(x, exponnorm.pdf(x, K),
...        'r-', lw=5, alpha=0.6, label='exponnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponnorm(K)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponnorm.ppf([0.001, 0.5, 0.999], K)
>>> np.allclose([0.001, 0.5, 0.999], exponnorm.cdf(vals, K))
True

Generate random numbers:

>>> r = exponnorm.rvs(K, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponpow : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Exponpow_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponential power continuous random variable.

As an instance of the `rv_continuous` class, `exponpow` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `exponpow` is:

.. math::

    f(x, b) = b x^{b-1} \exp(1 + x^b - \exp(x^b))

for :math:`x \ge 0`, :math:`b > 0`.  Note that this is a different
distribution from the exponential power distribution that is also known
under the names 'generalized normal' or 'generalized Gaussian'.

`exponpow` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponpow.pdf(x, b, loc, scale)`` is identically
equivalent to ``exponpow.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

References
----------
http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf

Examples
--------
>>> from scipy.stats import exponpow
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 2.7
>>> mean, var, skew, kurt = exponpow.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponpow.ppf(0.01, b),
...                 exponpow.ppf(0.99, b), 100)
>>> ax.plot(x, exponpow.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='exponpow pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponpow(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponpow.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], exponpow.cdf(vals, b))
True

Generate random numbers:

>>> r = exponpow.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponweib : ?loc:float -> ?scale:float -> a:Py.Object.t -> c:Py.Object.t -> unit -> [`Exponweib_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponentiated Weibull continuous random variable.

As an instance of the `rv_continuous` class, `exponweib` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, c, loc=0, scale=1)
    Median of the distribution.
mean(a, c, loc=0, scale=1)
    Mean of the distribution.
var(a, c, loc=0, scale=1)
    Variance of the distribution.
std(a, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min, numpy.random.mtrand.RandomState.weibull

Notes
-----
The probability density function for `exponweib` is:

.. math::

    f(x, a, c) = a c [1-\exp(-x^c)]^{a-1} \exp(-x^c) x^{c-1}

and its cumulative distribution function is:

.. math::

    F(x, a, c) = [1-\exp(-x^c)]^a

for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.

`exponweib` takes :math:`a` and :math:`c` as shape parameters:

* :math:`a` is the exponentiation parameter,
  with the special case :math:`a=1` corresponding to the
  (non-exponentiated) Weibull distribution `weibull_min`.
* :math:`c` is the shape parameter of the non-exponentiated Weibull law.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponweib.pdf(x, a, c, loc, scale)`` is identically
equivalent to ``exponweib.pdf(y, a, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution

Examples
--------
>>> from scipy.stats import exponweib
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, c = 2.89, 1.95
>>> mean, var, skew, kurt = exponweib.stats(a, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponweib.ppf(0.01, a, c),
...                 exponweib.ppf(0.99, a, c), 100)
>>> ax.plot(x, exponweib.pdf(x, a, c),
...        'r-', lw=5, alpha=0.6, label='exponweib pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponweib(a, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponweib.ppf([0.001, 0.5, 0.999], a, c)
>>> np.allclose([0.001, 0.5, 0.999], exponweib.cdf(vals, a, c))
True

Generate random numbers:

>>> r = exponweib.rvs(a, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val f : ?loc:float -> ?scale:float -> dfn:Py.Object.t -> dfd:Py.Object.t -> unit -> [`F_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An F continuous random variable.

As an instance of the `rv_continuous` class, `f` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(dfn, dfd, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, dfn, dfd, loc=0, scale=1)
    Probability density function.
logpdf(x, dfn, dfd, loc=0, scale=1)
    Log of the probability density function.
cdf(x, dfn, dfd, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, dfn, dfd, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, dfn, dfd, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, dfn, dfd, loc=0, scale=1)
    Log of the survival function.
ppf(q, dfn, dfd, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, dfn, dfd, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, dfn, dfd, loc=0, scale=1)
    Non-central moment of order n
stats(dfn, dfd, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(dfn, dfd, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, dfn, dfd, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(dfn, dfd), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(dfn, dfd, loc=0, scale=1)
    Median of the distribution.
mean(dfn, dfd, loc=0, scale=1)
    Mean of the distribution.
var(dfn, dfd, loc=0, scale=1)
    Variance of the distribution.
std(dfn, dfd, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, dfn, dfd, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `f` is:

.. math::

    f(x, df_1, df_2) = \frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}
                            {(df_2+df_1 x)^{(df_1+df_2)/2}
                             B(df_1/2, df_2/2)}

for :math:`x > 0`.

`f` takes ``dfn`` and ``dfd`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``f.pdf(x, dfn, dfd, loc, scale)`` is identically
equivalent to ``f.pdf(y, dfn, dfd) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import f
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> dfn, dfd = 29, 18
>>> mean, var, skew, kurt = f.stats(dfn, dfd, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(f.ppf(0.01, dfn, dfd),
...                 f.ppf(0.99, dfn, dfd), 100)
>>> ax.plot(x, f.pdf(x, dfn, dfd),
...        'r-', lw=5, alpha=0.6, label='f pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = f(dfn, dfd)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)
>>> np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd))
True

Generate random numbers:

>>> r = f.rvs(dfn, dfd, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val fatiguelife : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Fatiguelife_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A fatigue-life (Birnbaum-Saunders) continuous random variable.

As an instance of the `rv_continuous` class, `fatiguelife` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `fatiguelife` is:

.. math::

    f(x, c) = \frac{x+1}{2c\sqrt{2\pi x^3}} \exp(-\frac{(x-1)^2}{2x c^2})

for :math:`x >= 0` and :math:`c > 0`.

`fatiguelife` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``fatiguelife.pdf(x, c, loc, scale)`` is identically
equivalent to ``fatiguelife.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] 'Birnbaum-Saunders distribution',
       https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution

Examples
--------
>>> from scipy.stats import fatiguelife
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 29
>>> mean, var, skew, kurt = fatiguelife.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(fatiguelife.ppf(0.01, c),
...                 fatiguelife.ppf(0.99, c), 100)
>>> ax.plot(x, fatiguelife.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='fatiguelife pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = fatiguelife(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = fatiguelife.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], fatiguelife.cdf(vals, c))
True

Generate random numbers:

>>> r = fatiguelife.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val fisk : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Fisk_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Fisk continuous random variable.

The Fisk distribution is also known as the log-logistic distribution.

As an instance of the `rv_continuous` class, `fisk` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `fisk` is:

.. math::

    f(x, c) = c x^{-c-1} (1 + x^{-c})^{-2}

for :math:`x >= 0` and :math:`c > 0`.

`fisk` takes ``c`` as a shape parameter for :math:`c`.

`fisk` is a special case of `burr` or `burr12` with ``d=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``fisk.pdf(x, c, loc, scale)`` is identically
equivalent to ``fisk.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
burr

Examples
--------
>>> from scipy.stats import fisk
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.09
>>> mean, var, skew, kurt = fisk.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(fisk.ppf(0.01, c),
...                 fisk.ppf(0.99, c), 100)
>>> ax.plot(x, fisk.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='fisk pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = fisk(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = fisk.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], fisk.cdf(vals, c))
True

Generate random numbers:

>>> r = fisk.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val foldcauchy : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Foldcauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A folded Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `foldcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `foldcauchy` is:

.. math::

    f(x, c) = \frac{1}{\pi (1+(x-c)^2)} + \frac{1}{\pi (1+(x+c)^2)}

for :math:`x \ge 0`.

`foldcauchy` takes ``c`` as a shape parameter for :math:`c`.

Examples
--------
>>> from scipy.stats import foldcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 4.72
>>> mean, var, skew, kurt = foldcauchy.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(foldcauchy.ppf(0.01, c),
...                 foldcauchy.ppf(0.99, c), 100)
>>> ax.plot(x, foldcauchy.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='foldcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = foldcauchy(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = foldcauchy.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], foldcauchy.cdf(vals, c))
True

Generate random numbers:

>>> r = foldcauchy.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val foldnorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Foldnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A folded normal continuous random variable.

As an instance of the `rv_continuous` class, `foldnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `foldnorm` is:

.. math::

    f(x, c) = \sqrt{2/\pi} cosh(c x) \exp(-\frac{x^2+c^2}{2})

for :math:`c \ge 0`.

`foldnorm` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``foldnorm.pdf(x, c, loc, scale)`` is identically
equivalent to ``foldnorm.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import foldnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.95
>>> mean, var, skew, kurt = foldnorm.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(foldnorm.ppf(0.01, c),
...                 foldnorm.ppf(0.99, c), 100)
>>> ax.plot(x, foldnorm.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='foldnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = foldnorm(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = foldnorm.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], foldnorm.cdf(vals, c))
True

Generate random numbers:

>>> r = foldnorm.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val frechet_l : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Frechet_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Frechet left (or Weibull maximum) continuous random variable.

As an instance of the `rv_continuous` class, `frechet_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_max : The same distribution as `frechet_l`.

Notes
-----
The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``frechet_l.pdf(x, c, loc, scale)`` is identically
equivalent to ``frechet_l.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import frechet_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.63
>>> mean, var, skew, kurt = frechet_l.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(frechet_l.ppf(0.01, c),
...                 frechet_l.ppf(0.99, c), 100)
>>> ax.plot(x, frechet_l.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='frechet_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = frechet_l(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = frechet_l.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], frechet_l.cdf(vals, c))
True

Generate random numbers:

>>> r = frechet_l.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val frechet_r : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Frechet_r_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Frechet right (or Weibull minimum) continuous random variable.

As an instance of the `rv_continuous` class, `frechet_r` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min : The same distribution as `frechet_r`.

Notes
-----
The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``frechet_r.pdf(x, c, loc, scale)`` is identically
equivalent to ``frechet_r.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import frechet_r
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.89
>>> mean, var, skew, kurt = frechet_r.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(frechet_r.ppf(0.01, c),
...                 frechet_r.ppf(0.99, c), 100)
>>> ax.plot(x, frechet_r.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='frechet_r pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = frechet_r(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = frechet_r.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], frechet_r.cdf(vals, c))
True

Generate random numbers:

>>> r = frechet_r.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Gamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A gamma continuous random variable.

As an instance of the `rv_continuous` class, `gamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
erlang, expon

Notes
-----
The probability density function for `gamma` is:

.. math::

    f(x, a) = \frac{x^{a-1} \exp(-x)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`. Here :math:`\Gamma(a)` refers to the
gamma function.

`gamma` takes ``a`` as a shape parameter for :math:`a`.

When :math:`a` is an integer, `gamma` reduces to the Erlang
distribution, and when :math:`a=1` to the exponential distribution.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``gamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.99
>>> mean, var, skew, kurt = gamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gamma.ppf(0.01, a),
...                 gamma.ppf(0.99, a), 100)
>>> ax.plot(x, gamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='gamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a))
True

Generate random numbers:

>>> r = gamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gausshyper : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> c:Py.Object.t -> z:Py.Object.t -> unit -> [`Gausshyper_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gauss hypergeometric continuous random variable.

As an instance of the `rv_continuous` class, `gausshyper` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, c, z, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, c, z, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, c, z, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, c, z, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, c, z, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, c, z, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, c, z, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, c, z, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, c, z, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, c, z, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, c, z, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, c, z, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, c, z, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b, c, z), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, c, z, loc=0, scale=1)
    Median of the distribution.
mean(a, b, c, z, loc=0, scale=1)
    Mean of the distribution.
var(a, b, c, z, loc=0, scale=1)
    Variance of the distribution.
std(a, b, c, z, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, c, z, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gausshyper` is:

.. math::

    f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}

for :math:`0 \le x \le 1`, :math:`a > 0`, :math:`b > 0`, and
:math:`C = \frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.
:math:`F[2, 1]` is the Gauss hypergeometric function
`scipy.special.hyp2f1`.

`gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape
parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gausshyper.pdf(x, a, b, c, z, loc, scale)`` is identically
equivalent to ``gausshyper.pdf(y, a, b, c, z) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gausshyper
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b, c, z = 13.8, 3.12, 2.51, 5.18
>>> mean, var, skew, kurt = gausshyper.stats(a, b, c, z, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gausshyper.ppf(0.01, a, b, c, z),
...                 gausshyper.ppf(0.99, a, b, c, z), 100)
>>> ax.plot(x, gausshyper.pdf(x, a, b, c, z),
...        'r-', lw=5, alpha=0.6, label='gausshyper pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gausshyper(a, b, c, z)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gausshyper.ppf([0.001, 0.5, 0.999], a, b, c, z)
>>> np.allclose([0.001, 0.5, 0.999], gausshyper.cdf(vals, a, b, c, z))
True

Generate random numbers:

>>> r = gausshyper.rvs(a, b, c, z, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genexpon : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> c:Py.Object.t -> unit -> [`Genexpon_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized exponential continuous random variable.

As an instance of the `rv_continuous` class, `genexpon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, c, loc=0, scale=1)
    Median of the distribution.
mean(a, b, c, loc=0, scale=1)
    Mean of the distribution.
var(a, b, c, loc=0, scale=1)
    Variance of the distribution.
std(a, b, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genexpon` is:

.. math::

    f(x, a, b, c) = (a + b (1 - \exp(-c x)))
                    \exp(-a x - b x + \frac{b}{c}  (1-\exp(-c x)))

for :math:`x \ge 0`, :math:`a, b, c > 0`.

`genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genexpon.pdf(x, a, b, c, loc, scale)`` is identically
equivalent to ``genexpon.pdf(y, a, b, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
H.K. Ryu, 'An Extension of Marshall and Olkin's Bivariate Exponential
Distribution', Journal of the American Statistical Association, 1993.

N. Balakrishnan, 'The Exponential Distribution: Theory, Methods and
Applications', Asit P. Basu.

Examples
--------
>>> from scipy.stats import genexpon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b, c = 9.13, 16.2, 3.28
>>> mean, var, skew, kurt = genexpon.stats(a, b, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genexpon.ppf(0.01, a, b, c),
...                 genexpon.ppf(0.99, a, b, c), 100)
>>> ax.plot(x, genexpon.pdf(x, a, b, c),
...        'r-', lw=5, alpha=0.6, label='genexpon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genexpon(a, b, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genexpon.ppf([0.001, 0.5, 0.999], a, b, c)
>>> np.allclose([0.001, 0.5, 0.999], genexpon.cdf(vals, a, b, c))
True

Generate random numbers:

>>> r = genexpon.rvs(a, b, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genextreme : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genextreme_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized extreme value continuous random variable.

As an instance of the `rv_continuous` class, `genextreme` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_r

Notes
-----
For :math:`c=0`, `genextreme` is equal to `gumbel_r`.
The probability density function for `genextreme` is:

.. math::

    f(x, c) = \begin{cases}
                \exp(-\exp(-x)) \exp(-x)              &\text{for } c = 0\\
                \exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1}  &\text{for }
                                                        x \le 1/c, c > 0
              \end{cases}


Note that several sources and software packages use the opposite
convention for the sign of the shape parameter :math:`c`.

`genextreme` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genextreme.pdf(x, c, loc, scale)`` is identically
equivalent to ``genextreme.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genextreme
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = -0.1
>>> mean, var, skew, kurt = genextreme.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genextreme.ppf(0.01, c),
...                 genextreme.ppf(0.99, c), 100)
>>> ax.plot(x, genextreme.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genextreme pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genextreme(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genextreme.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c))
True

Generate random numbers:

>>> r = genextreme.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gengamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> c:Py.Object.t -> unit -> [`Gengamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized gamma continuous random variable.

As an instance of the `rv_continuous` class, `gengamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, c, loc=0, scale=1)
    Median of the distribution.
mean(a, c, loc=0, scale=1)
    Mean of the distribution.
var(a, c, loc=0, scale=1)
    Variance of the distribution.
std(a, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gengamma` is:

.. math::

    f(x, a, c) = \frac{ |c| x^{c a-1} \exp(-x^c)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`, and :math:`c \ne 0`.
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gengamma` takes :math:`a` and :math:`c` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gengamma.pdf(x, a, c, loc, scale)`` is identically
equivalent to ``gengamma.pdf(y, a, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gengamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, c = 4.42, -3.12
>>> mean, var, skew, kurt = gengamma.stats(a, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gengamma.ppf(0.01, a, c),
...                 gengamma.ppf(0.99, a, c), 100)
>>> ax.plot(x, gengamma.pdf(x, a, c),
...        'r-', lw=5, alpha=0.6, label='gengamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gengamma(a, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gengamma.ppf([0.001, 0.5, 0.999], a, c)
>>> np.allclose([0.001, 0.5, 0.999], gengamma.cdf(vals, a, c))
True

Generate random numbers:

>>> r = gengamma.rvs(a, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genhalflogistic : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genhalflogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized half-logistic continuous random variable.

As an instance of the `rv_continuous` class, `genhalflogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genhalflogistic` is:

.. math::

    f(x, c) = \frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}

for :math:`0 \le x \le 1/c`, and :math:`c > 0`.

`genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genhalflogistic.pdf(x, c, loc, scale)`` is identically
equivalent to ``genhalflogistic.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genhalflogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.773
>>> mean, var, skew, kurt = genhalflogistic.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genhalflogistic.ppf(0.01, c),
...                 genhalflogistic.ppf(0.99, c), 100)
>>> ax.plot(x, genhalflogistic.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genhalflogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genhalflogistic(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genhalflogistic.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genhalflogistic.cdf(vals, c))
True

Generate random numbers:

>>> r = genhalflogistic.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val geninvgauss : ?loc:float -> ?scale:float -> p:Py.Object.t -> b:Py.Object.t -> unit -> [`Geninvgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Generalized Inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `geninvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, p, b, loc=0, scale=1)
    Probability density function.
logpdf(x, p, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, p, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, p, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, p, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, p, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, p, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, p, b, loc=0, scale=1)
    Non-central moment of order n
stats(p, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, p, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(p, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, b, loc=0, scale=1)
    Median of the distribution.
mean(p, b, loc=0, scale=1)
    Mean of the distribution.
var(p, b, loc=0, scale=1)
    Variance of the distribution.
std(p, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, p, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `geninvgauss` is:

.. math::

    f(x, p, b) = x^{p-1} \exp(-b (x + 1/x) / 2) / (2 K_p(b))

where `x > 0`, and the parameters `p, b` satisfy `b > 0` ([1]_).
:math:`K_p` is the modified Bessel function of second kind of order `p`
(`scipy.special.kv`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``geninvgauss.pdf(x, p, b, loc, scale)`` is identically
equivalent to ``geninvgauss.pdf(y, p, b) / scale`` with
``y = (x - loc) / scale``.

The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of
`geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.

Generating random variates is challenging for this distribution. The
implementation is based on [2]_.

References
----------
.. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, 'First hitting time
   models for the generalized inverse gaussian distribution',
   Stochastic Processes and their Applications 7, pp. 49--54, 1978.

.. [2] W. Hoermann and J. Leydold, 'Generating generalized inverse Gaussian
   random variates', Statistics and Computing, 24(4), p. 547--557, 2014.

Examples
--------
>>> from scipy.stats import geninvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p, b = 2.3, 1.5
>>> mean, var, skew, kurt = geninvgauss.stats(p, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(geninvgauss.ppf(0.01, p, b),
...                 geninvgauss.ppf(0.99, p, b), 100)
>>> ax.plot(x, geninvgauss.pdf(x, p, b),
...        'r-', lw=5, alpha=0.6, label='geninvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = geninvgauss(p, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = geninvgauss.ppf([0.001, 0.5, 0.999], p, b)
>>> np.allclose([0.001, 0.5, 0.999], geninvgauss.cdf(vals, p, b))
True

Generate random numbers:

>>> r = geninvgauss.rvs(p, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genlogistic : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genlogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized logistic continuous random variable.

As an instance of the `rv_continuous` class, `genlogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genlogistic` is:

.. math::

    f(x, c) = c \frac{\exp(-x)}
                     {(1 + \exp(-x))^{c+1}}

for :math:`x >= 0`, :math:`c > 0`.

`genlogistic` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genlogistic.pdf(x, c, loc, scale)`` is identically
equivalent to ``genlogistic.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genlogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.412
>>> mean, var, skew, kurt = genlogistic.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genlogistic.ppf(0.01, c),
...                 genlogistic.ppf(0.99, c), 100)
>>> ax.plot(x, genlogistic.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genlogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genlogistic(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genlogistic.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genlogistic.cdf(vals, c))
True

Generate random numbers:

>>> r = genlogistic.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gennorm : ?loc:float -> ?scale:float -> beta:Py.Object.t -> unit -> [`Gennorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized normal continuous random variable.

As an instance of the `rv_continuous` class, `gennorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, loc=0, scale=1)
    Non-central moment of order n
stats(beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, loc=0, scale=1)
    Median of the distribution.
mean(beta, loc=0, scale=1)
    Mean of the distribution.
var(beta, loc=0, scale=1)
    Variance of the distribution.
std(beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gennorm` is [1]_:

.. math::

    f(x, \beta) = \frac{\beta}{2 \Gamma(1/\beta)} \exp(-|x|^\beta)

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to a Laplace distribution.
For :math:`\beta = 2`, it is identical to a normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
laplace : Laplace distribution
norm : normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

Examples
--------
>>> from scipy.stats import gennorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta = 1.3
>>> mean, var, skew, kurt = gennorm.stats(beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gennorm.ppf(0.01, beta),
...                 gennorm.ppf(0.99, beta), 100)
>>> ax.plot(x, gennorm.pdf(x, beta),
...        'r-', lw=5, alpha=0.6, label='gennorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gennorm(beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gennorm.ppf([0.001, 0.5, 0.999], beta)
>>> np.allclose([0.001, 0.5, 0.999], gennorm.cdf(vals, beta))
True

Generate random numbers:

>>> r = gennorm.rvs(beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genpareto : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genpareto_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized Pareto continuous random variable.

As an instance of the `rv_continuous` class, `genpareto` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genpareto` is:

.. math::

    f(x, c) = (1 + c x)^{-1 - 1/c}

defined for :math:`x \ge 0` if :math:`c \ge 0`, and for
:math:`0 \le x \le -1/c` if :math:`c < 0`.

`genpareto` takes ``c`` as a shape parameter for :math:`c`.

For :math:`c=0`, `genpareto` reduces to the exponential
distribution, `expon`:

.. math::

    f(x, 0) = \exp(-x)

For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:

.. math::

    f(x, -1) = 1

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genpareto.pdf(x, c, loc, scale)`` is identically
equivalent to ``genpareto.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genpareto
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.1
>>> mean, var, skew, kurt = genpareto.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genpareto.ppf(0.01, c),
...                 genpareto.ppf(0.99, c), 100)
>>> ax.plot(x, genpareto.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genpareto pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genpareto(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genpareto.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genpareto.cdf(vals, c))
True

Generate random numbers:

>>> r = genpareto.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val geom : ?loc:float -> p:Py.Object.t -> unit -> [`Geom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A geometric discrete random variable.

As an instance of the `rv_discrete` class, `geom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `geom` is:

.. math::

    f(k) = (1-p)^{k-1} p

for :math:`k \ge 1`.

`geom` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``geom.pmf(k, p, loc)`` is identically
equivalent to ``geom.pmf(k - loc, p)``.

See Also
--------
planck

Examples
--------
>>> from scipy.stats import geom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.5
>>> mean, var, skew, kurt = geom.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(geom.ppf(0.01, p),
...               geom.ppf(0.99, p))
>>> ax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')
>>> ax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = geom(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = geom.cdf(x, p)
>>> np.allclose(x, geom.ppf(prob, p))
True

Generate random numbers:

>>> r = geom.rvs(p, size=1000)
*)

val gilbrat : ?loc:float -> ?scale:float -> unit -> [`Gilbrat_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gilbrat continuous random variable.

As an instance of the `rv_continuous` class, `gilbrat` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gilbrat` is:

.. math::

    f(x) = \frac{1}{x \sqrt{2\pi}} \exp(-\frac{1}{2} (\log(x))^2)

`gilbrat` is a special case of `lognorm` with ``s=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gilbrat.pdf(x, loc, scale)`` is identically
equivalent to ``gilbrat.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gilbrat
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gilbrat.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gilbrat.ppf(0.01),
...                 gilbrat.ppf(0.99), 100)
>>> ax.plot(x, gilbrat.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gilbrat pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gilbrat()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gilbrat.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gilbrat.cdf(vals))
True

Generate random numbers:

>>> r = gilbrat.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gompertz : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Gompertz_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gompertz (or truncated Gumbel) continuous random variable.

As an instance of the `rv_continuous` class, `gompertz` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gompertz` is:

.. math::

    f(x, c) = c \exp(x) \exp(-c (e^x-1))

for :math:`x \ge 0`, :math:`c > 0`.

`gompertz` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gompertz.pdf(x, c, loc, scale)`` is identically
equivalent to ``gompertz.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gompertz
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.947
>>> mean, var, skew, kurt = gompertz.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gompertz.ppf(0.01, c),
...                 gompertz.ppf(0.99, c), 100)
>>> ax.plot(x, gompertz.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='gompertz pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gompertz(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gompertz.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], gompertz.cdf(vals, c))
True

Generate random numbers:

>>> r = gompertz.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gumbel_l : ?loc:float -> ?scale:float -> unit -> [`Gumbel_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A left-skewed Gumbel continuous random variable.

As an instance of the `rv_continuous` class, `gumbel_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_r, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_l` is:

.. math::

    f(x) = \exp(x - e^x)

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gumbel_l.pdf(x, loc, scale)`` is identically
equivalent to ``gumbel_l.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gumbel_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gumbel_l.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gumbel_l.ppf(0.01),
...                 gumbel_l.ppf(0.99), 100)
>>> ax.plot(x, gumbel_l.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gumbel_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gumbel_l()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gumbel_l.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gumbel_l.cdf(vals))
True

Generate random numbers:

>>> r = gumbel_l.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gumbel_r : ?loc:float -> ?scale:float -> unit -> [`Gumbel_r_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A right-skewed Gumbel continuous random variable.

As an instance of the `rv_continuous` class, `gumbel_r` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_l, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_r` is:

.. math::

    f(x) = \exp(-(x + e^{-x}))

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gumbel_r.pdf(x, loc, scale)`` is identically
equivalent to ``gumbel_r.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gumbel_r
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gumbel_r.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gumbel_r.ppf(0.01),
...                 gumbel_r.ppf(0.99), 100)
>>> ax.plot(x, gumbel_r.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gumbel_r pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gumbel_r()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gumbel_r.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gumbel_r.cdf(vals))
True

Generate random numbers:

>>> r = gumbel_r.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfcauchy : ?loc:float -> ?scale:float -> unit -> [`Halfcauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Half-Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `halfcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfcauchy` is:

.. math::

    f(x) = \frac{2}{\pi (1 + x^2)}

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halfcauchy.pdf(x, loc, scale)`` is identically
equivalent to ``halfcauchy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halfcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halfcauchy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfcauchy.ppf(0.01),
...                 halfcauchy.ppf(0.99), 100)
>>> ax.plot(x, halfcauchy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halfcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfcauchy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfcauchy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halfcauchy.cdf(vals))
True

Generate random numbers:

>>> r = halfcauchy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfgennorm : ?loc:float -> ?scale:float -> beta:Py.Object.t -> unit -> [`Halfgennorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
The upper half of a generalized normal continuous random variable.

As an instance of the `rv_continuous` class, `halfgennorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, loc=0, scale=1)
    Non-central moment of order n
stats(beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, loc=0, scale=1)
    Median of the distribution.
mean(beta, loc=0, scale=1)
    Mean of the distribution.
var(beta, loc=0, scale=1)
    Variance of the distribution.
std(beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfgennorm` is:

.. math::

    f(x, \beta) = \frac{\beta}{\Gamma(1/\beta)} \exp(-|x|^\beta)

for :math:`x > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to an exponential distribution.
For :math:`\beta = 2`, it is identical to a half normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
gennorm : generalized normal distribution
expon : exponential distribution
halfnorm : half normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

Examples
--------
>>> from scipy.stats import halfgennorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta = 0.675
>>> mean, var, skew, kurt = halfgennorm.stats(beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfgennorm.ppf(0.01, beta),
...                 halfgennorm.ppf(0.99, beta), 100)
>>> ax.plot(x, halfgennorm.pdf(x, beta),
...        'r-', lw=5, alpha=0.6, label='halfgennorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfgennorm(beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfgennorm.ppf([0.001, 0.5, 0.999], beta)
>>> np.allclose([0.001, 0.5, 0.999], halfgennorm.cdf(vals, beta))
True

Generate random numbers:

>>> r = halfgennorm.rvs(beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halflogistic : ?loc:float -> ?scale:float -> unit -> [`Halflogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A half-logistic continuous random variable.

As an instance of the `rv_continuous` class, `halflogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halflogistic` is:

.. math::

    f(x) = \frac{ 2 e^{-x} }{ (1+e^{-x})^2 }
         = \frac{1}{2} \text{sech}(x/2)^2

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halflogistic.pdf(x, loc, scale)`` is identically
equivalent to ``halflogistic.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halflogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halflogistic.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halflogistic.ppf(0.01),
...                 halflogistic.ppf(0.99), 100)
>>> ax.plot(x, halflogistic.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halflogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halflogistic()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halflogistic.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halflogistic.cdf(vals))
True

Generate random numbers:

>>> r = halflogistic.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfnorm : ?loc:float -> ?scale:float -> unit -> [`Halfnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A half-normal continuous random variable.

As an instance of the `rv_continuous` class, `halfnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfnorm` is:

.. math::

    f(x) = \sqrt{2/\pi} \exp(-x^2 / 2)

for :math:`x >= 0`.

`halfnorm` is a special case of `chi` with ``df=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halfnorm.pdf(x, loc, scale)`` is identically
equivalent to ``halfnorm.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halfnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halfnorm.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfnorm.ppf(0.01),
...                 halfnorm.ppf(0.99), 100)
>>> ax.plot(x, halfnorm.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halfnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfnorm()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfnorm.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halfnorm.cdf(vals))
True

Generate random numbers:

>>> r = halfnorm.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val hypergeom : ?loc:float -> m:Py.Object.t -> n:Py.Object.t -> n':Py.Object.t -> unit -> [`Hypergeom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A hypergeometric discrete random variable.

The hypergeometric distribution models drawing objects from a bin.
`M` is the total number of objects, `n` is total number of Type I objects.
The random variate represents the number of Type I objects in `N` drawn
without replacement from the total population.

As an instance of the `rv_discrete` class, `hypergeom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(M, n, N, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, M, n, N, loc=0)
    Probability mass function.
logpmf(k, M, n, N, loc=0)
    Log of the probability mass function.
cdf(k, M, n, N, loc=0)
    Cumulative distribution function.
logcdf(k, M, n, N, loc=0)
    Log of the cumulative distribution function.
sf(k, M, n, N, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, M, n, N, loc=0)
    Log of the survival function.
ppf(q, M, n, N, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, M, n, N, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(M, n, N, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(M, n, N, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(M, n, N), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(M, n, N, loc=0)
    Median of the distribution.
mean(M, n, N, loc=0)
    Mean of the distribution.
var(M, n, N, loc=0)
    Variance of the distribution.
std(M, n, N, loc=0)
    Standard deviation of the distribution.
interval(alpha, M, n, N, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not
universally accepted.  See the Examples for a clarification of the
definitions used here.

The probability mass function is defined as,

.. math:: p(k, M, n, N) = \frac{\binom{n}{k} \binom{M - n}{N - k}}
                               {\binom{M}{N}}

for :math:`k \in [\max(0, N - M + n), \min(n, N)]`, where the binomial
coefficients are defined as,

.. math:: \binom{n}{k} \equiv \frac{n!}{k! (n - k)!}.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``hypergeom.pmf(k, M, n, N, loc)`` is identically
equivalent to ``hypergeom.pmf(k - loc, M, n, N)``.

Examples
--------
>>> from scipy.stats import hypergeom
>>> import matplotlib.pyplot as plt

Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if
we want to know the probability of finding a given number of dogs if we
choose at random 12 of the 20 animals, we can initialize a frozen
distribution and plot the probability mass function:

>>> [M, n, N] = [20, 7, 12]
>>> rv = hypergeom(M, n, N)
>>> x = np.arange(0, n+1)
>>> pmf_dogs = rv.pmf(x)

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, pmf_dogs, 'bo')
>>> ax.vlines(x, 0, pmf_dogs, lw=2)
>>> ax.set_xlabel('# of dogs in our group of chosen animals')
>>> ax.set_ylabel('hypergeom PMF')
>>> plt.show()

Instead of using a frozen distribution we can also use `hypergeom`
methods directly.  To for example obtain the cumulative distribution
function, use:

>>> prb = hypergeom.cdf(x, M, n, N)

And to generate random numbers:

>>> R = hypergeom.rvs(M, n, N, size=10)
*)

val hypsecant : ?loc:float -> ?scale:float -> unit -> [`Hypsecant_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A hyperbolic secant continuous random variable.

As an instance of the `rv_continuous` class, `hypsecant` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `hypsecant` is:

.. math::

    f(x) = \frac{1}{\pi} \text{sech}(x)

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``hypsecant.pdf(x, loc, scale)`` is identically
equivalent to ``hypsecant.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import hypsecant
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = hypsecant.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(hypsecant.ppf(0.01),
...                 hypsecant.ppf(0.99), 100)
>>> ax.plot(x, hypsecant.pdf(x),
...        'r-', lw=5, alpha=0.6, label='hypsecant pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = hypsecant()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = hypsecant.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], hypsecant.cdf(vals))
True

Generate random numbers:

>>> r = hypsecant.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invgamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Invgamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverted gamma continuous random variable.

As an instance of the `rv_continuous` class, `invgamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invgamma` is:

.. math::

    f(x, a) = \frac{x^{-a-1}}{\Gamma(a)} \exp(-\frac{1}{x})

for :math:`x >= 0`, :math:`a > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`invgamma` takes ``a`` as a shape parameter for :math:`a`.

`invgamma` is a special case of `gengamma` with ``c=-1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invgamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``invgamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import invgamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 4.07
>>> mean, var, skew, kurt = invgamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invgamma.ppf(0.01, a),
...                 invgamma.ppf(0.99, a), 100)
>>> ax.plot(x, invgamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='invgamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invgamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invgamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], invgamma.cdf(vals, a))
True

Generate random numbers:

>>> r = invgamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invgauss : ?loc:float -> ?scale:float -> mu:Py.Object.t -> unit -> [`Invgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `invgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, mu, loc=0, scale=1)
    Probability density function.
logpdf(x, mu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, mu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, mu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, mu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, mu, loc=0, scale=1)
    Log of the survival function.
ppf(q, mu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, mu, loc=0, scale=1)
    Non-central moment of order n
stats(mu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, mu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0, scale=1)
    Median of the distribution.
mean(mu, loc=0, scale=1)
    Mean of the distribution.
var(mu, loc=0, scale=1)
    Variance of the distribution.
std(mu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2 \pi x^3}}
                \exp(-\frac{(x-\mu)^2}{2 x \mu^2})

for :math:`x >= 0` and :math:`\mu > 0`.

`invgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invgauss.pdf(x, mu, loc, scale)`` is identically
equivalent to ``invgauss.pdf(y, mu) / scale`` with
``y = (x - loc) / scale``.

When :math:`\mu` is too small, evaluating the cumulative distribution
function will be inaccurate due to ``cdf(mu -> 0) = inf * 0``.
NaNs are returned for :math:`\mu \le 0.0028`.

Examples
--------
>>> from scipy.stats import invgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.145
>>> mean, var, skew, kurt = invgauss.stats(mu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invgauss.ppf(0.01, mu),
...                 invgauss.ppf(0.99, mu), 100)
>>> ax.plot(x, invgauss.pdf(x, mu),
...        'r-', lw=5, alpha=0.6, label='invgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invgauss(mu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invgauss.ppf([0.001, 0.5, 0.999], mu)
>>> np.allclose([0.001, 0.5, 0.999], invgauss.cdf(vals, mu))
True

Generate random numbers:

>>> r = invgauss.rvs(mu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invweibull : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Invweibull_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverted Weibull continuous random variable.

This distribution is also known as the Frchet distribution or the
type II extreme value distribution.

As an instance of the `rv_continuous` class, `invweibull` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invweibull` is:

.. math::

    f(x, c) = c x^{-c-1} \exp(-x^{-c})

for :math:`x > 0`, :math:`c > 0`.

`invweibull` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invweibull.pdf(x, c, loc, scale)`` is identically
equivalent to ``invweibull.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, 'The generalized inverse
Weibull distribution', Stat. Papers, vol. 52, pp. 591-619, 2011.

Examples
--------
>>> from scipy.stats import invweibull
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 10.6
>>> mean, var, skew, kurt = invweibull.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invweibull.ppf(0.01, c),
...                 invweibull.ppf(0.99, c), 100)
>>> ax.plot(x, invweibull.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='invweibull pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invweibull(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invweibull.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], invweibull.cdf(vals, c))
True

Generate random numbers:

>>> r = invweibull.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val johnsonsb : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Johnsonsb_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Johnson SB continuous random variable.

As an instance of the `rv_continuous` class, `johnsonsb` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
johnsonsu

Notes
-----
The probability density function for `johnsonsb` is:

.. math::

    f(x, a, b) = \frac{b}{x(1-x)}  \phi(a + b \log \frac{x}{1-x} )

for :math:`0 <= x < =1` and :math:`a, b > 0`, and :math:`\phi` is the normal
pdf.

`johnsonsb` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``johnsonsb.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``johnsonsb.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import johnsonsb
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 4.32, 3.18
>>> mean, var, skew, kurt = johnsonsb.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(johnsonsb.ppf(0.01, a, b),
...                 johnsonsb.ppf(0.99, a, b), 100)
>>> ax.plot(x, johnsonsb.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='johnsonsb pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = johnsonsb(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = johnsonsb.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], johnsonsb.cdf(vals, a, b))
True

Generate random numbers:

>>> r = johnsonsb.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val johnsonsu : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Johnsonsu_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Johnson SU continuous random variable.

As an instance of the `rv_continuous` class, `johnsonsu` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
johnsonsb

Notes
-----
The probability density function for `johnsonsu` is:

.. math::

    f(x, a, b) = \frac{b}{\sqrt{x^2 + 1}}
                 \phi(a + b \log(x + \sqrt{x^2 + 1}))

for all :math:`x, a, b > 0`, and :math:`\phi` is the normal pdf.

`johnsonsu` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``johnsonsu.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``johnsonsu.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import johnsonsu
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 2.55, 2.25
>>> mean, var, skew, kurt = johnsonsu.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(johnsonsu.ppf(0.01, a, b),
...                 johnsonsu.ppf(0.99, a, b), 100)
>>> ax.plot(x, johnsonsu.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='johnsonsu pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = johnsonsu(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = johnsonsu.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], johnsonsu.cdf(vals, a, b))
True

Generate random numbers:

>>> r = johnsonsu.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kappa3 : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Kappa3_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kappa 3 parameter distribution.

As an instance of the `rv_continuous` class, `kappa3` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `kappa3` is:

.. math::

    f(x, a) = a (a + x^a)^{-(a + 1)/a}

for :math:`x > 0` and :math:`a > 0`.

`kappa3` takes ``a`` as a shape parameter for :math:`a`.

References
----------
P.W. Mielke and E.S. Johnson, 'Three-Parameter Kappa Distribution Maximum
Likelihood and Likelihood Ratio Tests', Methods in Weather Research,
701-707, (September, 1973),
https://doi.org/10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2

B. Kumphon, 'Maximum Entropy and Maximum Likelihood Estimation for the
Three-Parameter Kappa Distribution', Open Journal of Statistics, vol 2,
415-419 (2012), https://doi.org/10.4236/ojs.2012.24050

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kappa3.pdf(x, a, loc, scale)`` is identically
equivalent to ``kappa3.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import kappa3
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1
>>> mean, var, skew, kurt = kappa3.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kappa3.ppf(0.01, a),
...                 kappa3.ppf(0.99, a), 100)
>>> ax.plot(x, kappa3.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='kappa3 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kappa3(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kappa3.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], kappa3.cdf(vals, a))
True

Generate random numbers:

>>> r = kappa3.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kappa4 : ?loc:float -> ?scale:float -> h:Py.Object.t -> k:Py.Object.t -> unit -> [`Kappa4_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kappa 4 parameter distribution.

As an instance of the `rv_continuous` class, `kappa4` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(h, k, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, h, k, loc=0, scale=1)
    Probability density function.
logpdf(x, h, k, loc=0, scale=1)
    Log of the probability density function.
cdf(x, h, k, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, h, k, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, h, k, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, h, k, loc=0, scale=1)
    Log of the survival function.
ppf(q, h, k, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, h, k, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, h, k, loc=0, scale=1)
    Non-central moment of order n
stats(h, k, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(h, k, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, h, k, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(h, k), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(h, k, loc=0, scale=1)
    Median of the distribution.
mean(h, k, loc=0, scale=1)
    Mean of the distribution.
var(h, k, loc=0, scale=1)
    Variance of the distribution.
std(h, k, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, h, k, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for kappa4 is:

.. math::

    f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}

if :math:`h` and :math:`k` are not equal to 0.

If :math:`h` or :math:`k` are zero then the pdf can be simplified:

h = 0 and k != 0::

    kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*
                          exp(-(1.0 - k*x)**(1.0/k))

h != 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)

h = 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))

kappa4 takes :math:`h` and :math:`k` as shape parameters.

The kappa4 distribution returns other distributions when certain
:math:`h` and :math:`k` values are used.

+------+-------------+----------------+------------------+
| h    | k=0.0       | k=1.0          | -inf<=k<=inf     |
+======+=============+================+==================+
| -1.0 | Logistic    |                | Generalized      |
|      |             |                | Logistic(1)      |
|      |             |                |                  |
|      | logistic(x) |                |                  |
+------+-------------+----------------+------------------+
|  0.0 | Gumbel      | Reverse        | Generalized      |
|      |             | Exponential(2) | Extreme Value    |
|      |             |                |                  |
|      | gumbel_r(x) |                | genextreme(x, k) |
+------+-------------+----------------+------------------+
|  1.0 | Exponential | Uniform        | Generalized      |
|      |             |                | Pareto           |
|      |             |                |                  |
|      | expon(x)    | uniform(x)     | genpareto(x, -k) |
+------+-------------+----------------+------------------+

(1) There are at least five generalized logistic distributions.
    Four are described here:
    https://en.wikipedia.org/wiki/Generalized_logistic_distribution
    The 'fifth' one is the one kappa4 should match which currently
    isn't implemented in scipy:
    https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution
    https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html
(2) This distribution is currently not in scipy.

References
----------
J.C. Finney, 'Optimization of a Skewed Logistic Distribution With Respect
to the Kolmogorov-Smirnov Test', A Dissertation Submitted to the Graduate
Faculty of the Louisiana State University and Agricultural and Mechanical
College, (August, 2004),
https://digitalcommons.lsu.edu/gradschool_dissertations/3672

J.R.M. Hosking, 'The four-parameter kappa distribution'. IBM J. Res.
Develop. 38 (3), 25 1-258 (1994).

B. Kumphon, A. Kaew-Man, P. Seenoi, 'A Rainfall Distribution for the Lampao
Site in the Chi River Basin, Thailand', Journal of Water Resource and
Protection, vol. 4, 866-869, (2012).
https://doi.org/10.4236/jwarp.2012.410101

C. Winchester, 'On Estimation of the Four-Parameter Kappa Distribution', A
Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March
2000).
http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kappa4.pdf(x, h, k, loc, scale)`` is identically
equivalent to ``kappa4.pdf(y, h, k) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import kappa4
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> h, k = 0.1, 0
>>> mean, var, skew, kurt = kappa4.stats(h, k, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kappa4.ppf(0.01, h, k),
...                 kappa4.ppf(0.99, h, k), 100)
>>> ax.plot(x, kappa4.pdf(x, h, k),
...        'r-', lw=5, alpha=0.6, label='kappa4 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kappa4(h, k)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kappa4.ppf([0.001, 0.5, 0.999], h, k)
>>> np.allclose([0.001, 0.5, 0.999], kappa4.cdf(vals, h, k))
True

Generate random numbers:

>>> r = kappa4.rvs(h, k, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val ksone : ?loc:float -> ?scale:float -> n:Py.Object.t -> unit -> [`Ksone_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
General Kolmogorov-Smirnov one-sided test.

This is the distribution of the one-sided Kolmogorov-Smirnov (KS)
statistics :math:`D_n^+` and :math:`D_n^-`
for a finite sample size ``n`` (the shape parameter).

As an instance of the `rv_continuous` class, `ksone` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, n, loc=0, scale=1)
    Probability density function.
logpdf(x, n, loc=0, scale=1)
    Log of the probability density function.
cdf(x, n, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, n, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, n, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, n, loc=0, scale=1)
    Log of the survival function.
ppf(q, n, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, n, loc=0, scale=1)
    Non-central moment of order n
stats(n, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, n, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, loc=0, scale=1)
    Median of the distribution.
mean(n, loc=0, scale=1)
    Mean of the distribution.
var(n, loc=0, scale=1)
    Variance of the distribution.
std(n, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, n, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
:math:`D_n^+` and :math:`D_n^-` are given by

.. math::

    D_n^+ &= \text{sup}_x (F_n(x) - F(x)),\\
    D_n^- &= \text{sup}_x (F(x) - F_n(x)),\\

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `ksone`
describes the distribution under the null hypothesis of the KS test
that the empirical CDF corresponds to :math:`n` i.i.d. random variates
with CDF :math:`F`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ksone.pdf(x, n, loc, scale)`` is identically
equivalent to ``ksone.pdf(y, n) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
kstwobign, kstest

References
----------
.. [1] Birnbaum, Z. W. and Tingey, F.H. 'One-sided confidence contours
   for probability distribution functions', The Annals of Mathematical
   Statistics, 22(4), pp 592-596 (1951).

Examples
--------
>>> from scipy.stats import ksone
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n = 1e+03
>>> mean, var, skew, kurt = ksone.stats(n, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ksone.ppf(0.01, n),
...                 ksone.ppf(0.99, n), 100)
>>> ax.plot(x, ksone.pdf(x, n),
...        'r-', lw=5, alpha=0.6, label='ksone pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ksone(n)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ksone.ppf([0.001, 0.5, 0.999], n)
>>> np.allclose([0.001, 0.5, 0.999], ksone.cdf(vals, n))
True

Generate random numbers:

>>> r = ksone.rvs(n, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kstwobign : ?loc:float -> ?scale:float -> unit -> [`Kstwobign_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kolmogorov-Smirnov two-sided test for large N.

This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov
statistic :math:`\sqrt{n} D_n` that measures the maximum absolute
distance of the theoretical CDF from the empirical CDF (see `kstest`).

As an instance of the `rv_continuous` class, `kstwobign` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
:math:`\sqrt{n} D_n` is given by

.. math::

    D_n = \text{sup}_x |F_n(x) - F(x)|

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `kstwobign`
describes the asymptotic distribution (i.e. the limit of
:math:`\sqrt{n} D_n`) under the null hypothesis of the KS test that the
empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kstwobign.pdf(x, loc, scale)`` is identically
equivalent to ``kstwobign.pdf(y) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
ksone, kstest

References
----------
.. [1] Marsaglia, G. et al. 'Evaluating Kolmogorov's distribution',
   Journal of Statistical Software, 8(18), 2003.

Examples
--------
>>> from scipy.stats import kstwobign
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = kstwobign.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kstwobign.ppf(0.01),
...                 kstwobign.ppf(0.99), 100)
>>> ax.plot(x, kstwobign.pdf(x),
...        'r-', lw=5, alpha=0.6, label='kstwobign pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kstwobign()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kstwobign.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], kstwobign.cdf(vals))
True

Generate random numbers:

>>> r = kstwobign.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val laplace : ?loc:float -> ?scale:float -> unit -> [`Laplace_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Laplace continuous random variable.

As an instance of the `rv_continuous` class, `laplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `laplace` is

.. math::

    f(x) = \frac{1}{2} \exp(-|x|)

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``laplace.pdf(x, loc, scale)`` is identically
equivalent to ``laplace.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import laplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = laplace.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(laplace.ppf(0.01),
...                 laplace.ppf(0.99), 100)
>>> ax.plot(x, laplace.pdf(x),
...        'r-', lw=5, alpha=0.6, label='laplace pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = laplace()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = laplace.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], laplace.cdf(vals))
True

Generate random numbers:

>>> r = laplace.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levy : ?loc:float -> ?scale:float -> unit -> [`Levy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Levy continuous random variable.

As an instance of the `rv_continuous` class, `levy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy_stable, levy_l

Notes
-----
The probability density function for `levy` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp\left(-\frac{1}{2x}\right)

for :math:`x >= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy.pdf(x, loc, scale)`` is identically
equivalent to ``levy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import levy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = levy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy.ppf(0.01),
...                 levy.ppf(0.99), 100)
>>> ax.plot(x, levy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='levy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], levy.cdf(vals))
True

Generate random numbers:

>>> r = levy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levy_l : ?loc:float -> ?scale:float -> unit -> [`Levy_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A left-skewed Levy continuous random variable.

As an instance of the `rv_continuous` class, `levy_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy, levy_stable

Notes
-----
The probability density function for `levy_l` is:

.. math::
    f(x) = \frac{1}{ |x| \sqrt{2\pi |x| }} \exp{ \left(-\frac{1}{2|x| } \right)}

for :math:`x <= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=-1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy_l.pdf(x, loc, scale)`` is identically
equivalent to ``levy_l.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import levy_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = levy_l.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy_l.ppf(0.01),
...                 levy_l.ppf(0.99), 100)
>>> ax.plot(x, levy_l.pdf(x),
...        'r-', lw=5, alpha=0.6, label='levy_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy_l()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy_l.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], levy_l.cdf(vals))
True

Generate random numbers:

>>> r = levy_l.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levy_stable : ?loc:float -> ?scale:float -> alpha:Py.Object.t -> beta:Py.Object.t -> unit -> [`Levy_stable_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Levy-stable continuous random variable.

As an instance of the `rv_continuous` class, `levy_stable` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(alpha, beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, alpha, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, alpha, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, alpha, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, alpha, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, alpha, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, alpha, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, alpha, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, alpha, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, alpha, beta, loc=0, scale=1)
    Non-central moment of order n
stats(alpha, beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(alpha, beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, alpha, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(alpha, beta), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(alpha, beta, loc=0, scale=1)
    Median of the distribution.
mean(alpha, beta, loc=0, scale=1)
    Mean of the distribution.
var(alpha, beta, loc=0, scale=1)
    Variance of the distribution.
std(alpha, beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy, levy_l

Notes
-----
The distribution for `levy_stable` has characteristic function:

.. math::

    \varphi(t, \alpha, \beta, c, \mu) =
    e^{it\mu -|ct|^{\alpha}(1-i\beta \operatorname{sign}(t)\Phi(\alpha, t))}

where:

.. math::

    \Phi = \begin{cases}
            \tan \left({\frac {\pi \alpha }{2}}\right)&\alpha \neq 1\\
            -{\frac {2}{\pi }}\log |t|&\alpha =1
            \end{cases}

The probability density function for `levy_stable` is:

.. math::

    f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \varphi(t)e^{-ixt}\,dt

where :math:`-\infty < t < \infty`. This integral does not have a known closed form.

For evaluation of pdf we use either Zolotarev :math:`S_0` parameterization with integration,
direct integration of standard parameterization of characteristic function or FFT of
characteristic function. If set to other than None and if number of points is greater than
``levy_stable.pdf_fft_min_points_threshold`` (defaults to None) we use FFT otherwise we use one
of the other methods.

The default method is 'best' which uses Zolotarev's method if alpha = 1 and integration of
characteristic function otherwise. The default method can be changed by setting
``levy_stable.pdf_default_method`` to either 'zolotarev', 'quadrature' or 'best'.

To increase accuracy of FFT calculation one can specify ``levy_stable.pdf_fft_grid_spacing``
(defaults to 0.001) and ``pdf_fft_n_points_two_power`` (defaults to a value that covers the
input range * 4). Setting ``pdf_fft_n_points_two_power`` to 16 should be sufficiently accurate
in most cases at the expense of CPU time.

For evaluation of cdf we use Zolatarev :math:`S_0` parameterization with integration or integral of
the pdf FFT interpolated spline. The settings affecting FFT calculation are the same as
for pdf calculation. Setting the threshold to ``None`` (default) will disable FFT. For cdf
calculations the Zolatarev method is superior in accuracy, so FFT is disabled by default.

Fitting estimate uses quantile estimation method in [MC]. MLE estimation of parameters in
fit method uses this quantile estimate initially. Note that MLE doesn't always converge if
using FFT for pdf calculations; so it's best that ``pdf_fft_min_points_threshold`` is left unset.

.. warning::

    For pdf calculations implementation of Zolatarev is unstable for values where alpha = 1 and
    beta != 0. In this case the quadrature method is recommended. FFT calculation is also
    considered experimental.

    For cdf calculations FFT calculation is considered experimental. Use Zolatarev's method
    instead (default).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy_stable.pdf(x, alpha, beta, loc, scale)`` is identically
equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [MC] McCulloch, J., 1986. Simple consistent estimators of stable distribution parameters.
   Communications in Statistics - Simulation and Computation 15, 11091136.
.. [MS] Mittnik, S.T. Rachev, T. Doganoglu, D. Chenyao, 1999. Maximum likelihood estimation
   of stable Paretian models, Mathematical and Computer Modelling, Volume 29, Issue 10,
   1999, Pages 275-293.
.. [BS] Borak, S., Hardle, W., Rafal, W. 2005. Stable distributions, Economic Risk.

Examples
--------
>>> from scipy.stats import levy_stable
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> alpha, beta = 1.8, -0.5
>>> mean, var, skew, kurt = levy_stable.stats(alpha, beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy_stable.ppf(0.01, alpha, beta),
...                 levy_stable.ppf(0.99, alpha, beta), 100)
>>> ax.plot(x, levy_stable.pdf(x, alpha, beta),
...        'r-', lw=5, alpha=0.6, label='levy_stable pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy_stable(alpha, beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy_stable.ppf([0.001, 0.5, 0.999], alpha, beta)
>>> np.allclose([0.001, 0.5, 0.999], levy_stable.cdf(vals, alpha, beta))
True

Generate random numbers:

>>> r = levy_stable.rvs(alpha, beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val loggamma : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Loggamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A log gamma continuous random variable.

As an instance of the `rv_continuous` class, `loggamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `loggamma` is:

.. math::

    f(x, c) = \frac{\exp(c x - \exp(x))}
                   {\Gamma(c)}

for all :math:`x, c > 0`. Here, :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`loggamma` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loggamma.pdf(x, c, loc, scale)`` is identically
equivalent to ``loggamma.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import loggamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.414
>>> mean, var, skew, kurt = loggamma.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loggamma.ppf(0.01, c),
...                 loggamma.ppf(0.99, c), 100)
>>> ax.plot(x, loggamma.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='loggamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loggamma(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loggamma.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], loggamma.cdf(vals, c))
True

Generate random numbers:

>>> r = loggamma.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val logistic : ?loc:float -> ?scale:float -> unit -> [`Logistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A logistic (or Sech-squared) continuous random variable.

As an instance of the `rv_continuous` class, `logistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `logistic` is:

.. math::

    f(x) = \frac{\exp(-x)}
                {(1+\exp(-x))^2}

`logistic` is a special case of `genlogistic` with ``c=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``logistic.pdf(x, loc, scale)`` is identically
equivalent to ``logistic.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import logistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = logistic.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(logistic.ppf(0.01),
...                 logistic.ppf(0.99), 100)
>>> ax.plot(x, logistic.pdf(x),
...        'r-', lw=5, alpha=0.6, label='logistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = logistic()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = logistic.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], logistic.cdf(vals))
True

Generate random numbers:

>>> r = logistic.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val loglaplace : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Loglaplace_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A log-Laplace continuous random variable.

As an instance of the `rv_continuous` class, `loglaplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `loglaplace` is:

.. math::

    f(x, c) = \begin{cases}\frac{c}{2} x^{ c-1}  &\text{for } 0 < x < 1\\
                           \frac{c}{2} x^{-c-1}  &\text{for } x \ge 1
              \end{cases}

for :math:`c > 0`.

`loglaplace` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loglaplace.pdf(x, c, loc, scale)`` is identically
equivalent to ``loglaplace.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
T.J. Kozubowski and K. Podgorski, 'A log-Laplace growth rate model',
The Mathematical Scientist, vol. 28, pp. 49-60, 2003.

Examples
--------
>>> from scipy.stats import loglaplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.25
>>> mean, var, skew, kurt = loglaplace.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loglaplace.ppf(0.01, c),
...                 loglaplace.ppf(0.99, c), 100)
>>> ax.plot(x, loglaplace.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='loglaplace pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loglaplace(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loglaplace.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], loglaplace.cdf(vals, c))
True

Generate random numbers:

>>> r = loglaplace.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val lognorm : ?loc:float -> ?scale:float -> s:Py.Object.t -> unit -> [`Lognorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A lognormal continuous random variable.

As an instance of the `rv_continuous` class, `lognorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, s, loc=0, scale=1)
    Probability density function.
logpdf(x, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, s, loc=0, scale=1)
    Non-central moment of order n
stats(s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(s,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(s, loc=0, scale=1)
    Median of the distribution.
mean(s, loc=0, scale=1)
    Mean of the distribution.
var(s, loc=0, scale=1)
    Variance of the distribution.
std(s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `lognorm` is:

.. math::

    f(x, s) = \frac{1}{s x \sqrt{2\pi}}
              \exp\left(-\frac{\log^2(x)}{2s^2}\right)

for :math:`x > 0`, :math:`s > 0`.

`lognorm` takes ``s`` as a shape parameter for :math:`s`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``lognorm.pdf(x, s, loc, scale)`` is identically
equivalent to ``lognorm.pdf(y, s) / scale`` with
``y = (x - loc) / scale``.

A common parametrization for a lognormal random variable ``Y`` is in
terms of the mean, ``mu``, and standard deviation, ``sigma``, of the
unique normally distributed random variable ``X`` such that exp(X) = Y.
This parametrization corresponds to setting ``s = sigma`` and ``scale =
exp(mu)``.

Examples
--------
>>> from scipy.stats import lognorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> s = 0.954
>>> mean, var, skew, kurt = lognorm.stats(s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(lognorm.ppf(0.01, s),
...                 lognorm.ppf(0.99, s), 100)
>>> ax.plot(x, lognorm.pdf(x, s),
...        'r-', lw=5, alpha=0.6, label='lognorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = lognorm(s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = lognorm.ppf([0.001, 0.5, 0.999], s)
>>> np.allclose([0.001, 0.5, 0.999], lognorm.cdf(vals, s))
True

Generate random numbers:

>>> r = lognorm.rvs(s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val logser : ?loc:float -> p:Py.Object.t -> unit -> [`Logser_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Logarithmic (Log-Series, Series) discrete random variable.

As an instance of the `rv_discrete` class, `logser` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `logser` is:

.. math::

    f(k) = - \frac{p^k}{k \log(1-p)}

for :math:`k \ge 1`.

`logser` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``logser.pmf(k, p, loc)`` is identically
equivalent to ``logser.pmf(k - loc, p)``.

Examples
--------
>>> from scipy.stats import logser
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.6
>>> mean, var, skew, kurt = logser.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(logser.ppf(0.01, p),
...               logser.ppf(0.99, p))
>>> ax.plot(x, logser.pmf(x, p), 'bo', ms=8, label='logser pmf')
>>> ax.vlines(x, 0, logser.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = logser(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = logser.cdf(x, p)
>>> np.allclose(x, logser.ppf(prob, p))
True

Generate random numbers:

>>> r = logser.rvs(p, size=1000)
*)

val loguniform : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> Py.Object.t
(**
A loguniform or reciprocal continuous random variable.

As an instance of the `rv_continuous` class, `loguniform` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for this class is:

.. math::

    f(x, a, b) = \frac{1}{x \log(b/a)}

for :math:`a \le x \le b`, :math:`b > a > 0`. This class takes
:math:`a` and :math:`b` as shape parameters. The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loguniform.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``loguniform.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import loguniform
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.01, 1
>>> mean, var, skew, kurt = loguniform.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loguniform.ppf(0.01, a, b),
...                 loguniform.ppf(0.99, a, b), 100)
>>> ax.plot(x, loguniform.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='loguniform pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loguniform(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loguniform.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], loguniform.cdf(vals, a, b))
True

Generate random numbers:

>>> r = loguniform.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


This doesn't show the equal probability of ``0.01``, ``0.1`` and
``1``. This is best when the x-axis is log-scaled:

>>> import numpy as np
>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log10(r))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$10^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()

This random variable will be log-uniform regardless of the base chosen for
``a`` and ``b``. Let's specify with base ``2`` instead:

>>> rvs = loguniform(2**-2, 2**0).rvs(size=1000)

Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random
variable.  Here's the histogram:

>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log2(rvs))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$2^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()
*)

val lomax : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Lomax_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Lomax (Pareto of the second kind) continuous random variable.

As an instance of the `rv_continuous` class, `lomax` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `lomax` is:

.. math::

    f(x, c) = \frac{c}{(1+x)^{c+1}}

for :math:`x \ge 0`, :math:`c > 0`.

`lomax` takes ``c`` as a shape parameter for :math:`c`.

`lomax` is a special case of `pareto` with ``loc=-1.0``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``lomax.pdf(x, c, loc, scale)`` is identically
equivalent to ``lomax.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import lomax
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.88
>>> mean, var, skew, kurt = lomax.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(lomax.ppf(0.01, c),
...                 lomax.ppf(0.99, c), 100)
>>> ax.plot(x, lomax.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='lomax pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = lomax(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = lomax.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], lomax.cdf(vals, c))
True

Generate random numbers:

>>> r = lomax.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val maxwell : ?loc:float -> ?scale:float -> unit -> [`Maxwell_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Maxwell continuous random variable.

As an instance of the `rv_continuous` class, `maxwell` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,
and given ``scale = a``, where ``a`` is the parameter used in the
Mathworld description [1]_.

The probability density function for `maxwell` is:

.. math::

    f(x) = \sqrt{2/\pi}x^2 \exp(-x^2/2)

for :math:`x >= 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``maxwell.pdf(x, loc, scale)`` is identically
equivalent to ``maxwell.pdf(y) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] http://mathworld.wolfram.com/MaxwellDistribution.html

Examples
--------
>>> from scipy.stats import maxwell
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = maxwell.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(maxwell.ppf(0.01),
...                 maxwell.ppf(0.99), 100)
>>> ax.plot(x, maxwell.pdf(x),
...        'r-', lw=5, alpha=0.6, label='maxwell pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = maxwell()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = maxwell.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], maxwell.cdf(vals))
True

Generate random numbers:

>>> r = maxwell.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val mielke : ?loc:float -> ?scale:float -> k:Py.Object.t -> s:Py.Object.t -> unit -> [`Mielke_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Mielke Beta-Kappa / Dagum continuous random variable.

As an instance of the `rv_continuous` class, `mielke` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(k, s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, k, s, loc=0, scale=1)
    Probability density function.
logpdf(x, k, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, k, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, k, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, k, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, k, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, k, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, k, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, k, s, loc=0, scale=1)
    Non-central moment of order n
stats(k, s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(k, s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, k, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(k, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(k, s, loc=0, scale=1)
    Median of the distribution.
mean(k, s, loc=0, scale=1)
    Mean of the distribution.
var(k, s, loc=0, scale=1)
    Variance of the distribution.
std(k, s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, k, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `mielke` is:

.. math::

    f(x, k, s) = \frac{k x^{k-1}}{(1+x^s)^{1+k/s}}

for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes
called Dagum distribution ([2]_). It was already defined in [3]_, called
a Burr Type III distribution (`burr` with parameters ``c=s`` and
``d=k/s``).

`mielke` takes ``k`` and ``s`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``mielke.pdf(x, k, s, loc, scale)`` is identically
equivalent to ``mielke.pdf(y, k, s) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Mielke, P.W., 1973 'Another Family of Distributions for Describing
       and Analyzing Precipitation Data.' J. Appl. Meteor., 12, 275-280
.. [2] Dagum, C., 1977 'A new model for personal income distribution.'
       Economie Appliquee, 33, 327-367.
.. [3] Burr, I. W. 'Cumulative frequency functions', Annals of
       Mathematical Statistics, 13(2), pp 215-232 (1942).

Examples
--------
>>> from scipy.stats import mielke
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> k, s = 10.4, 4.6
>>> mean, var, skew, kurt = mielke.stats(k, s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(mielke.ppf(0.01, k, s),
...                 mielke.ppf(0.99, k, s), 100)
>>> ax.plot(x, mielke.pdf(x, k, s),
...        'r-', lw=5, alpha=0.6, label='mielke pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = mielke(k, s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = mielke.ppf([0.001, 0.5, 0.999], k, s)
>>> np.allclose([0.001, 0.5, 0.999], mielke.cdf(vals, k, s))
True

Generate random numbers:

>>> r = mielke.rvs(k, s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val moyal : ?loc:float -> ?scale:float -> unit -> [`Moyal_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Moyal continuous random variable.

As an instance of the `rv_continuous` class, `moyal` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `moyal` is:

.. math::

    f(x) = \exp(-(x + \exp(-x))/2) / \sqrt{2\pi}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``moyal.pdf(x, loc, scale)`` is identically
equivalent to ``moyal.pdf(y) / scale`` with
``y = (x - loc) / scale``.

This distribution has utility in high-energy physics and radiation
detection. It describes the energy loss of a charged relativistic
particle due to ionization of the medium [1]_. It also provides an
approximation for the Landau distribution. For an in depth description
see [2]_. For additional description, see [3]_.

References
----------
.. [1] J.E. Moyal, 'XXX. Theory of ionization fluctuations',
       The London, Edinburgh, and Dublin Philosophical Magazine
       and Journal of Science, vol 46, 263-280, (1955).
       :doi:`10.1080/14786440308521076` (gated)
.. [2] G. Cordeiro et al., 'The beta Moyal: a useful skew distribution',
       International Journal of Research and Reviews in Applied Sciences,
       vol 10, 171-192, (2012).
       http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf
.. [3] C. Walck, 'Handbook on Statistical Distributions for
       Experimentalists; International Report SUF-PFY/96-01', Chapter 26,
       University of Stockholm: Stockholm, Sweden, (2007).
       http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf

.. versionadded:: 1.1.0

Examples
--------
>>> from scipy.stats import moyal
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = moyal.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(moyal.ppf(0.01),
...                 moyal.ppf(0.99), 100)
>>> ax.plot(x, moyal.pdf(x),
...        'r-', lw=5, alpha=0.6, label='moyal pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = moyal()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = moyal.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], moyal.cdf(vals))
True

Generate random numbers:

>>> r = moyal.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val nakagami : ?loc:float -> ?scale:float -> nu:Py.Object.t -> unit -> [`Nakagami_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Nakagami continuous random variable.

As an instance of the `rv_continuous` class, `nakagami` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(nu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, nu, loc=0, scale=1)
    Probability density function.
logpdf(x, nu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, nu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, nu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, nu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, nu, loc=0, scale=1)
    Log of the survival function.
ppf(q, nu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, nu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, nu, loc=0, scale=1)
    Non-central moment of order n
stats(nu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(nu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, nu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(nu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(nu, loc=0, scale=1)
    Median of the distribution.
mean(nu, loc=0, scale=1)
    Mean of the distribution.
var(nu, loc=0, scale=1)
    Variance of the distribution.
std(nu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, nu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `nakagami` is:

.. math::

    f(x, \nu) = \frac{2 \nu^\nu}{\Gamma(\nu)} x^{2\nu-1} \exp(-\nu x^2)

for :math:`x >= 0`, :math:`\nu > 0`.

`nakagami` takes ``nu`` as a shape parameter for :math:`\nu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``nakagami.pdf(x, nu, loc, scale)`` is identically
equivalent to ``nakagami.pdf(y, nu) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import nakagami
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> nu = 4.97
>>> mean, var, skew, kurt = nakagami.stats(nu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(nakagami.ppf(0.01, nu),
...                 nakagami.ppf(0.99, nu), 100)
>>> ax.plot(x, nakagami.pdf(x, nu),
...        'r-', lw=5, alpha=0.6, label='nakagami pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = nakagami(nu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = nakagami.ppf([0.001, 0.5, 0.999], nu)
>>> np.allclose([0.001, 0.5, 0.999], nakagami.cdf(vals, nu))
True

Generate random numbers:

>>> r = nakagami.rvs(nu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val nbinom : ?loc:float -> n:Py.Object.t -> p:Py.Object.t -> unit -> [`Nbinom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A negative binomial discrete random variable.

As an instance of the `rv_discrete` class, `nbinom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, p, loc=0)
    Probability mass function.
logpmf(k, n, p, loc=0)
    Log of the probability mass function.
cdf(k, n, p, loc=0)
    Cumulative distribution function.
logcdf(k, n, p, loc=0)
    Log of the cumulative distribution function.
sf(k, n, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, p, loc=0)
    Log of the survival function.
ppf(q, n, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, p, loc=0)
    Median of the distribution.
mean(n, p, loc=0)
    Mean of the distribution.
var(n, p, loc=0)
    Variance of the distribution.
std(n, p, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
Negative binomial distribution describes a sequence of i.i.d. Bernoulli
trials, repeated until a predefined, non-random number of successes occurs.

The probability mass function of the number of failures for `nbinom` is:

.. math::

   f(k) = \binom{k+n-1}{n-1} p^n (1-p)^k

for :math:`k \ge 0`.

`nbinom` takes :math:`n` and :math:`p` as shape parameters where n is the
number of successes, whereas p is the probability of a single success.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``nbinom.pmf(k, n, p, loc)`` is identically
equivalent to ``nbinom.pmf(k - loc, n, p)``.

Examples
--------
>>> from scipy.stats import nbinom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, p = 0.4, 0.4
>>> mean, var, skew, kurt = nbinom.stats(n, p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(nbinom.ppf(0.01, n, p),
...               nbinom.ppf(0.99, n, p))
>>> ax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')
>>> ax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = nbinom(n, p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = nbinom.cdf(x, n, p)
>>> np.allclose(x, nbinom.ppf(prob, n, p))
True

Generate random numbers:

>>> r = nbinom.rvs(n, p, size=1000)
*)

val ncf : ?loc:float -> ?scale:float -> dfn:Py.Object.t -> dfd:Py.Object.t -> nc:Py.Object.t -> unit -> [`Ncf_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central F distribution continuous random variable.

As an instance of the `rv_continuous` class, `ncf` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(dfn, dfd, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, dfn, dfd, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, dfn, dfd, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, dfn, dfd, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, dfn, dfd, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, dfn, dfd, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, dfn, dfd, nc, loc=0, scale=1)
    Non-central moment of order n
stats(dfn, dfd, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(dfn, dfd, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, dfn, dfd, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(dfn, dfd, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(dfn, dfd, nc, loc=0, scale=1)
    Median of the distribution.
mean(dfn, dfd, nc, loc=0, scale=1)
    Mean of the distribution.
var(dfn, dfd, nc, loc=0, scale=1)
    Variance of the distribution.
std(dfn, dfd, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, dfn, dfd, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `ncf` is:

.. math::

    f(x, n_1, n_2, \lambda) =
                      \exp(\frac{\lambda}{2} + \lambda n_1 \frac{x}{2(n_1 x+n_2)})
                      n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\
                      (n_2+n_1 x)^{-(n_1+n_2)/2}
                      \gamma(n_1/2) \gamma(1+n_2/2) \\
                     \frac{L^{\frac{v_1}{2}-1}_{v_2/2}
                           (-\lambda v_1 \frac{x}{2(v_1 x+v_2)})}
                          {B(v_1/2, v_2/2)  \gamma(\frac{v_1+v_2}{2})}

for :math:`n_1 > 1`, :math:`n_2, \lambda > 0`.  Here :math:`n_1` is the
degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in
the denominator, :math:`\lambda` the non-centrality parameter,
:math:`\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a
generalized Laguerre polynomial and :math:`B` is the beta function.

`ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ncf.pdf(x, dfn, dfd, nc, loc, scale)`` is identically
equivalent to ``ncf.pdf(y, dfn, dfd, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import ncf
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> dfn, dfd, nc = 27, 27, 0.416
>>> mean, var, skew, kurt = ncf.stats(dfn, dfd, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ncf.ppf(0.01, dfn, dfd, nc),
...                 ncf.ppf(0.99, dfn, dfd, nc), 100)
>>> ax.plot(x, ncf.pdf(x, dfn, dfd, nc),
...        'r-', lw=5, alpha=0.6, label='ncf pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ncf(dfn, dfd, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ncf.ppf([0.001, 0.5, 0.999], dfn, dfd, nc)
>>> np.allclose([0.001, 0.5, 0.999], ncf.cdf(vals, dfn, dfd, nc))
True

Generate random numbers:

>>> r = ncf.rvs(dfn, dfd, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val nct : ?loc:float -> ?scale:float -> df:Py.Object.t -> nc:Py.Object.t -> unit -> [`Nct_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central Student's t continuous random variable.

As an instance of the `rv_continuous` class, `nct` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, df, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, nc, loc=0, scale=1)
    Non-central moment of order n
stats(df, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, nc, loc=0, scale=1)
    Median of the distribution.
mean(df, nc, loc=0, scale=1)
    Mean of the distribution.
var(df, nc, loc=0, scale=1)
    Variance of the distribution.
std(df, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
If :math:`Y` is a standard normal random variable and :math:`V` is
an independent chi-square random variable (`chi2`) with :math:`k` degrees
of freedom, then

.. math::

    X = \frac{Y + c}{\sqrt{V/k}}

has a non-central Student's t distribution on the real line.
The degrees of freedom parameter :math:`k` (denoted ``df`` in the
implementation) satisfies :math:`k > 0` and the noncentrality parameter
:math:`c` (denoted ``nc`` in the implementation) is a real number.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``nct.pdf(x, df, nc, loc, scale)`` is identically
equivalent to ``nct.pdf(y, df, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import nct
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df, nc = 14, 0.24
>>> mean, var, skew, kurt = nct.stats(df, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(nct.ppf(0.01, df, nc),
...                 nct.ppf(0.99, df, nc), 100)
>>> ax.plot(x, nct.pdf(x, df, nc),
...        'r-', lw=5, alpha=0.6, label='nct pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = nct(df, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = nct.ppf([0.001, 0.5, 0.999], df, nc)
>>> np.allclose([0.001, 0.5, 0.999], nct.cdf(vals, df, nc))
True

Generate random numbers:

>>> r = nct.rvs(df, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val ncx2 : ?loc:float -> ?scale:float -> df:Py.Object.t -> nc:Py.Object.t -> unit -> [`Ncx2_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central chi-squared continuous random variable.

As an instance of the `rv_continuous` class, `ncx2` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, df, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, nc, loc=0, scale=1)
    Non-central moment of order n
stats(df, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, nc, loc=0, scale=1)
    Median of the distribution.
mean(df, nc, loc=0, scale=1)
    Mean of the distribution.
var(df, nc, loc=0, scale=1)
    Variance of the distribution.
std(df, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `ncx2` is:

.. math::

    f(x, k, \lambda) = \frac{1}{2} \exp(-(\lambda+x)/2)
        (x/\lambda)^{(k-2)/4}  I_{(k-2)/2}(\sqrt{\lambda x})

for :math:`x >= 0` and :math:`k, \lambda > 0`. :math:`k` specifies the
degrees of freedom (denoted ``df`` in the implementation) and
:math:`\lambda` is the non-centrality parameter (denoted ``nc`` in the
implementation). :math:`I_\nu` denotes the modified Bessel function of
first order of degree :math:`\nu` (`scipy.special.iv`).

`ncx2` takes ``df`` and ``nc`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ncx2.pdf(x, df, nc, loc, scale)`` is identically
equivalent to ``ncx2.pdf(y, df, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import ncx2
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df, nc = 21, 1.06
>>> mean, var, skew, kurt = ncx2.stats(df, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ncx2.ppf(0.01, df, nc),
...                 ncx2.ppf(0.99, df, nc), 100)
>>> ax.plot(x, ncx2.pdf(x, df, nc),
...        'r-', lw=5, alpha=0.6, label='ncx2 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ncx2(df, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ncx2.ppf([0.001, 0.5, 0.999], df, nc)
>>> np.allclose([0.001, 0.5, 0.999], ncx2.cdf(vals, df, nc))
True

Generate random numbers:

>>> r = ncx2.rvs(df, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val norm : ?loc:float -> ?scale:float -> unit -> [`Norm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A normal continuous random variable.

The location (``loc``) keyword specifies the mean.
The scale (``scale``) keyword specifies the standard deviation.

As an instance of the `rv_continuous` class, `norm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `norm` is:

.. math::

    f(x) = \frac{\exp(-x^2/2)}{\sqrt{2\pi}}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``norm.pdf(x, loc, scale)`` is identically
equivalent to ``norm.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import norm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = norm.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(norm.ppf(0.01),
...                 norm.ppf(0.99), 100)
>>> ax.plot(x, norm.pdf(x),
...        'r-', lw=5, alpha=0.6, label='norm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = norm()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = norm.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))
True

Generate random numbers:

>>> r = norm.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val norminvgauss : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Norminvgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Normal Inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `norminvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `norminvgauss` is:

.. math::

    f(x, a, b) = (a \exp(\sqrt{a^2 - b^2} + b x)) /
                 (\pi \sqrt{1 + x^2} \, K_1(a \sqrt{1 + x^2}))

where :math:`x` is a real number, the parameter :math:`a` is the tail
heaviness and :math:`b` is the asymmetry parameter satisfying
:math:`a > 0` and :math:`|b| <= a`.
:math:`K_1` is the modified Bessel function of second kind
(`scipy.special.k1`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``norminvgauss.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``norminvgauss.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

A normal inverse Gaussian random variable `Y` with parameters `a` and `b`
can be expressed as a normal mean-variance mixture:
`Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is
`invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used
to generate random variates.

References
----------
O. Barndorff-Nielsen, 'Hyperbolic Distributions and Distributions on
Hyperbolae', Scandinavian Journal of Statistics, Vol. 5(3),
pp. 151-157, 1978.

O. Barndorff-Nielsen, 'Normal Inverse Gaussian Distributions and Stochastic
Volatility Modelling', Scandinavian Journal of Statistics, Vol. 24,
pp. 1-13, 1997.

Examples
--------
>>> from scipy.stats import norminvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 1, 0.5
>>> mean, var, skew, kurt = norminvgauss.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(norminvgauss.ppf(0.01, a, b),
...                 norminvgauss.ppf(0.99, a, b), 100)
>>> ax.plot(x, norminvgauss.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='norminvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = norminvgauss(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = norminvgauss.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], norminvgauss.cdf(vals, a, b))
True

Generate random numbers:

>>> r = norminvgauss.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val pareto : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Pareto_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Pareto continuous random variable.

As an instance of the `rv_continuous` class, `pareto` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `pareto` is:

.. math::

    f(x, b) = \frac{b}{x^{b+1}}

for :math:`x \ge 1`, :math:`b > 0`.

`pareto` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``pareto.pdf(x, b, loc, scale)`` is identically
equivalent to ``pareto.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import pareto
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 2.62
>>> mean, var, skew, kurt = pareto.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(pareto.ppf(0.01, b),
...                 pareto.ppf(0.99, b), 100)
>>> ax.plot(x, pareto.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='pareto pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = pareto(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = pareto.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], pareto.cdf(vals, b))
True

Generate random numbers:

>>> r = pareto.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val pearson3 : ?loc:float -> ?scale:float -> skew:Py.Object.t -> unit -> [`Object|`Pearson3_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A pearson type III continuous random variable.

As an instance of the `rv_continuous` class, `pearson3` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(skew, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, skew, loc=0, scale=1)
    Probability density function.
logpdf(x, skew, loc=0, scale=1)
    Log of the probability density function.
cdf(x, skew, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, skew, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, skew, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, skew, loc=0, scale=1)
    Log of the survival function.
ppf(q, skew, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, skew, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, skew, loc=0, scale=1)
    Non-central moment of order n
stats(skew, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(skew, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, skew, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(skew,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(skew, loc=0, scale=1)
    Median of the distribution.
mean(skew, loc=0, scale=1)
    Mean of the distribution.
var(skew, loc=0, scale=1)
    Variance of the distribution.
std(skew, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, skew, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `pearson3` is:

.. math::

    f(x, skew) = \frac{ |\beta| }{\Gamma(\alpha)}
                 (\beta (x - \zeta))^{\alpha - 1}
                 \exp(-\beta (x - \zeta))

where:

.. math::

        \beta = \frac{2}{skew  stddev}
        \alpha = (stddev \beta)^2
        \zeta = loc - \frac{\alpha}{\beta}

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).
`pearson3` takes ``skew`` as a shape parameter for :math:`skew`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``pearson3.pdf(x, skew, loc, scale)`` is identically
equivalent to ``pearson3.pdf(y, skew) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import pearson3
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> skew = 0.1
>>> mean, var, skew, kurt = pearson3.stats(skew, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(pearson3.ppf(0.01, skew),
...                 pearson3.ppf(0.99, skew), 100)
>>> ax.plot(x, pearson3.pdf(x, skew),
...        'r-', lw=5, alpha=0.6, label='pearson3 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = pearson3(skew)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = pearson3.ppf([0.001, 0.5, 0.999], skew)
>>> np.allclose([0.001, 0.5, 0.999], pearson3.cdf(vals, skew))
True

Generate random numbers:

>>> r = pearson3.rvs(skew, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


References
----------
R.W. Vogel and D.E. McMartin, 'Probability Plot Goodness-of-Fit and
Skewness Estimation Procedures for the Pearson Type 3 Distribution', Water
Resources Research, Vol.27, 3149-3158 (1991).

L.R. Salvosa, 'Tables of Pearson's Type III Function', Ann. Math. Statist.,
Vol.1, 191-198 (1930).

'Using Modern Computing Tools to Fit the Pearson Type III Distribution to
Aviation Loads Data', Office of Aviation Research (2003).
*)

val planck : ?loc:float -> lambda_:Py.Object.t -> unit -> [`Object|`Planck_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Planck discrete exponential random variable.

As an instance of the `rv_discrete` class, `planck` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lambda_, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, lambda_, loc=0)
    Probability mass function.
logpmf(k, lambda_, loc=0)
    Log of the probability mass function.
cdf(k, lambda_, loc=0)
    Cumulative distribution function.
logcdf(k, lambda_, loc=0)
    Log of the cumulative distribution function.
sf(k, lambda_, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, lambda_, loc=0)
    Log of the survival function.
ppf(q, lambda_, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lambda_, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(lambda_, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lambda_, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(lambda_,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(lambda_, loc=0)
    Median of the distribution.
mean(lambda_, loc=0)
    Mean of the distribution.
var(lambda_, loc=0)
    Variance of the distribution.
std(lambda_, loc=0)
    Standard deviation of the distribution.
interval(alpha, lambda_, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `planck` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k)

for :math:`k \ge 0` and :math:`\lambda > 0`.

`planck` takes :math:`\lambda` as shape parameter. The Planck distribution
can be written as a geometric distribution (`geom`) with
:math:`p = 1 - \exp(-\lambda)` shifted by `loc = -1`.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``planck.pmf(k, lambda_, loc)`` is identically
equivalent to ``planck.pmf(k - loc, lambda_)``.

See Also
--------
geom

Examples
--------
>>> from scipy.stats import planck
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lambda_ = 0.51
>>> mean, var, skew, kurt = planck.stats(lambda_, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(planck.ppf(0.01, lambda_),
...               planck.ppf(0.99, lambda_))
>>> ax.plot(x, planck.pmf(x, lambda_), 'bo', ms=8, label='planck pmf')
>>> ax.vlines(x, 0, planck.pmf(x, lambda_), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = planck(lambda_)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = planck.cdf(x, lambda_)
>>> np.allclose(x, planck.ppf(prob, lambda_))
True

Generate random numbers:

>>> r = planck.rvs(lambda_, size=1000)
*)

val poisson : ?loc:float -> mu:Py.Object.t -> unit -> [`Object|`Poisson_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Poisson discrete random variable.

As an instance of the `rv_discrete` class, `poisson` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, mu, loc=0)
    Probability mass function.
logpmf(k, mu, loc=0)
    Log of the probability mass function.
cdf(k, mu, loc=0)
    Cumulative distribution function.
logcdf(k, mu, loc=0)
    Log of the cumulative distribution function.
sf(k, mu, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, mu, loc=0)
    Log of the survival function.
ppf(q, mu, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(mu, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(mu,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0)
    Median of the distribution.
mean(mu, loc=0)
    Mean of the distribution.
var(mu, loc=0)
    Variance of the distribution.
std(mu, loc=0)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `poisson` is:

.. math::

    f(k) = \exp(-\mu) \frac{\mu^k}{k!}

for :math:`k \ge 0`.

`poisson` takes :math:`\mu` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``poisson.pmf(k, mu, loc)`` is identically
equivalent to ``poisson.pmf(k - loc, mu)``.

Examples
--------
>>> from scipy.stats import poisson
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.6
>>> mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(poisson.ppf(0.01, mu),
...               poisson.ppf(0.99, mu))
>>> ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')
>>> ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = poisson(mu)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = poisson.cdf(x, mu)
>>> np.allclose(x, poisson.ppf(prob, mu))
True

Generate random numbers:

>>> r = poisson.rvs(mu, size=1000)
*)

val powerlaw : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Object|`Powerlaw_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power-function continuous random variable.

As an instance of the `rv_continuous` class, `powerlaw` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powerlaw` is:

.. math::

    f(x, a) = a x^{a-1}

for :math:`0 \le x \le 1`, :math:`a > 0`.

`powerlaw` takes ``a`` as a shape parameter for :math:`a`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powerlaw.pdf(x, a, loc, scale)`` is identically
equivalent to ``powerlaw.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

`powerlaw` is a special case of `beta` with ``b=1``.

Examples
--------
>>> from scipy.stats import powerlaw
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.66
>>> mean, var, skew, kurt = powerlaw.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powerlaw.ppf(0.01, a),
...                 powerlaw.ppf(0.99, a), 100)
>>> ax.plot(x, powerlaw.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='powerlaw pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powerlaw(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powerlaw.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], powerlaw.cdf(vals, a))
True

Generate random numbers:

>>> r = powerlaw.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val powerlognorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> s:Py.Object.t -> unit -> [`Object|`Powerlognorm_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power log-normal continuous random variable.

As an instance of the `rv_continuous` class, `powerlognorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, s, loc=0, scale=1)
    Probability density function.
logpdf(x, c, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, s, loc=0, scale=1)
    Non-central moment of order n
stats(c, s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, s, loc=0, scale=1)
    Median of the distribution.
mean(c, s, loc=0, scale=1)
    Mean of the distribution.
var(c, s, loc=0, scale=1)
    Variance of the distribution.
std(c, s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powerlognorm` is:

.. math::

    f(x, c, s) = \frac{c}{x s} \phi(\log(x)/s)
                 (\Phi(-\log(x)/s))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x > 0`, :math:`s, c > 0`.

`powerlognorm` takes :math:`c` and :math:`s` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powerlognorm.pdf(x, c, s, loc, scale)`` is identically
equivalent to ``powerlognorm.pdf(y, c, s) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import powerlognorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, s = 2.14, 0.446
>>> mean, var, skew, kurt = powerlognorm.stats(c, s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powerlognorm.ppf(0.01, c, s),
...                 powerlognorm.ppf(0.99, c, s), 100)
>>> ax.plot(x, powerlognorm.pdf(x, c, s),
...        'r-', lw=5, alpha=0.6, label='powerlognorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powerlognorm(c, s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powerlognorm.ppf([0.001, 0.5, 0.999], c, s)
>>> np.allclose([0.001, 0.5, 0.999], powerlognorm.cdf(vals, c, s))
True

Generate random numbers:

>>> r = powerlognorm.rvs(c, s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val powernorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Powernorm_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power normal continuous random variable.

As an instance of the `rv_continuous` class, `powernorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powernorm` is:

.. math::

    f(x, c) = c \phi(x) (\Phi(-x))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x >= 0`, :math:`c > 0`.

`powernorm` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powernorm.pdf(x, c, loc, scale)`` is identically
equivalent to ``powernorm.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import powernorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 4.45
>>> mean, var, skew, kurt = powernorm.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powernorm.ppf(0.01, c),
...                 powernorm.ppf(0.99, c), 100)
>>> ax.plot(x, powernorm.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='powernorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powernorm(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powernorm.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], powernorm.cdf(vals, c))
True

Generate random numbers:

>>> r = powernorm.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val randint : ?loc:float -> low:Py.Object.t -> high:Py.Object.t -> unit -> [`Object|`Randint_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A uniform discrete random variable.

As an instance of the `rv_discrete` class, `randint` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(low, high, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, low, high, loc=0)
    Probability mass function.
logpmf(k, low, high, loc=0)
    Log of the probability mass function.
cdf(k, low, high, loc=0)
    Cumulative distribution function.
logcdf(k, low, high, loc=0)
    Log of the cumulative distribution function.
sf(k, low, high, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, low, high, loc=0)
    Log of the survival function.
ppf(q, low, high, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, low, high, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(low, high, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(low, high, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(low, high), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(low, high, loc=0)
    Median of the distribution.
mean(low, high, loc=0)
    Mean of the distribution.
var(low, high, loc=0)
    Variance of the distribution.
std(low, high, loc=0)
    Standard deviation of the distribution.
interval(alpha, low, high, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `randint` is:

.. math::

    f(k) = \frac{1}{high - low}

for ``k = low, ..., high - 1``.

`randint` takes ``low`` and ``high`` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``randint.pmf(k, low, high, loc)`` is identically
equivalent to ``randint.pmf(k - loc, low, high)``.

Examples
--------
>>> from scipy.stats import randint
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> low, high = 7, 31
>>> mean, var, skew, kurt = randint.stats(low, high, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(randint.ppf(0.01, low, high),
...               randint.ppf(0.99, low, high))
>>> ax.plot(x, randint.pmf(x, low, high), 'bo', ms=8, label='randint pmf')
>>> ax.vlines(x, 0, randint.pmf(x, low, high), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = randint(low, high)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = randint.cdf(x, low, high)
>>> np.allclose(x, randint.ppf(prob, low, high))
True

Generate random numbers:

>>> r = randint.rvs(low, high, size=1000)
*)

val rayleigh : ?loc:float -> ?scale:float -> unit -> [`Object|`Rayleigh_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Rayleigh continuous random variable.

As an instance of the `rv_continuous` class, `rayleigh` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rayleigh` is:

.. math::

    f(x) = x \exp(-x^2/2)

for :math:`x \ge 0`.

`rayleigh` is a special case of `chi` with ``df=2``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rayleigh.pdf(x, loc, scale)`` is identically
equivalent to ``rayleigh.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import rayleigh
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = rayleigh.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rayleigh.ppf(0.01),
...                 rayleigh.ppf(0.99), 100)
>>> ax.plot(x, rayleigh.pdf(x),
...        'r-', lw=5, alpha=0.6, label='rayleigh pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rayleigh()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rayleigh.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], rayleigh.cdf(vals))
True

Generate random numbers:

>>> r = rayleigh.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val rdist : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rdist_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An R-distributed (symmetric beta) continuous random variable.

As an instance of the `rv_continuous` class, `rdist` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rdist` is:

.. math::

    f(x, c) = \frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}

for :math:`-1 \le x \le 1`, :math:`c > 0`. `rdist` is also called the
symmetric beta distribution: if B has a `beta` distribution with
parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with
parameter c.

`rdist` takes ``c`` as a shape parameter for :math:`c`.

This distribution includes the following distribution kernels as
special cases::

    c = 2:  uniform
    c = 3:  `semicircular`
    c = 4:  Epanechnikov (parabolic)
    c = 6:  quartic (biweight)
    c = 8:  triweight

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rdist.pdf(x, c, loc, scale)`` is identically
equivalent to ``rdist.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import rdist
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.6
>>> mean, var, skew, kurt = rdist.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rdist.ppf(0.01, c),
...                 rdist.ppf(0.99, c), 100)
>>> ax.plot(x, rdist.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='rdist pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rdist(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rdist.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], rdist.cdf(vals, c))
True

Generate random numbers:

>>> r = rdist.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val recipinvgauss : ?loc:float -> ?scale:float -> mu:Py.Object.t -> unit -> [`Object|`Recipinvgauss_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A reciprocal inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `recipinvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, mu, loc=0, scale=1)
    Probability density function.
logpdf(x, mu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, mu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, mu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, mu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, mu, loc=0, scale=1)
    Log of the survival function.
ppf(q, mu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, mu, loc=0, scale=1)
    Non-central moment of order n
stats(mu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, mu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0, scale=1)
    Median of the distribution.
mean(mu, loc=0, scale=1)
    Mean of the distribution.
var(mu, loc=0, scale=1)
    Variance of the distribution.
std(mu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `recipinvgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2\pi x}}
                \exp\left(\frac{-(1-\mu x)^2}{2\mu^2x}\right)

for :math:`x \ge 0`.

`recipinvgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``recipinvgauss.pdf(x, mu, loc, scale)`` is identically
equivalent to ``recipinvgauss.pdf(y, mu) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import recipinvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.63
>>> mean, var, skew, kurt = recipinvgauss.stats(mu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(recipinvgauss.ppf(0.01, mu),
...                 recipinvgauss.ppf(0.99, mu), 100)
>>> ax.plot(x, recipinvgauss.pdf(x, mu),
...        'r-', lw=5, alpha=0.6, label='recipinvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = recipinvgauss(mu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = recipinvgauss.ppf([0.001, 0.5, 0.999], mu)
>>> np.allclose([0.001, 0.5, 0.999], recipinvgauss.cdf(vals, mu))
True

Generate random numbers:

>>> r = recipinvgauss.rvs(mu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val reciprocal : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Object|`Reciprocal_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A loguniform or reciprocal continuous random variable.

As an instance of the `rv_continuous` class, `reciprocal` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for this class is:

.. math::

    f(x, a, b) = \frac{1}{x \log(b/a)}

for :math:`a \le x \le b`, :math:`b > a > 0`. This class takes
:math:`a` and :math:`b` as shape parameters. The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``reciprocal.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``reciprocal.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import reciprocal
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.01, 1
>>> mean, var, skew, kurt = reciprocal.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(reciprocal.ppf(0.01, a, b),
...                 reciprocal.ppf(0.99, a, b), 100)
>>> ax.plot(x, reciprocal.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='reciprocal pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = reciprocal(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = reciprocal.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], reciprocal.cdf(vals, a, b))
True

Generate random numbers:

>>> r = reciprocal.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


This doesn't show the equal probability of ``0.01``, ``0.1`` and
``1``. This is best when the x-axis is log-scaled:

>>> import numpy as np
>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log10(r))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$10^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()

This random variable will be log-uniform regardless of the base chosen for
``a`` and ``b``. Let's specify with base ``2`` instead:

>>> rvs = reciprocal(2**-2, 2**0).rvs(size=1000)

Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random
variable.  Here's the histogram:

>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log2(rvs))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$2^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()
*)

val rice : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Rice_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Rice continuous random variable.

As an instance of the `rv_continuous` class, `rice` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rice` is:

.. math::

    f(x, b) = x \exp(- \frac{x^2 + b^2}{2}) I_0(x b)

for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel
function of order zero (`scipy.special.i0`).

`rice` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rice.pdf(x, b, loc, scale)`` is identically
equivalent to ``rice.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

The Rice distribution describes the length, :math:`r`, of a 2-D vector with
components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,
v` are independent Gaussian random variables with standard deviation
:math:`s`.  Let :math:`R = \sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is
``rice.pdf(x, R/s, scale=s)``.

Examples
--------
>>> from scipy.stats import rice
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 0.775
>>> mean, var, skew, kurt = rice.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rice.ppf(0.01, b),
...                 rice.ppf(0.99, b), 100)
>>> ax.plot(x, rice.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='rice pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rice(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rice.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], rice.cdf(vals, b))
True

Generate random numbers:

>>> r = rice.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val semicircular : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Semicircular_gen] Np.Obj.t
(**
A semicircular continuous random variable.

As an instance of the `rv_continuous` class, `semicircular` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `semicircular` is:

.. math::

    f(x) = \frac{2}{\pi} \sqrt{1-x^2}

for :math:`-1 \le x \le 1`.

The distribution is a special case of `rdist` with `c = 3`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``semicircular.pdf(x, loc, scale)`` is identically
equivalent to ``semicircular.pdf(y) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
rdist

References
----------
.. [1] 'Wigner semicircle distribution',
       https://en.wikipedia.org/wiki/Wigner_semicircle_distribution

Examples
--------
>>> from scipy.stats import semicircular
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = semicircular.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(semicircular.ppf(0.01),
...                 semicircular.ppf(0.99), 100)
>>> ax.plot(x, semicircular.pdf(x),
...        'r-', lw=5, alpha=0.6, label='semicircular pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = semicircular()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = semicircular.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], semicircular.cdf(vals))
True

Generate random numbers:

>>> r = semicircular.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val skellam : ?loc:float -> mu1:Py.Object.t -> mu2:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Skellam_gen] Np.Obj.t
(**
A  Skellam discrete random variable.

As an instance of the `rv_discrete` class, `skellam` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu1, mu2, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, mu1, mu2, loc=0)
    Probability mass function.
logpmf(k, mu1, mu2, loc=0)
    Log of the probability mass function.
cdf(k, mu1, mu2, loc=0)
    Cumulative distribution function.
logcdf(k, mu1, mu2, loc=0)
    Log of the cumulative distribution function.
sf(k, mu1, mu2, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, mu1, mu2, loc=0)
    Log of the survival function.
ppf(q, mu1, mu2, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu1, mu2, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(mu1, mu2, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu1, mu2, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(mu1, mu2), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu1, mu2, loc=0)
    Median of the distribution.
mean(mu1, mu2, loc=0)
    Mean of the distribution.
var(mu1, mu2, loc=0)
    Variance of the distribution.
std(mu1, mu2, loc=0)
    Standard deviation of the distribution.
interval(alpha, mu1, mu2, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
Probability distribution of the difference of two correlated or
uncorrelated Poisson random variables.

Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with
expected values :math:`\lambda_1` and :math:`\lambda_2`. Then,
:math:`k_1 - k_2` follows a Skellam distribution with parameters
:math:`\mu_1 = \lambda_1 - \rho \sqrt{\lambda_1 \lambda_2}` and
:math:`\mu_2 = \lambda_2 - \rho \sqrt{\lambda_1 \lambda_2}`, where
:math:`\rho` is the correlation coefficient between :math:`k_1` and
:math:`k_2`. If the two Poisson-distributed r.v. are independent then
:math:`\rho = 0`.

Parameters :math:`\mu_1` and :math:`\mu_2` must be strictly positive.

For details see: https://en.wikipedia.org/wiki/Skellam_distribution

`skellam` takes :math:`\mu_1` and :math:`\mu_2` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``skellam.pmf(k, mu1, mu2, loc)`` is identically
equivalent to ``skellam.pmf(k - loc, mu1, mu2)``.

Examples
--------
>>> from scipy.stats import skellam
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu1, mu2 = 15, 8
>>> mean, var, skew, kurt = skellam.stats(mu1, mu2, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(skellam.ppf(0.01, mu1, mu2),
...               skellam.ppf(0.99, mu1, mu2))
>>> ax.plot(x, skellam.pmf(x, mu1, mu2), 'bo', ms=8, label='skellam pmf')
>>> ax.vlines(x, 0, skellam.pmf(x, mu1, mu2), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = skellam(mu1, mu2)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = skellam.cdf(x, mu1, mu2)
>>> np.allclose(x, skellam.ppf(prob, mu1, mu2))
True

Generate random numbers:

>>> r = skellam.rvs(mu1, mu2, size=1000)
*)

val skewnorm : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> Py.Object.t
(**
A skew-normal random variable.

As an instance of the `rv_continuous` class, `skewnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The pdf is::

    skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)

`skewnorm` takes a real number :math:`a` as a skewness parameter
When ``a = 0`` the distribution is identical to a normal distribution
(`norm`). `rvs` implements the method of [1]_.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``skewnorm.pdf(x, a, loc, scale)`` is identically
equivalent to ``skewnorm.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import skewnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 4
>>> mean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(skewnorm.ppf(0.01, a),
...                 skewnorm.ppf(0.99, a), 100)
>>> ax.plot(x, skewnorm.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='skewnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = skewnorm(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = skewnorm.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], skewnorm.cdf(vals, a))
True

Generate random numbers:

>>> r = skewnorm.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


References
----------
.. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of the
    multivariate skew-normal distribution. J. Roy. Statist. Soc., B 61, 579-602.
    https://arxiv.org/abs/0911.2093
*)

val t : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`T_gen] Np.Obj.t
(**
A Student's t continuous random variable.

As an instance of the `rv_continuous` class, `t` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `t` is:

.. math::

    f(x, \nu) = \frac{\Gamma((\nu+1)/2)}
                    {\sqrt{\pi \nu} \Gamma(\nu/2)}
                (1+x^2/\nu)^{-(\nu+1)/2}

where :math:`x` is a real number and the degrees of freedom parameter
:math:`\nu` (denoted ``df`` in the implementation) satisfies
:math:`\nu > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``t.pdf(x, df, loc, scale)`` is identically
equivalent to ``t.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import t
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 2.74
>>> mean, var, skew, kurt = t.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(t.ppf(0.01, df),
...                 t.ppf(0.99, df), 100)
>>> ax.plot(x, t.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='t pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = t(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = t.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))
True

Generate random numbers:

>>> r = t.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val trapz : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Trapz_gen] Np.Obj.t
(**
A trapezoidal continuous random variable.

As an instance of the `rv_continuous` class, `trapz` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The trapezoidal distribution can be represented with an up-sloping line
from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``
and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.

`trapz` takes :math:`c` and :math:`d` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``trapz.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``trapz.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

Examples
--------
>>> from scipy.stats import trapz
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 0.2, 0.8
>>> mean, var, skew, kurt = trapz.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(trapz.ppf(0.01, c, d),
...                 trapz.ppf(0.99, c, d), 100)
>>> ax.plot(x, trapz.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='trapz pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = trapz(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = trapz.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], trapz.cdf(vals, c, d))
True

Generate random numbers:

>>> r = trapz.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val triang : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Triang_gen] Np.Obj.t
(**
A triangular continuous random variable.

As an instance of the `rv_continuous` class, `triang` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The triangular distribution can be represented with an up-sloping line from
``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``
to ``(loc + scale)``.

`triang` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``triang.pdf(x, c, loc, scale)`` is identically
equivalent to ``triang.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

Examples
--------
>>> from scipy.stats import triang
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.158
>>> mean, var, skew, kurt = triang.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(triang.ppf(0.01, c),
...                 triang.ppf(0.99, c), 100)
>>> ax.plot(x, triang.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='triang pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = triang(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = triang.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], triang.cdf(vals, c))
True

Generate random numbers:

>>> r = triang.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val truncexpon : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Truncexpon_gen] Np.Obj.t
(**
A truncated exponential continuous random variable.

As an instance of the `rv_continuous` class, `truncexpon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `truncexpon` is:

.. math::

    f(x, b) = \frac{\exp(-x)}{1 - \exp(-b)}

for :math:`0 <= x <= b`.

`truncexpon` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``truncexpon.pdf(x, b, loc, scale)`` is identically
equivalent to ``truncexpon.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import truncexpon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 4.69
>>> mean, var, skew, kurt = truncexpon.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(truncexpon.ppf(0.01, b),
...                 truncexpon.ppf(0.99, b), 100)
>>> ax.plot(x, truncexpon.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='truncexpon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = truncexpon(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = truncexpon.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], truncexpon.cdf(vals, b))
True

Generate random numbers:

>>> r = truncexpon.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val truncnorm : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Truncnorm_gen] Np.Obj.t
(**
A truncated normal continuous random variable.

As an instance of the `rv_continuous` class, `truncnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The standard form of this distribution is a standard normal truncated to
the range [a, b] --- notice that a and b are defined over the domain of the
standard normal.  To convert clip values for a specific mean and standard
deviation, use::

    a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std

`truncnorm` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``truncnorm.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``truncnorm.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import truncnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.1, 2
>>> mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(truncnorm.ppf(0.01, a, b),
...                 truncnorm.ppf(0.99, a, b), 100)
>>> ax.plot(x, truncnorm.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='truncnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = truncnorm(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = truncnorm.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], truncnorm.cdf(vals, a, b))
True

Generate random numbers:

>>> r = truncnorm.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val tukeylambda : ?loc:float -> ?scale:float -> lam:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Tukeylambda_gen] Np.Obj.t
(**
A Tukey-Lamdba continuous random variable.

As an instance of the `rv_continuous` class, `tukeylambda` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lam, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, lam, loc=0, scale=1)
    Probability density function.
logpdf(x, lam, loc=0, scale=1)
    Log of the probability density function.
cdf(x, lam, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, lam, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, lam, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, lam, loc=0, scale=1)
    Log of the survival function.
ppf(q, lam, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lam, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, lam, loc=0, scale=1)
    Non-central moment of order n
stats(lam, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lam, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, lam, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(lam,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(lam, loc=0, scale=1)
    Median of the distribution.
mean(lam, loc=0, scale=1)
    Mean of the distribution.
var(lam, loc=0, scale=1)
    Variance of the distribution.
std(lam, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, lam, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
A flexible distribution, able to represent and interpolate between the
following distributions:

- Cauchy                (:math:`lambda = -1`)
- logistic              (:math:`lambda = 0`)
- approx Normal         (:math:`lambda = 0.14`)
- uniform from -1 to 1  (:math:`lambda = 1`)

`tukeylambda` takes a real number :math:`lambda` (denoted ``lam``
in the implementation) as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``tukeylambda.pdf(x, lam, loc, scale)`` is identically
equivalent to ``tukeylambda.pdf(y, lam) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import tukeylambda
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lam = 3.13
>>> mean, var, skew, kurt = tukeylambda.stats(lam, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(tukeylambda.ppf(0.01, lam),
...                 tukeylambda.ppf(0.99, lam), 100)
>>> ax.plot(x, tukeylambda.pdf(x, lam),
...        'r-', lw=5, alpha=0.6, label='tukeylambda pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = tukeylambda(lam)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = tukeylambda.ppf([0.001, 0.5, 0.999], lam)
>>> np.allclose([0.001, 0.5, 0.999], tukeylambda.cdf(vals, lam))
True

Generate random numbers:

>>> r = tukeylambda.rvs(lam, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val uniform : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Uniform_gen] Np.Obj.t
(**
A uniform continuous random variable.

In the standard form, the distribution is uniform on ``[0, 1]``. Using
the parameters ``loc`` and ``scale``, one obtains the uniform distribution
on ``[loc, loc + scale]``.

As an instance of the `rv_continuous` class, `uniform` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Examples
--------
>>> from scipy.stats import uniform
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = uniform.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(uniform.ppf(0.01),
...                 uniform.ppf(0.99), 100)
>>> ax.plot(x, uniform.pdf(x),
...        'r-', lw=5, alpha=0.6, label='uniform pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = uniform()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = uniform.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], uniform.cdf(vals))
True

Generate random numbers:

>>> r = uniform.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val vonmises : ?loc:float -> ?scale:float -> kappa:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Vonmises_gen] Np.Obj.t
(**
A Von Mises continuous random variable.

As an instance of the `rv_continuous` class, `vonmises` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(kappa, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, kappa, loc=0, scale=1)
    Probability density function.
logpdf(x, kappa, loc=0, scale=1)
    Log of the probability density function.
cdf(x, kappa, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, kappa, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, kappa, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, kappa, loc=0, scale=1)
    Log of the survival function.
ppf(q, kappa, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, kappa, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, kappa, loc=0, scale=1)
    Non-central moment of order n
stats(kappa, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(kappa, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, kappa, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(kappa, loc=0, scale=1)
    Median of the distribution.
mean(kappa, loc=0, scale=1)
    Mean of the distribution.
var(kappa, loc=0, scale=1)
    Variance of the distribution.
std(kappa, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, kappa, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `vonmises` and `vonmises_line` is:

.. math::

    f(x, \kappa) = \frac{ \exp(\kappa \cos(x)) }{ 2 \pi I_0(\kappa) }

for :math:`-\pi \le x \le \pi`, :math:`\kappa > 0`. :math:`I_0` is the
modified Bessel function of order zero (`scipy.special.i0`).

`vonmises` is a circular distribution which does not restrict the
distribution to a fixed interval. Currently, there is no circular
distribution framework in scipy. The ``cdf`` is implemented such that
``cdf(x + 2*np.pi) == cdf(x) + 1``.

`vonmises_line` is the same distribution, defined on :math:`[-\pi, \pi]`
on the real line. This is a regular (i.e. non-circular) distribution.

`vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``vonmises.pdf(x, kappa, loc, scale)`` is identically
equivalent to ``vonmises.pdf(y, kappa) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import vonmises
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> kappa = 3.99
>>> mean, var, skew, kurt = vonmises.stats(kappa, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(vonmises.ppf(0.01, kappa),
...                 vonmises.ppf(0.99, kappa), 100)
>>> ax.plot(x, vonmises.pdf(x, kappa),
...        'r-', lw=5, alpha=0.6, label='vonmises pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = vonmises(kappa)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = vonmises.ppf([0.001, 0.5, 0.999], kappa)
>>> np.allclose([0.001, 0.5, 0.999], vonmises.cdf(vals, kappa))
True

Generate random numbers:

>>> r = vonmises.rvs(kappa, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val vonmises_line : ?loc:float -> ?scale:float -> kappa:Py.Object.t -> unit -> Py.Object.t
(**
A Von Mises continuous random variable.

As an instance of the `rv_continuous` class, `vonmises_line` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(kappa, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, kappa, loc=0, scale=1)
    Probability density function.
logpdf(x, kappa, loc=0, scale=1)
    Log of the probability density function.
cdf(x, kappa, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, kappa, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, kappa, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, kappa, loc=0, scale=1)
    Log of the survival function.
ppf(q, kappa, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, kappa, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, kappa, loc=0, scale=1)
    Non-central moment of order n
stats(kappa, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(kappa, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, kappa, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(kappa, loc=0, scale=1)
    Median of the distribution.
mean(kappa, loc=0, scale=1)
    Mean of the distribution.
var(kappa, loc=0, scale=1)
    Variance of the distribution.
std(kappa, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, kappa, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `vonmises` and `vonmises_line` is:

.. math::

    f(x, \kappa) = \frac{ \exp(\kappa \cos(x)) }{ 2 \pi I_0(\kappa) }

for :math:`-\pi \le x \le \pi`, :math:`\kappa > 0`. :math:`I_0` is the
modified Bessel function of order zero (`scipy.special.i0`).

`vonmises` is a circular distribution which does not restrict the
distribution to a fixed interval. Currently, there is no circular
distribution framework in scipy. The ``cdf`` is implemented such that
``cdf(x + 2*np.pi) == cdf(x) + 1``.

`vonmises_line` is the same distribution, defined on :math:`[-\pi, \pi]`
on the real line. This is a regular (i.e. non-circular) distribution.

`vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``vonmises_line.pdf(x, kappa, loc, scale)`` is identically
equivalent to ``vonmises_line.pdf(y, kappa) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import vonmises_line
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> kappa = 3.99
>>> mean, var, skew, kurt = vonmises_line.stats(kappa, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(vonmises_line.ppf(0.01, kappa),
...                 vonmises_line.ppf(0.99, kappa), 100)
>>> ax.plot(x, vonmises_line.pdf(x, kappa),
...        'r-', lw=5, alpha=0.6, label='vonmises_line pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = vonmises_line(kappa)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = vonmises_line.ppf([0.001, 0.5, 0.999], kappa)
>>> np.allclose([0.001, 0.5, 0.999], vonmises_line.cdf(vals, kappa))
True

Generate random numbers:

>>> r = vonmises_line.rvs(kappa, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val wald : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Wald_gen] Np.Obj.t
(**
A Wald continuous random variable.

As an instance of the `rv_continuous` class, `wald` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `wald` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp(- \frac{ (x-1)^2 }{ 2x })

for :math:`x >= 0`.

`wald` is a special case of `invgauss` with ``mu=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``wald.pdf(x, loc, scale)`` is identically
equivalent to ``wald.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import wald
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = wald.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(wald.ppf(0.01),
...                 wald.ppf(0.99), 100)
>>> ax.plot(x, wald.pdf(x),
...        'r-', lw=5, alpha=0.6, label='wald pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = wald()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = wald.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], wald.cdf(vals))
True

Generate random numbers:

>>> r = wald.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val weibull_max : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Weibull_max_gen] Np.Obj.t
(**
Weibull maximum continuous random variable.

As an instance of the `rv_continuous` class, `weibull_max` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min

Notes
-----
The probability density function for `weibull_max` is:

.. math::

    f(x, c) = c (-x)^{c-1} \exp(-(-x)^c)

for :math:`x < 0`, :math:`c > 0`.

`weibull_max` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``weibull_max.pdf(x, c, loc, scale)`` is identically
equivalent to ``weibull_max.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import weibull_max
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 2.87
>>> mean, var, skew, kurt = weibull_max.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(weibull_max.ppf(0.01, c),
...                 weibull_max.ppf(0.99, c), 100)
>>> ax.plot(x, weibull_max.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='weibull_max pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = weibull_max(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = weibull_max.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], weibull_max.cdf(vals, c))
True

Generate random numbers:

>>> r = weibull_max.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val weibull_min : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Weibull_min_gen] Np.Obj.t
(**
Weibull minimum continuous random variable.

The Weibull Minimum Extreme Value distribution, from extreme value theory,
is also often simply called the Weibull distribution.

As an instance of the `rv_continuous` class, `weibull_min` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_max, numpy.random.mtrand.RandomState.weibull, exponweib

Notes
-----
The probability density function for `weibull_min` is:

.. math::

    f(x, c) = c x^{c-1} \exp(-x^c)

for :math:`x >= 0`, :math:`c > 0`.

`weibull_min` takes ``c`` as a shape parameter for :math:`c`.
(named :math:`k` in Wikipedia article and :math:`a` in
``numpy.random.weibull``).  Special shape values are :math:`c=1` and
:math:`c=2` where Weibull distribution reduces to the `expon` and
`rayleigh` distributions respectively.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``weibull_min.pdf(x, c, loc, scale)`` is identically
equivalent to ``weibull_min.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
https://en.wikipedia.org/wiki/Weibull_distribution

Examples
--------
>>> from scipy.stats import weibull_min
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.79
>>> mean, var, skew, kurt = weibull_min.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(weibull_min.ppf(0.01, c),
...                 weibull_min.ppf(0.99, c), 100)
>>> ax.plot(x, weibull_min.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='weibull_min pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = weibull_min(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = weibull_min.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], weibull_min.cdf(vals, c))
True

Generate random numbers:

>>> r = weibull_min.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val wrapcauchy : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Wrapcauchy_gen] Np.Obj.t
(**
A wrapped Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `wrapcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `wrapcauchy` is:

.. math::

    f(x, c) = \frac{1-c^2}{2\pi (1+c^2 - 2c \cos(x))}

for :math:`0 \le x \le 2\pi`, :math:`0 < c < 1`.

`wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``wrapcauchy.pdf(x, c, loc, scale)`` is identically
equivalent to ``wrapcauchy.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import wrapcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.0311
>>> mean, var, skew, kurt = wrapcauchy.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(wrapcauchy.ppf(0.01, c),
...                 wrapcauchy.ppf(0.99, c), 100)
>>> ax.plot(x, wrapcauchy.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='wrapcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = wrapcauchy(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = wrapcauchy.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], wrapcauchy.cdf(vals, c))
True

Generate random numbers:

>>> r = wrapcauchy.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val yulesimon : ?loc:float -> alpha:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Yulesimon_gen] Np.Obj.t
(**
A Yule-Simon discrete random variable.

As an instance of the `rv_discrete` class, `yulesimon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(alpha, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, alpha, loc=0)
    Probability mass function.
logpmf(k, alpha, loc=0)
    Log of the probability mass function.
cdf(k, alpha, loc=0)
    Cumulative distribution function.
logcdf(k, alpha, loc=0)
    Log of the cumulative distribution function.
sf(k, alpha, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, alpha, loc=0)
    Log of the survival function.
ppf(q, alpha, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, alpha, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(alpha, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(alpha, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(alpha,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(alpha, loc=0)
    Median of the distribution.
mean(alpha, loc=0)
    Mean of the distribution.
var(alpha, loc=0)
    Variance of the distribution.
std(alpha, loc=0)
    Standard deviation of the distribution.
interval(alpha, alpha, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----

The probability mass function for the `yulesimon` is:

.. math::

    f(k) =  \alpha B(k, \alpha+1)

for :math:`k=1,2,3,...`, where :math:`\alpha>0`.
Here :math:`B` refers to the `scipy.special.beta` function.

The sampling of random variates is based on pg 553, Section 6.3 of [1]_.
Our notation maps to the referenced logic via :math:`\alpha=a-1`.

For details see the wikipedia entry [2]_.

References
----------
.. [1] Devroye, Luc. 'Non-uniform Random Variate Generation',
     (1986) Springer, New York.

.. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``yulesimon.pmf(k, alpha, loc)`` is identically
equivalent to ``yulesimon.pmf(k - loc, alpha)``.

Examples
--------
>>> from scipy.stats import yulesimon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> alpha = 11
>>> mean, var, skew, kurt = yulesimon.stats(alpha, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(yulesimon.ppf(0.01, alpha),
...               yulesimon.ppf(0.99, alpha))
>>> ax.plot(x, yulesimon.pmf(x, alpha), 'bo', ms=8, label='yulesimon pmf')
>>> ax.vlines(x, 0, yulesimon.pmf(x, alpha), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = yulesimon(alpha)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = yulesimon.cdf(x, alpha)
>>> np.allclose(x, yulesimon.ppf(prob, alpha))
True

Generate random numbers:

>>> r = yulesimon.rvs(alpha, size=1000)
*)

val zipf : ?loc:float -> a:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Zipf_gen] Np.Obj.t
(**
A Zipf discrete random variable.

As an instance of the `rv_discrete` class, `zipf` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, a, loc=0)
    Probability mass function.
logpmf(k, a, loc=0)
    Log of the probability mass function.
cdf(k, a, loc=0)
    Cumulative distribution function.
logcdf(k, a, loc=0)
    Log of the cumulative distribution function.
sf(k, a, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, a, loc=0)
    Log of the survival function.
ppf(q, a, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(a, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0)
    Median of the distribution.
mean(a, loc=0)
    Mean of the distribution.
var(a, loc=0)
    Variance of the distribution.
std(a, loc=0)
    Standard deviation of the distribution.
interval(alpha, a, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `zipf` is:

.. math::

    f(k, a) = \frac{1}{\zeta(a) k^a}

for :math:`k \ge 1`.

`zipf` takes :math:`a` as shape parameter. :math:`\zeta` is the
Riemann zeta function (`scipy.special.zeta`)

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``zipf.pmf(k, a, loc)`` is identically
equivalent to ``zipf.pmf(k - loc, a)``.

Examples
--------
>>> from scipy.stats import zipf
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 6.5
>>> mean, var, skew, kurt = zipf.stats(a, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(zipf.ppf(0.01, a),
...               zipf.ppf(0.99, a))
>>> ax.plot(x, zipf.pmf(x, a), 'bo', ms=8, label='zipf pmf')
>>> ax.vlines(x, 0, zipf.pmf(x, a), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = zipf(a)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = zipf.cdf(x, a)
>>> np.allclose(x, zipf.ppf(prob, a))
True

Generate random numbers:

>>> r = zipf.rvs(a, size=1000)
*)


end

module Kde : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

val asarray : ?dtype:Np.Dtype.t -> ?order:[`F | `C] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Convert the input to an array.

Parameters
----------
a : array_like
    Input data, in any form that can be converted to an array.  This
    includes lists, lists of tuples, tuples, tuples of tuples, tuples
    of lists and ndarrays.
dtype : data-type, optional
    By default, the data-type is inferred from the input data.
order : {'C', 'F'}, optional
    Whether to use row-major (C-style) or
    column-major (Fortran-style) memory representation.
    Defaults to 'C'.

Returns
-------
out : ndarray
    Array interpretation of `a`.  No copy is performed if the input
    is already an ndarray with matching dtype and order.  If `a` is a
    subclass of ndarray, a base class ndarray is returned.

See Also
--------
asanyarray : Similar function which passes through subclasses.
ascontiguousarray : Convert input to a contiguous array.
asfarray : Convert input to a floating point ndarray.
asfortranarray : Convert input to an ndarray with column-major
                 memory order.
asarray_chkfinite : Similar function which checks input for NaNs and Infs.
fromiter : Create an array from an iterator.
fromfunction : Construct an array by executing a function on grid
               positions.

Examples
--------
Convert a list into an array:

>>> a = [1, 2]
>>> np.asarray(a)
array([1, 2])

Existing arrays are not copied:

>>> a = np.array([1, 2])
>>> np.asarray(a) is a
True

If `dtype` is set, array is copied only if dtype does not match:

>>> a = np.array([1, 2], dtype=np.float32)
>>> np.asarray(a, dtype=np.float32) is a
True
>>> np.asarray(a, dtype=np.float64) is a
False

Contrary to `asanyarray`, ndarray subclasses are not passed through:

>>> issubclass(np.recarray, np.ndarray)
True
>>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)
>>> np.asarray(a) is a
False
>>> np.asanyarray(a) is a
True
*)

val atleast_1d : Py.Object.t list -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Convert inputs to arrays with at least one dimension.

Scalar inputs are converted to 1-dimensional arrays, whilst
higher-dimensional inputs are preserved.

Parameters
----------
arys1, arys2, ... : array_like
    One or more input arrays.

Returns
-------
ret : ndarray
    An array, or list of arrays, each with ``a.ndim >= 1``.
    Copies are made only if necessary.

See Also
--------
atleast_2d, atleast_3d

Examples
--------
>>> np.atleast_1d(1.0)
array([1.])

>>> x = np.arange(9.0).reshape(3,3)
>>> np.atleast_1d(x)
array([[0., 1., 2.],
       [3., 4., 5.],
       [6., 7., 8.]])
>>> np.atleast_1d(x) is x
True

>>> np.atleast_1d(1, [3, 4])
[array([1]), array([3, 4])]
*)

val atleast_2d : Py.Object.t list -> Py.Object.t
(**
View inputs as arrays with at least two dimensions.

Parameters
----------
arys1, arys2, ... : array_like
    One or more array-like sequences.  Non-array inputs are converted
    to arrays.  Arrays that already have two or more dimensions are
    preserved.

Returns
-------
res, res2, ... : ndarray
    An array, or list of arrays, each with ``a.ndim >= 2``.
    Copies are avoided where possible, and views with two or more
    dimensions are returned.

See Also
--------
atleast_1d, atleast_3d

Examples
--------
>>> np.atleast_2d(3.0)
array([[3.]])

>>> x = np.arange(3.0)
>>> np.atleast_2d(x)
array([[0., 1., 2.]])
>>> np.atleast_2d(x).base is x
True

>>> np.atleast_2d(1, [1, 2], [[1, 2]])
[array([[1]]), array([[1, 2]]), array([[1, 2]])]
*)

val callable : Py.Object.t -> Py.Object.t
(**
None
*)

val check_random_state : Py.Object.t -> Py.Object.t
(**
Turn seed into a np.random.RandomState instance

If seed is None (or np.random), return the RandomState singleton used
by np.random.
If seed is an int, return a new RandomState instance seeded with seed.
If seed is already a RandomState instance, return it.
If seed is a new-style np.random.Generator, return it.
Otherwise raise ValueError.
*)

val cov : ?y:[>`Ndarray] Np.Obj.t -> ?rowvar:bool -> ?bias:bool -> ?ddof:int -> ?fweights:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> ?aweights:[>`Ndarray] Np.Obj.t -> m:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Estimate a covariance matrix, given data and weights.

Covariance indicates the level to which two variables vary together.
If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
then the covariance matrix element :math:`C_{ij}` is the covariance of
:math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
of :math:`x_i`.

See the notes for an outline of the algorithm.

Parameters
----------
m : array_like
    A 1-D or 2-D array containing multiple variables and observations.
    Each row of `m` represents a variable, and each column a single
    observation of all those variables. Also see `rowvar` below.
y : array_like, optional
    An additional set of variables and observations. `y` has the same form
    as that of `m`.
rowvar : bool, optional
    If `rowvar` is True (default), then each row represents a
    variable, with observations in the columns. Otherwise, the relationship
    is transposed: each column represents a variable, while the rows
    contain observations.
bias : bool, optional
    Default normalization (False) is by ``(N - 1)``, where ``N`` is the
    number of observations given (unbiased estimate). If `bias` is True,
    then normalization is by ``N``. These values can be overridden by using
    the keyword ``ddof`` in numpy versions >= 1.5.
ddof : int, optional
    If not ``None`` the default value implied by `bias` is overridden.
    Note that ``ddof=1`` will return the unbiased estimate, even if both
    `fweights` and `aweights` are specified, and ``ddof=0`` will return
    the simple average. See the notes for the details. The default value
    is ``None``.

    .. versionadded:: 1.5
fweights : array_like, int, optional
    1-D array of integer frequency weights; the number of times each
    observation vector should be repeated.

    .. versionadded:: 1.10
aweights : array_like, optional
    1-D array of observation vector weights. These relative weights are
    typically large for observations considered 'important' and smaller for
    observations considered less 'important'. If ``ddof=0`` the array of
    weights can be used to assign probabilities to observation vectors.

    .. versionadded:: 1.10

Returns
-------
out : ndarray
    The covariance matrix of the variables.

See Also
--------
corrcoef : Normalized covariance matrix

Notes
-----
Assume that the observations are in the columns of the observation
array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
steps to compute the weighted covariance are as follows::

    >>> m = np.arange(10, dtype=np.float64)
    >>> f = np.arange(10) * 2
    >>> a = np.arange(10) ** 2.
    >>> ddof = 1
    >>> w = f * a
    >>> v1 = np.sum(w)
    >>> v2 = np.sum(w * a)
    >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1
    >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)

Note that when ``a == 1``, the normalization factor
``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
as it should.

Examples
--------
Consider two variables, :math:`x_0` and :math:`x_1`, which
correlate perfectly, but in opposite directions:

>>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
>>> x
array([[0, 1, 2],
       [2, 1, 0]])

Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
matrix shows this clearly:

>>> np.cov(x)
array([[ 1., -1.],
       [-1.,  1.]])

Note that element :math:`C_{0,1}`, which shows the correlation between
:math:`x_0` and :math:`x_1`, is negative.

Further, note how `x` and `y` are combined:

>>> x = [-2.1, -1,  4.3]
>>> y = [3,  1.1,  0.12]
>>> X = np.stack((x, y), axis=0)
>>> np.cov(X)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x, y)
array([[11.71      , -4.286     ], # may vary
       [-4.286     ,  2.144133]])
>>> np.cov(x)
array(11.71)
*)

val dot : ?out:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> b:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
dot(a, b, out=None)

Dot product of two arrays. Specifically,

- If both `a` and `b` are 1-D arrays, it is inner product of vectors
  (without complex conjugation).

- If both `a` and `b` are 2-D arrays, it is matrix multiplication,
  but using :func:`matmul` or ``a @ b`` is preferred.

- If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`
  and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.

- If `a` is an N-D array and `b` is a 1-D array, it is a sum product over
  the last axis of `a` and `b`.

- If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a
  sum product over the last axis of `a` and the second-to-last axis of `b`::

    dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])

Parameters
----------
a : array_like
    First argument.
b : array_like
    Second argument.
out : ndarray, optional
    Output argument. This must have the exact kind that would be returned
    if it was not used. In particular, it must have the right type, must be
    C-contiguous, and its dtype must be the dtype that would be returned
    for `dot(a,b)`. This is a performance feature. Therefore, if these
    conditions are not met, an exception is raised, instead of attempting
    to be flexible.

Returns
-------
output : ndarray
    Returns the dot product of `a` and `b`.  If `a` and `b` are both
    scalars or both 1-D arrays then a scalar is returned; otherwise
    an array is returned.
    If `out` is given, then it is returned.

Raises
------
ValueError
    If the last dimension of `a` is not the same size as
    the second-to-last dimension of `b`.

See Also
--------
vdot : Complex-conjugating dot product.
tensordot : Sum products over arbitrary axes.
einsum : Einstein summation convention.
matmul : '@' operator as method with out parameter.

Examples
--------
>>> np.dot(3, 4)
12

Neither argument is complex-conjugated:

>>> np.dot([2j, 3j], [2j, 3j])
(-13+0j)

For 2-D arrays it is the matrix product:

>>> a = [[1, 0], [0, 1]]
>>> b = [[4, 1], [2, 2]]
>>> np.dot(a, b)
array([[4, 1],
       [2, 2]])

>>> a = np.arange(3*4*5*6).reshape((3,4,5,6))
>>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))
>>> np.dot(a, b)[2,3,2,1,2,2]
499128
>>> sum(a[2,3,2,:] * b[1,2,:,2])
499128
*)

val exp : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Calculate the exponential of all elements in the input array.

Parameters
----------
x : array_like
    Input values.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
out : ndarray or scalar
    Output array, element-wise exponential of `x`.
    This is a scalar if `x` is a scalar.

See Also
--------
expm1 : Calculate ``exp(x) - 1`` for all elements in the array.
exp2  : Calculate ``2**x`` for all elements in the array.

Notes
-----
The irrational number ``e`` is also known as Euler's number.  It is
approximately 2.718281, and is the base of the natural logarithm,
``ln`` (this means that, if :math:`x = \ln y = \log_e y`,
then :math:`e^x = y`. For real input, ``exp(x)`` is always positive.

For complex arguments, ``x = a + ib``, we can write
:math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already
known (it is the real argument, described above).  The second term,
:math:`e^{ib}`, is :math:`\cos b + i \sin b`, a function with
magnitude 1 and a periodic phase.

References
----------
.. [1] Wikipedia, 'Exponential function',
       https://en.wikipedia.org/wiki/Exponential_function
.. [2] M. Abramovitz and I. A. Stegun, 'Handbook of Mathematical Functions
       with Formulas, Graphs, and Mathematical Tables,' Dover, 1964, p. 69,
       http://www.math.sfu.ca/~cbm/aands/page_69.htm

Examples
--------
Plot the magnitude and phase of ``exp(x)`` in the complex plane:

>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-2*np.pi, 2*np.pi, 100)
>>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane
>>> out = np.exp(xx)

>>> plt.subplot(121)
>>> plt.imshow(np.abs(out),
...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')
>>> plt.title('Magnitude of exp(x)')

>>> plt.subplot(122)
>>> plt.imshow(np.angle(out),
...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')
>>> plt.title('Phase (angle) of exp(x)')
>>> plt.show()
*)

val logsumexp : ?axis:int list -> ?b:[>`Ndarray] Np.Obj.t -> ?keepdims:bool -> ?return_sign:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Compute the log of the sum of exponentials of input elements.

Parameters
----------
a : array_like
    Input array.
axis : None or int or tuple of ints, optional
    Axis or axes over which the sum is taken. By default `axis` is None,
    and all elements are summed.

    .. versionadded:: 0.11.0
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left in the
    result as dimensions with size one. With this option, the result
    will broadcast correctly against the original array.

    .. versionadded:: 0.15.0
b : array-like, optional
    Scaling factor for exp(`a`) must be of the same shape as `a` or
    broadcastable to `a`. These values may be negative in order to
    implement subtraction.

    .. versionadded:: 0.12.0
return_sign : bool, optional
    If this is set to True, the result will be a pair containing sign
    information; if False, results that are negative will be returned
    as NaN. Default is False (no sign information).

    .. versionadded:: 0.16.0

Returns
-------
res : ndarray
    The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically
    more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``
    is returned.
sgn : ndarray
    If return_sign is True, this will be an array of floating-point
    numbers matching res and +1, 0, or -1 depending on the sign
    of the result. If False, only one result is returned.

See Also
--------
numpy.logaddexp, numpy.logaddexp2

Notes
-----
NumPy has a logaddexp function which is very similar to `logsumexp`, but
only handles two arguments. `logaddexp.reduce` is similar to this
function, but may be less stable.

Examples
--------
>>> from scipy.special import logsumexp
>>> a = np.arange(10)
>>> np.log(np.sum(np.exp(a)))
9.4586297444267107
>>> logsumexp(a)
9.4586297444267107

With weights

>>> a = np.arange(10)
>>> b = np.arange(10, 0, -1)
>>> logsumexp(a, b=b)
9.9170178533034665
>>> np.log(np.sum(b*np.exp(a)))
9.9170178533034647

Returning a sign flag

>>> logsumexp([1,2],b=[1,-1],return_sign=True)
(1.5413248546129181, -1.0)

Notice that `logsumexp` does not directly support masked arrays. To use it
on a masked array, convert the mask into zero weights:

>>> a = np.ma.array([np.log(2), 2, np.log(3)],
...                  mask=[False, True, False])
>>> b = (~a.mask).astype(int)
>>> logsumexp(a.data, b=b), np.log(5)
1.6094379124341005, 1.6094379124341005
*)

val ones : ?dtype:Np.Dtype.t -> ?order:[`C | `F] -> shape:[`I of int | `Is of int list] -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return a new array of given shape and type, filled with ones.

Parameters
----------
shape : int or sequence of ints
    Shape of the new array, e.g., ``(2, 3)`` or ``2``.
dtype : data-type, optional
    The desired data-type for the array, e.g., `numpy.int8`.  Default is
    `numpy.float64`.
order : {'C', 'F'}, optional, default: C
    Whether to store multi-dimensional data in row-major
    (C-style) or column-major (Fortran-style) order in
    memory.

Returns
-------
out : ndarray
    Array of ones with the given shape, dtype, and order.

See Also
--------
ones_like : Return an array of ones with shape and type of input.
empty : Return a new uninitialized array.
zeros : Return a new array setting values to zero.
full : Return a new array of given shape filled with value.


Examples
--------
>>> np.ones(5)
array([1., 1., 1., 1., 1.])

>>> np.ones((5,), dtype=int)
array([1, 1, 1, 1, 1])

>>> np.ones((2, 1))
array([[1.],
       [1.]])

>>> s = (2,2)
>>> np.ones(s)
array([[1.,  1.],
       [1.,  1.]])
*)

val power : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:Py.Object.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
power(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

First array elements raised to powers from second array, element-wise.

Raise each base in `x1` to the positionally-corresponding power in
`x2`.  `x1` and `x2` must be broadcastable to the same shape. Note that an
integer type raised to a negative integer power will raise a ValueError.

Parameters
----------
x1 : array_like
    The bases.
x2 : array_like
    The exponents. If ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes the shape of the output).
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray
    The bases in `x1` raised to the exponents in `x2`.
    This is a scalar if both `x1` and `x2` are scalars.

See Also
--------
float_power : power function that promotes integers to float

Examples
--------
Cube each element in a list.

>>> x1 = range(6)
>>> x1
[0, 1, 2, 3, 4, 5]
>>> np.power(x1, 3)
array([  0,   1,   8,  27,  64, 125])

Raise the bases to different exponents.

>>> x2 = [1.0, 2.0, 3.0, 3.0, 2.0, 1.0]
>>> np.power(x1, x2)
array([  0.,   1.,   8.,  27.,  16.,   5.])

The effect of broadcasting.

>>> x2 = np.array([[1, 2, 3, 3, 2, 1], [1, 2, 3, 3, 2, 1]])
>>> x2
array([[1, 2, 3, 3, 2, 1],
       [1, 2, 3, 3, 2, 1]])
>>> np.power(x1, x2)
array([[ 0,  1,  8, 27, 16,  5],
       [ 0,  1,  8, 27, 16,  5]])
*)

val ravel : ?order:[`C | `F | `A | `K] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return a contiguous flattened array.

A 1-D array, containing the elements of the input, is returned.  A copy is
made only if needed.

As of NumPy 1.10, the returned array will have the same type as the input
array. (for example, a masked array will be returned for a masked array
input)

Parameters
----------
a : array_like
    Input array.  The elements in `a` are read in the order specified by
    `order`, and packed as a 1-D array.
order : {'C','F', 'A', 'K'}, optional

    The elements of `a` are read using this index order. 'C' means
    to index the elements in row-major, C-style order,
    with the last axis index changing fastest, back to the first
    axis index changing slowest.  'F' means to index the elements
    in column-major, Fortran-style order, with the
    first index changing fastest, and the last index changing
    slowest. Note that the 'C' and 'F' options take no account of
    the memory layout of the underlying array, and only refer to
    the order of axis indexing.  'A' means to read the elements in
    Fortran-like index order if `a` is Fortran *contiguous* in
    memory, C-like order otherwise.  'K' means to read the
    elements in the order they occur in memory, except for
    reversing the data when strides are negative.  By default, 'C'
    index order is used.

Returns
-------
y : array_like
    y is an array of the same subtype as `a`, with shape ``(a.size,)``.
    Note that matrices are special cased for backward compatibility, if `a`
    is a matrix, then y is a 1-D ndarray.

See Also
--------
ndarray.flat : 1-D iterator over an array.
ndarray.flatten : 1-D array copy of the elements of an array
                  in row-major order.
ndarray.reshape : Change the shape of an array without changing its data.

Notes
-----
In row-major, C-style order, in two dimensions, the row index
varies the slowest, and the column index the quickest.  This can
be generalized to multiple dimensions, where row-major order
implies that the index along the first axis varies slowest, and
the index along the last quickest.  The opposite holds for
column-major, Fortran-style index ordering.

When a view is desired in as many cases as possible, ``arr.reshape(-1)``
may be preferable.

Examples
--------
It is equivalent to ``reshape(-1, order=order)``.

>>> x = np.array([[1, 2, 3], [4, 5, 6]])
>>> np.ravel(x)
array([1, 2, 3, 4, 5, 6])

>>> x.reshape(-1)
array([1, 2, 3, 4, 5, 6])

>>> np.ravel(x, order='F')
array([1, 4, 2, 5, 3, 6])

When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:

>>> np.ravel(x.T)
array([1, 4, 2, 5, 3, 6])
>>> np.ravel(x.T, order='A')
array([1, 2, 3, 4, 5, 6])

When ``order`` is 'K', it will preserve orderings that are neither 'C'
nor 'F', but won't reverse axes:

>>> a = np.arange(3)[::-1]; a
array([2, 1, 0])
>>> a.ravel(order='C')
array([2, 1, 0])
>>> a.ravel(order='K')
array([2, 1, 0])

>>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a
array([[[ 0,  2,  4],
        [ 1,  3,  5]],
       [[ 6,  8, 10],
        [ 7,  9, 11]]])
>>> a.ravel(order='C')
array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])
>>> a.ravel(order='K')
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
*)

val reshape : ?order:[`C | `F | `A] -> a:[>`Ndarray] Np.Obj.t -> newshape:int list -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Gives a new shape to an array without changing its data.

Parameters
----------
a : array_like
    Array to be reshaped.
newshape : int or tuple of ints
    The new shape should be compatible with the original shape. If
    an integer, then the result will be a 1-D array of that length.
    One shape dimension can be -1. In this case, the value is
    inferred from the length of the array and remaining dimensions.
order : {'C', 'F', 'A'}, optional
    Read the elements of `a` using this index order, and place the
    elements into the reshaped array using this index order.  'C'
    means to read / write the elements using C-like index order,
    with the last axis index changing fastest, back to the first
    axis index changing slowest. 'F' means to read / write the
    elements using Fortran-like index order, with the first index
    changing fastest, and the last index changing slowest. Note that
    the 'C' and 'F' options take no account of the memory layout of
    the underlying array, and only refer to the order of indexing.
    'A' means to read / write the elements in Fortran-like index
    order if `a` is Fortran *contiguous* in memory, C-like order
    otherwise.

Returns
-------
reshaped_array : ndarray
    This will be a new view object if possible; otherwise, it will
    be a copy.  Note there is no guarantee of the *memory layout* (C- or
    Fortran- contiguous) of the returned array.

See Also
--------
ndarray.reshape : Equivalent method.

Notes
-----
It is not always possible to change the shape of an array without
copying the data. If you want an error to be raised when the data is copied,
you should assign the new shape to the shape attribute of the array::

 >>> a = np.zeros((10, 2))

 # A transpose makes the array non-contiguous
 >>> b = a.T

 # Taking a view makes it possible to modify the shape without modifying
 # the initial object.
 >>> c = b.view()
 >>> c.shape = (20)
 Traceback (most recent call last):
    ...
 AttributeError: incompatible shape for a non-contiguous array

The `order` keyword gives the index ordering both for *fetching* the values
from `a`, and then *placing* the values into the output array.
For example, let's say you have an array:

>>> a = np.arange(6).reshape((3, 2))
>>> a
array([[0, 1],
       [2, 3],
       [4, 5]])

You can think of reshaping as first raveling the array (using the given
index order), then inserting the elements from the raveled array into the
new array using the same kind of index ordering as was used for the
raveling.

>>> np.reshape(a, (2, 3)) # C-like index ordering
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering
array([[0, 4, 3],
       [2, 1, 5]])
>>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')
array([[0, 4, 3],
       [2, 1, 5]])

Examples
--------
>>> a = np.array([[1,2,3], [4,5,6]])
>>> np.reshape(a, 6)
array([1, 2, 3, 4, 5, 6])
>>> np.reshape(a, 6, order='F')
array([1, 4, 2, 5, 3, 6])

>>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2
array([[1, 2],
       [3, 4],
       [5, 6]])
*)

val sqrt : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Return the non-negative square-root of an array, element-wise.

Parameters
----------
x : array_like
    The values whose square-roots are required.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray
    An array of the same shape as `x`, containing the positive
    square-root of each element in `x`.  If any element in `x` is
    complex, a complex array is returned (and the square-roots of
    negative reals are calculated).  If all of the elements in `x`
    are real, so is `y`, with negative elements returning ``nan``.
    If `out` was provided, `y` is a reference to it.
    This is a scalar if `x` is a scalar.

See Also
--------
lib.scimath.sqrt
    A version which returns complex numbers when given negative reals.

Notes
-----
*sqrt* has--consistent with common convention--as its branch cut the
real 'interval' [`-inf`, 0), and is continuous from above on it.
A branch cut is a curve in the complex plane across which a given
complex function fails to be continuous.

Examples
--------
>>> np.sqrt([1,4,9])
array([ 1.,  2.,  3.])

>>> np.sqrt([4, -1, -3+4J])
array([ 2.+0.j,  0.+1.j,  1.+2.j])

>>> np.sqrt([4, -1, np.inf])
array([ 2., nan, inf])
*)

val squeeze : ?axis:int list -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Remove single-dimensional entries from the shape of an array.

Parameters
----------
a : array_like
    Input data.
axis : None or int or tuple of ints, optional
    .. versionadded:: 1.7.0

    Selects a subset of the single-dimensional entries in the
    shape. If an axis is selected with shape entry greater than
    one, an error is raised.

Returns
-------
squeezed : ndarray
    The input array, but with all or a subset of the
    dimensions of length 1 removed. This is always `a` itself
    or a view into `a`.

Raises
------
ValueError
    If `axis` is not None, and an axis being squeezed is not of length 1

See Also
--------
expand_dims : The inverse operation, adding singleton dimensions
reshape : Insert, remove, and combine dimensions, and resize existing ones

Examples
--------
>>> x = np.array([[[0], [1], [2]]])
>>> x.shape
(1, 3, 1)
>>> np.squeeze(x).shape
(3,)
>>> np.squeeze(x, axis=0).shape
(3, 1)
>>> np.squeeze(x, axis=1).shape
Traceback (most recent call last):
...
ValueError: cannot select an axis to squeeze out which has size not equal to one
>>> np.squeeze(x, axis=2).shape
(1, 3)
*)

val sum : ?axis:int list -> ?dtype:Np.Dtype.t -> ?out:[>`Ndarray] Np.Obj.t -> ?keepdims:bool -> ?initial:[`F of float | `I of int | `Bool of bool | `S of string] -> ?where:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Sum of array elements over a given axis.

Parameters
----------
a : array_like
    Elements to sum.
axis : None or int or tuple of ints, optional
    Axis or axes along which a sum is performed.  The default,
    axis=None, will sum all of the elements of the input array.  If
    axis is negative it counts from the last to the first axis.

    .. versionadded:: 1.7.0

    If axis is a tuple of ints, a sum is performed on all of the axes
    specified in the tuple instead of a single axis or all the axes as
    before.
dtype : dtype, optional
    The type of the returned array and of the accumulator in which the
    elements are summed.  The dtype of `a` is used by default unless `a`
    has an integer dtype of less precision than the default platform
    integer.  In that case, if `a` is signed then the platform integer
    is used while if `a` is unsigned then an unsigned integer of the
    same precision as the platform integer is used.
out : ndarray, optional
    Alternative output array in which to place the result. It must have
    the same shape as the expected output, but the type of the output
    values will be cast if necessary.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `sum` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.
initial : scalar, optional
    Starting value for the sum. See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.15.0

where : array_like of bool, optional
    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.17.0

Returns
-------
sum_along_axis : ndarray
    An array with the same shape as `a`, with the specified
    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar
    is returned.  If an output array is specified, a reference to
    `out` is returned.

See Also
--------
ndarray.sum : Equivalent method.

add.reduce : Equivalent functionality of `add`.

cumsum : Cumulative sum of array elements.

trapz : Integration of array values using the composite trapezoidal rule.

mean, average

Notes
-----
Arithmetic is modular when using integer types, and no error is
raised on overflow.

The sum of an empty array is the neutral element 0:

>>> np.sum([])
0.0

For floating point numbers the numerical precision of sum (and
``np.add.reduce``) is in general limited by directly adding each number
individually to the result causing rounding errors in every step.
However, often numpy will use a  numerically better approach (partial
pairwise summation) leading to improved precision in many use-cases.
This improved precision is always provided when no ``axis`` is given.
When ``axis`` is given, it will depend on which axis is summed.
Technically, to provide the best speed possible, the improved precision
is only used when the summation is along the fast axis in memory.
Note that the exact precision may vary depending on other parameters.
In contrast to NumPy, Python's ``math.fsum`` function uses a slower but
more precise approach to summation.
Especially when summing a large number of lower precision floating point
numbers, such as ``float32``, numerical errors can become significant.
In such cases it can be advisable to use `dtype='float64'` to use a higher
precision for the output.

Examples
--------
>>> np.sum([0.5, 1.5])
2.0
>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)
1
>>> np.sum([[0, 1], [0, 5]])
6
>>> np.sum([[0, 1], [0, 5]], axis=0)
array([0, 6])
>>> np.sum([[0, 1], [0, 5]], axis=1)
array([1, 5])
>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)
array([1., 5.])

If the accumulator is too small, overflow occurs:

>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)
-128

You can also start the sum with a value other than zero:

>>> np.sum([10], initial=5)
15
*)

val transpose : ?axes:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Permute the dimensions of an array.

Parameters
----------
a : array_like
    Input array.
axes : list of ints, optional
    By default, reverse the dimensions, otherwise permute the axes
    according to the values given.

Returns
-------
p : ndarray
    `a` with its axes permuted.  A view is returned whenever
    possible.

See Also
--------
moveaxis
argsort

Notes
-----
Use `transpose(a, argsort(axes))` to invert the transposition of tensors
when using the `axes` keyword argument.

Transposing a 1-D array returns an unchanged view of the original array.

Examples
--------
>>> x = np.arange(4).reshape((2,2))
>>> x
array([[0, 1],
       [2, 3]])

>>> np.transpose(x)
array([[0, 2],
       [1, 3]])

>>> x = np.ones((1, 2, 3))
>>> np.transpose(x, (1, 0, 2)).shape
(2, 1, 3)
*)

val zeros : ?dtype:Np.Dtype.t -> ?order:[`C | `F] -> shape:int list -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
zeros(shape, dtype=float, order='C')

Return a new array of given shape and type, filled with zeros.

Parameters
----------
shape : int or tuple of ints
    Shape of the new array, e.g., ``(2, 3)`` or ``2``.
dtype : data-type, optional
    The desired data-type for the array, e.g., `numpy.int8`.  Default is
    `numpy.float64`.
order : {'C', 'F'}, optional, default: 'C'
    Whether to store multi-dimensional data in row-major
    (C-style) or column-major (Fortran-style) order in
    memory.

Returns
-------
out : ndarray
    Array of zeros with the given shape, dtype, and order.

See Also
--------
zeros_like : Return an array of zeros with shape and type of input.
empty : Return a new uninitialized array.
ones : Return a new array setting values to one.
full : Return a new array of given shape filled with value.

Examples
--------
>>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])

>>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])

>>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])

>>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])

>>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
*)


end

module Morestats : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module AndersonResult : sig
type tag = [`AndersonResult]
type t = [`AndersonResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> critical_values:Py.Object.t -> significance_level:Py.Object.t -> unit -> t
(**
AndersonResult(statistic, critical_values, significance_level)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Anderson_ksampResult : sig
type tag = [`Anderson_ksampResult]
type t = [`Anderson_ksampResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> critical_values:Py.Object.t -> significance_level:Py.Object.t -> unit -> t
(**
Anderson_ksampResult(statistic, critical_values, significance_level)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module AnsariResult : sig
type tag = [`AnsariResult]
type t = [`AnsariResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
AnsariResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module BartlettResult : sig
type tag = [`BartlettResult]
type t = [`BartlettResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
BartlettResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module FlignerResult : sig
type tag = [`FlignerResult]
type t = [`FlignerResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
FlignerResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module LeveneResult : sig
type tag = [`LeveneResult]
type t = [`LeveneResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
LeveneResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Mean : sig
type tag = [`Mean]
type t = [`Mean | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> minmax:Py.Object.t -> unit -> t
(**
Mean(statistic, minmax)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Std_dev : sig
type tag = [`Std_dev]
type t = [`Object | `Std_dev] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> minmax:Py.Object.t -> unit -> t
(**
Std_dev(statistic, minmax)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Variance : sig
type tag = [`Variance]
type t = [`Object | `Variance] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> minmax:Py.Object.t -> unit -> t
(**
Variance(statistic, minmax)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module WilcoxonResult : sig
type tag = [`WilcoxonResult]
type t = [`Object | `WilcoxonResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
WilcoxonResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Rv_generic : sig
type tag = [`Rv_generic]
type t = [`Object | `Rv_generic] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : ?seed:Py.Object.t -> unit -> t
(**
Class which encapsulates common functionality between rv_discrete
and rv_continuous.
*)

val entropy : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Differential entropy of the RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional  (continuous distributions only).
    Scale parameter (default=1).

Notes
-----
Entropy is defined base `e`:

>>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))
>>> np.allclose(drv.entropy(), np.log(2.0))
True
*)

val freeze : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Freeze the distribution for the given arguments.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution.  Should include all
    the non-optional arguments, may include ``loc`` and ``scale``.

Returns
-------
rv_frozen : rv_frozen instance
    The frozen distribution.
*)

val interval : ?kwds:(string * Py.Object.t) list -> alpha:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Confidence interval with equal areas around the median.

Parameters
----------
alpha : array_like of float
    Probability that an rv will be drawn from the returned range.
    Each value should be in the range [0, 1].
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.

Returns
-------
a, b : ndarray of float
    end-points of range that contain ``100 * alpha %`` of the rv's
    possible values.
*)

val mean : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Mean of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
mean : float
    the mean of the distribution
*)

val median : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Median of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    Location parameter, Default is 0.
scale : array_like, optional
    Scale parameter, Default is 1.

Returns
-------
median : float
    The median of the distribution.

See Also
--------
rv_discrete.ppf
    Inverse of the CDF
*)

val moment : ?kwds:(string * Py.Object.t) list -> n:[`N_1 of Py.Object.t | `I of int] -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
n-th order non-central moment of distribution.

Parameters
----------
n : int, n >= 1
    Order of moment.
arg1, arg2, arg3,... : float
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)
*)

val rvs : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Random variates of given type.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    Location parameter (default=0).
scale : array_like, optional
    Scale parameter (default=1).
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
random_state : None or int or ``np.random.RandomState`` instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None, rely on ``self.random_state``.
    Default is None.

Returns
-------
rvs : ndarray or scalar
    Random variates of given `size`.
*)

val stats : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Some statistics of the given RV.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional (continuous RVs only)
    scale parameter (default=1)
moments : str, optional
    composed of letters ['mvsk'] defining which moments to compute:
    'm' = mean,
    'v' = variance,
    's' = (Fisher's) skew,
    'k' = (Fisher's) kurtosis.
    (default is 'mv')

Returns
-------
stats : sequence
    of requested moments.
*)

val std : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Standard deviation of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
std : float
    standard deviation of the distribution
*)

val support : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
Return the support of the distribution.

Parameters
----------
arg1, arg2, ... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information).
loc : array_like, optional
    location parameter, Default is 0.
scale : array_like, optional
    scale parameter, Default is 1.
Returns
-------
a, b : float
    end-points of the distribution's support.
*)

val var : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> float
(**
Variance of the distribution.

Parameters
----------
arg1, arg2, arg3,... : array_like
    The shape parameter(s) for the distribution (see docstring of the
    instance object for more information)
loc : array_like, optional
    location parameter (default=0)
scale : array_like, optional
    scale parameter (default=1)

Returns
-------
var : float
    the variance of the distribution
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

val amax : ?axis:int list -> ?out:[>`Ndarray] Np.Obj.t -> ?keepdims:bool -> ?initial:[`F of float | `I of int | `Bool of bool | `S of string] -> ?where:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return the maximum of an array or maximum along an axis.

Parameters
----------
a : array_like
    Input data.
axis : None or int or tuple of ints, optional
    Axis or axes along which to operate.  By default, flattened input is
    used.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, the maximum is selected over multiple axes,
    instead of a single axis or all the axes as before.
out : ndarray, optional
    Alternative output array in which to place the result.  Must
    be of the same shape and buffer length as the expected output.
    See `ufuncs-output-type` for more details.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `amax` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

initial : scalar, optional
    The minimum value of an output element. Must be present to allow
    computation on empty slice. See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.15.0

where : array_like of bool, optional
    Elements to compare for the maximum. See `~numpy.ufunc.reduce`
    for details.

    .. versionadded:: 1.17.0

Returns
-------
amax : ndarray or scalar
    Maximum of `a`. If `axis` is None, the result is a scalar value.
    If `axis` is given, the result is an array of dimension
    ``a.ndim - 1``.

See Also
--------
amin :
    The minimum value of an array along a given axis, propagating any NaNs.
nanmax :
    The maximum value of an array along a given axis, ignoring any NaNs.
maximum :
    Element-wise maximum of two arrays, propagating any NaNs.
fmax :
    Element-wise maximum of two arrays, ignoring any NaNs.
argmax :
    Return the indices of the maximum values.

nanmin, minimum, fmin

Notes
-----
NaN values are propagated, that is if at least one item is NaN, the
corresponding max value will be NaN as well. To ignore NaN values
(MATLAB behavior), please use nanmax.

Don't use `amax` for element-wise comparison of 2 arrays; when
``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than
``amax(a, axis=0)``.

Examples
--------
>>> a = np.arange(4).reshape((2,2))
>>> a
array([[0, 1],
       [2, 3]])
>>> np.amax(a)           # Maximum of the flattened array
3
>>> np.amax(a, axis=0)   # Maxima along the first axis
array([2, 3])
>>> np.amax(a, axis=1)   # Maxima along the second axis
array([1, 3])
>>> np.amax(a, where=[False, True], initial=-1, axis=0)
array([-1,  3])
>>> b = np.arange(5, dtype=float)
>>> b[2] = np.NaN
>>> np.amax(b)
nan
>>> np.amax(b, where=~np.isnan(b), initial=-1)
4.0
>>> np.nanmax(b)
4.0

You can use an initial value to compute the maximum of an empty slice, or
to initialize it to a different value:

>>> np.max([[-50], [10]], axis=-1, initial=0)
array([ 0, 10])

Notice that the initial value is used as one of the elements for which the
maximum is determined, unlike for the default argument Python's max
function, which is only used for empty iterables.

>>> np.max([5], initial=6)
6
>>> max([5], default=6)
5
*)

val amin : ?axis:int list -> ?out:[>`Ndarray] Np.Obj.t -> ?keepdims:bool -> ?initial:[`F of float | `I of int | `Bool of bool | `S of string] -> ?where:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return the minimum of an array or minimum along an axis.

Parameters
----------
a : array_like
    Input data.
axis : None or int or tuple of ints, optional
    Axis or axes along which to operate.  By default, flattened input is
    used.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, the minimum is selected over multiple axes,
    instead of a single axis or all the axes as before.
out : ndarray, optional
    Alternative output array in which to place the result.  Must
    be of the same shape and buffer length as the expected output.
    See `ufuncs-output-type` for more details.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `amin` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

initial : scalar, optional
    The maximum value of an output element. Must be present to allow
    computation on empty slice. See `~numpy.ufunc.reduce` for details.

    .. versionadded:: 1.15.0

where : array_like of bool, optional
    Elements to compare for the minimum. See `~numpy.ufunc.reduce`
    for details.

    .. versionadded:: 1.17.0

Returns
-------
amin : ndarray or scalar
    Minimum of `a`. If `axis` is None, the result is a scalar value.
    If `axis` is given, the result is an array of dimension
    ``a.ndim - 1``.

See Also
--------
amax :
    The maximum value of an array along a given axis, propagating any NaNs.
nanmin :
    The minimum value of an array along a given axis, ignoring any NaNs.
minimum :
    Element-wise minimum of two arrays, propagating any NaNs.
fmin :
    Element-wise minimum of two arrays, ignoring any NaNs.
argmin :
    Return the indices of the minimum values.

nanmax, maximum, fmax

Notes
-----
NaN values are propagated, that is if at least one item is NaN, the
corresponding min value will be NaN as well. To ignore NaN values
(MATLAB behavior), please use nanmin.

Don't use `amin` for element-wise comparison of 2 arrays; when
``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than
``amin(a, axis=0)``.

Examples
--------
>>> a = np.arange(4).reshape((2,2))
>>> a
array([[0, 1],
       [2, 3]])
>>> np.amin(a)           # Minimum of the flattened array
0
>>> np.amin(a, axis=0)   # Minima along the first axis
array([0, 1])
>>> np.amin(a, axis=1)   # Minima along the second axis
array([0, 2])
>>> np.amin(a, where=[False, True], initial=10, axis=0)
array([10,  1])

>>> b = np.arange(5, dtype=float)
>>> b[2] = np.NaN
>>> np.amin(b)
nan
>>> np.amin(b, where=~np.isnan(b), initial=10)
0.0
>>> np.nanmin(b)
0.0

>>> np.min([[-50], [10]], axis=-1, initial=0)
array([-50,   0])

Notice that the initial value is used as one of the elements for which the
minimum is determined, unlike for the default argument Python's max
function, which is only used for empty iterables.

Notice that this isn't the same as Python's ``default`` argument.

>>> np.min([6], initial=5)
5
>>> min([6], default=5)
6
*)

val anderson : ?dist:[`Expon | `Logistic | `Gumbel | `Gumbel_l | `PyObject of Py.Object.t] -> x:[>`Ndarray] Np.Obj.t -> unit -> (float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Anderson-Darling test for data coming from a particular distribution.

The Anderson-Darling tests the null hypothesis that a sample is
drawn from a population that follows a particular distribution.
For the Anderson-Darling test, the critical values depend on
which distribution is being tested against.  This function works
for normal, exponential, logistic, or Gumbel (Extreme Value
Type I) distributions.

Parameters
----------
x : array_like
    Array of sample data.
dist : {'norm','expon','logistic','gumbel','gumbel_l', gumbel_r',
    'extreme1'}, optional
    the type of distribution to test against.  The default is 'norm'
    and 'extreme1', 'gumbel_l' and 'gumbel' are synonyms.

Returns
-------
statistic : float
    The Anderson-Darling test statistic.
critical_values : list
    The critical values for this distribution.
significance_level : list
    The significance levels for the corresponding critical values
    in percents.  The function returns critical values for a
    differing set of significance levels depending on the
    distribution that is being tested against.

See Also
--------
kstest : The Kolmogorov-Smirnov test for goodness-of-fit.

Notes
-----
Critical values provided are for the following significance levels:

normal/exponenential
    15%, 10%, 5%, 2.5%, 1%
logistic
    25%, 10%, 5%, 2.5%, 1%, 0.5%
Gumbel
    25%, 10%, 5%, 2.5%, 1%

If the returned statistic is larger than these critical values then
for the corresponding significance level, the null hypothesis that
the data come from the chosen distribution can be rejected.
The returned statistic is referred to as 'A2' in the references.

References
----------
.. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm
.. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and
       Some Comparisons, Journal of the American Statistical Association,
       Vol. 69, pp. 730-737.
.. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit
       Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,
       pp. 357-369.
.. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value
       Distribution, Biometrika, Vol. 64, pp. 583-588.
.. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference
       to Tests for Exponentiality , Technical Report No. 262,
       Department of Statistics, Stanford University, Stanford, CA.
.. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution
       Based on the Empirical Distribution Function, Biometrika, Vol. 66,
       pp. 591-595.
*)

val anderson_ksamp : ?midrank:bool -> samples:Py.Object.t -> unit -> (float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
The Anderson-Darling test for k-samples.

The k-sample Anderson-Darling test is a modification of the
one-sample Anderson-Darling test. It tests the null hypothesis
that k-samples are drawn from the same population without having
to specify the distribution function of that population. The
critical values depend on the number of samples.

Parameters
----------
samples : sequence of 1-D array_like
    Array of sample data in arrays.
midrank : bool, optional
    Type of Anderson-Darling test which is computed. Default
    (True) is the midrank test applicable to continuous and
    discrete populations. If False, the right side empirical
    distribution is used.

Returns
-------
statistic : float
    Normalized k-sample Anderson-Darling test statistic.
critical_values : array
    The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,
    0.5%, 0.1%.
significance_level : float
    An approximate significance level at which the null hypothesis for the
    provided samples can be rejected. The value is floored / capped at
    0.1% / 25%.

Raises
------
ValueError
    If less than 2 samples are provided, a sample is empty, or no
    distinct observations are in the samples.

See Also
--------
ks_2samp : 2 sample Kolmogorov-Smirnov test
anderson : 1 sample Anderson-Darling test

Notes
-----
[1]_ defines three versions of the k-sample Anderson-Darling test:
one for continuous distributions and two for discrete
distributions, in which ties between samples may occur. The
default of this routine is to compute the version based on the
midrank empirical distribution function. This test is applicable
to continuous and discrete data. If midrank is set to False, the
right side empirical distribution is used for a test for discrete
data. According to [1]_, the two discrete test statistics differ
only slightly if a few collisions due to round-off errors occur in
the test not adjusted for ties between samples.

The critical values corresponding to the significance levels from 0.01
to 0.25 are taken from [1]_. p-values are floored / capped
at 0.1% / 25%. Since the range of critical values might be extended in
future releases, it is recommended not to test ``p == 0.25``, but rather
``p >= 0.25`` (analogously for the lower bound).

.. versionadded:: 0.14.0

References
----------
.. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample
       Anderson-Darling Tests, Journal of the American Statistical
       Association, Vol. 82, pp. 918-924.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(314159)

The null hypothesis that the two random samples come from the same
distribution can be rejected at the 5% level because the returned
test value is greater than the critical value for 5% (1.961) but
not at the 2.5% level. The interpolation gives an approximate
significance level of 3.2%:

>>> stats.anderson_ksamp([np.random.normal(size=50),
... np.random.normal(loc=0.5, size=30)])
(2.4615796189876105,
  array([ 0.325,  1.226,  1.961,  2.718,  3.752, 4.592, 6.546]),
  0.03176687568842282)


The null hypothesis cannot be rejected for three samples from an
identical distribution. The reported p-value (25%) has been capped and
may not be very accurate (since it corresponds to the value 0.449
whereas the statistic is -0.731):

>>> stats.anderson_ksamp([np.random.normal(size=50),
... np.random.normal(size=30), np.random.normal(size=20)])
(-0.73091722665244196,
  array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,
  4.07210043, 5.56419101]),
  0.25)
*)

val ansari : x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Perform the Ansari-Bradley test for equal scale parameters.

The Ansari-Bradley test is a non-parametric test for the equality
of the scale parameter of the distributions from which two
samples were drawn.

Parameters
----------
x, y : array_like
    Arrays of sample data.

Returns
-------
statistic : float
    The Ansari-Bradley test statistic.
pvalue : float
    The p-value of the hypothesis test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
mood : A non-parametric test for the equality of two scale parameters

Notes
-----
The p-value given is exact when the sample sizes are both less than
55 and there are no ties, otherwise a normal approximation for the
p-value is used.

References
----------
.. [1] Sprent, Peter and N.C. Smeeton.  Applied nonparametric statistical
       methods.  3rd ed. Chapman and Hall/CRC. 2001.  Section 5.8.2.
*)

val any : ?axis:int list -> ?out:[>`Ndarray] Np.Obj.t -> ?keepdims:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Test whether any array element along a given axis evaluates to True.

Returns single boolean unless `axis` is not ``None``

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : None or int or tuple of ints, optional
    Axis or axes along which a logical OR reduction is performed.
    The default (``axis=None``) is to perform a logical OR over all
    the dimensions of the input array. `axis` may be negative, in
    which case it counts from the last to the first axis.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, a reduction is performed on multiple
    axes, instead of a single axis or all the axes as before.
out : ndarray, optional
    Alternate output array in which to place the result.  It must have
    the same shape as the expected output and its type is preserved
    (e.g., if it is of type float, then it will remain so, returning
    1.0 for True and 0.0 for False, regardless of the type of `a`).
    See `ufuncs-output-type` for more details.

keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `any` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

Returns
-------
any : bool or ndarray
    A new boolean or `ndarray` is returned unless `out` is specified,
    in which case a reference to `out` is returned.

See Also
--------
ndarray.any : equivalent method

all : Test whether all elements along a given axis evaluate to True.

Notes
-----
Not a Number (NaN), positive infinity and negative infinity evaluate
to `True` because these are not equal to zero.

Examples
--------
>>> np.any([[True, False], [True, True]])
True

>>> np.any([[True, False], [False, False]], axis=0)
array([ True, False])

>>> np.any([-1, 0, 5])
True

>>> np.any(np.nan)
True

>>> o=np.array(False)
>>> z=np.any([-1, 4, 5], out=o)
>>> z, o
(array(True), array(True))
>>> # Check now that z is a reference to o
>>> z is o
True
>>> id(z), id(o) # identity of z and o              # doctest: +SKIP
(191614240, 191614240)
*)

val arange : ?start:[`I of int | `F of float] -> ?step:[`I of int | `F of float] -> ?dtype:Np.Dtype.t -> stop:[`F of float | `I of int] -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
arange([start,] stop[, step,], dtype=None)

Return evenly spaced values within a given interval.

Values are generated within the half-open interval ``[start, stop)``
(in other words, the interval including `start` but excluding `stop`).
For integer arguments the function is equivalent to the Python built-in
`range` function, but returns an ndarray rather than a list.

When using a non-integer step, such as 0.1, the results will often not
be consistent.  It is better to use `numpy.linspace` for these cases.

Parameters
----------
start : number, optional
    Start of interval.  The interval includes this value.  The default
    start value is 0.
stop : number
    End of interval.  The interval does not include this value, except
    in some cases where `step` is not an integer and floating point
    round-off affects the length of `out`.
step : number, optional
    Spacing between values.  For any output `out`, this is the distance
    between two adjacent values, ``out[i+1] - out[i]``.  The default
    step size is 1.  If `step` is specified as a position argument,
    `start` must also be given.
dtype : dtype
    The type of the output array.  If `dtype` is not given, infer the data
    type from the other input arguments.

Returns
-------
arange : ndarray
    Array of evenly spaced values.

    For floating point arguments, the length of the result is
    ``ceil((stop - start)/step)``.  Because of floating point overflow,
    this rule may result in the last element of `out` being greater
    than `stop`.

See Also
--------
numpy.linspace : Evenly spaced numbers with careful handling of endpoints.
numpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.
numpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.

Examples
--------
>>> np.arange(3)
array([0, 1, 2])
>>> np.arange(3.0)
array([ 0.,  1.,  2.])
>>> np.arange(3,7)
array([3, 4, 5, 6])
>>> np.arange(3,7,2)
array([3, 5])
*)

val arctan2 : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:Py.Object.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
arctan2(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Element-wise arc tangent of ``x1/x2`` choosing the quadrant correctly.

The quadrant (i.e., branch) is chosen so that ``arctan2(x1, x2)`` is
the signed angle in radians between the ray ending at the origin and
passing through the point (1,0), and the ray ending at the origin and
passing through the point (`x2`, `x1`).  (Note the role reversal: the
'`y`-coordinate' is the first function parameter, the '`x`-coordinate'
is the second.)  By IEEE convention, this function is defined for
`x2` = +/-0 and for either or both of `x1` and `x2` = +/-inf (see
Notes for specific values).

This function is not defined for complex-valued arguments; for the
so-called argument of complex values, use `angle`.

Parameters
----------
x1 : array_like, real-valued
    `y`-coordinates.
x2 : array_like, real-valued
    `x`-coordinates. If ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes the shape of the output).
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
angle : ndarray
    Array of angles in radians, in the range ``[-pi, pi]``.
    This is a scalar if both `x1` and `x2` are scalars.

See Also
--------
arctan, tan, angle

Notes
-----
*arctan2* is identical to the `atan2` function of the underlying
C library.  The following special values are defined in the C
standard: [1]_

====== ====== ================
`x1`   `x2`   `arctan2(x1,x2)`
====== ====== ================
+/- 0  +0     +/- 0
+/- 0  -0     +/- pi
 > 0   +/-inf +0 / +pi
 < 0   +/-inf -0 / -pi
+/-inf +inf   +/- (pi/4)
+/-inf -inf   +/- (3*pi/4)
====== ====== ================

Note that +0 and -0 are distinct floating point numbers, as are +inf
and -inf.

References
----------
.. [1] ISO/IEC standard 9899:1999, 'Programming language C.'

Examples
--------
Consider four points in different quadrants:

>>> x = np.array([-1, +1, +1, -1])
>>> y = np.array([-1, -1, +1, +1])
>>> np.arctan2(y, x) * 180 / np.pi
array([-135.,  -45.,   45.,  135.])

Note the order of the parameters. `arctan2` is defined also when `x2` = 0
and at several other special points, obtaining values in
the range ``[-pi, pi]``:

>>> np.arctan2([1., -1.], [0., 0.])
array([ 1.57079633, -1.57079633])
>>> np.arctan2([0., 0., np.inf], [+0., -0., np.inf])
array([ 0.        ,  3.14159265,  0.78539816])
*)

val around : ?decimals:int -> ?out:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Evenly round to the given number of decimals.

Parameters
----------
a : array_like
    Input data.
decimals : int, optional
    Number of decimal places to round to (default: 0).  If
    decimals is negative, it specifies the number of positions to
    the left of the decimal point.
out : ndarray, optional
    Alternative output array in which to place the result. It must have
    the same shape as the expected output, but the type of the output
    values will be cast if necessary. See `ufuncs-output-type` for more
    details.

Returns
-------
rounded_array : ndarray
    An array of the same type as `a`, containing the rounded values.
    Unless `out` was specified, a new array is created.  A reference to
    the result is returned.

    The real and imaginary parts of complex numbers are rounded
    separately.  The result of rounding a float is a float.

See Also
--------
ndarray.round : equivalent method

ceil, fix, floor, rint, trunc


Notes
-----
For values exactly halfway between rounded decimal values, NumPy
rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,
-0.5 and 0.5 round to 0.0, etc.

``np.around`` uses a fast but sometimes inexact algorithm to round
floating-point datatypes. For positive `decimals` it is equivalent to
``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has
error due to the inexact representation of decimal fractions in the IEEE
floating point standard [1]_ and errors introduced when scaling by powers
of ten. For instance, note the extra '1' in the following:

    >>> np.round(56294995342131.5, 3)
    56294995342131.51

If your goal is to print such values with a fixed number of decimals, it is
preferable to use numpy's float printing routines to limit the number of
printed decimals:

    >>> np.format_float_positional(56294995342131.5, precision=3)
    '56294995342131.5'

The float printing routines use an accurate but much more computationally
demanding algorithm to compute the number of digits after the decimal
point.

Alternatively, Python's builtin `round` function uses a more accurate
but slower algorithm for 64-bit floating point values:

    >>> round(56294995342131.5, 3)
    56294995342131.5
    >>> np.round(16.055, 2), round(16.055, 2)  # equals 16.0549999999999997
    (16.06, 16.05)


References
----------
.. [1] 'Lecture Notes on the Status of IEEE 754', William Kahan,
       https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF
.. [2] 'How Futile are Mindless Assessments of
       Roundoff in Floating-Point Computation?', William Kahan,
       https://people.eecs.berkeley.edu/~wkahan/Mindless.pdf

Examples
--------
>>> np.around([0.37, 1.64])
array([0.,  2.])
>>> np.around([0.37, 1.64], decimals=1)
array([0.4,  1.6])
>>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value
array([0.,  2.,  2.,  4.,  4.])
>>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned
array([ 1,  2,  3, 11])
>>> np.around([1,2,3,11], decimals=-1)
array([ 0,  0,  0, 10])
*)

val array : ?dtype:Np.Dtype.t -> ?copy:bool -> ?order:[`K | `A | `C | `F] -> ?subok:bool -> ?ndmin:int -> object_:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)

Create an array.

Parameters
----------
object : array_like
    An array, any object exposing the array interface, an object whose
    __array__ method returns an array, or any (nested) sequence.
dtype : data-type, optional
    The desired data-type for the array.  If not given, then the type will
    be determined as the minimum type required to hold the objects in the
    sequence.
copy : bool, optional
    If true (default), then the object is copied.  Otherwise, a copy will
    only be made if __array__ returns a copy, if obj is a nested sequence,
    or if a copy is needed to satisfy any of the other requirements
    (`dtype`, `order`, etc.).
order : {'K', 'A', 'C', 'F'}, optional
    Specify the memory layout of the array. If object is not an array, the
    newly created array will be in C order (row major) unless 'F' is
    specified, in which case it will be in Fortran order (column major).
    If object is an array the following holds.

    ===== ========= ===================================================
    order  no copy                     copy=True
    ===== ========= ===================================================
    'K'   unchanged F & C order preserved, otherwise most similar order
    'A'   unchanged F order if input is F and not C, otherwise C order
    'C'   C order   C order
    'F'   F order   F order
    ===== ========= ===================================================

    When ``copy=False`` and a copy is made for other reasons, the result is
    the same as if ``copy=True``, with some exceptions for `A`, see the
    Notes section. The default order is 'K'.
subok : bool, optional
    If True, then sub-classes will be passed-through, otherwise
    the returned array will be forced to be a base-class array (default).
ndmin : int, optional
    Specifies the minimum number of dimensions that the resulting
    array should have.  Ones will be pre-pended to the shape as
    needed to meet this requirement.

Returns
-------
out : ndarray
    An array object satisfying the specified requirements.

See Also
--------
empty_like : Return an empty array with shape and type of input.
ones_like : Return an array of ones with shape and type of input.
zeros_like : Return an array of zeros with shape and type of input.
full_like : Return a new array with shape of input filled with value.
empty : Return a new uninitialized array.
ones : Return a new array setting values to one.
zeros : Return a new array setting values to zero.
full : Return a new array of given shape filled with value.


Notes
-----
When order is 'A' and `object` is an array in neither 'C' nor 'F' order,
and a copy is forced by a change in dtype, then the order of the result is
not necessarily 'C' as expected. This is likely a bug.

Examples
--------
>>> np.array([1, 2, 3])
array([1, 2, 3])

Upcasting:

>>> np.array([1, 2, 3.0])
array([ 1.,  2.,  3.])

More than one dimension:

>>> np.array([[1, 2], [3, 4]])
array([[1, 2],
       [3, 4]])

Minimum dimensions 2:

>>> np.array([1, 2, 3], ndmin=2)
array([[1, 2, 3]])

Type provided:

>>> np.array([1, 2, 3], dtype=complex)
array([ 1.+0.j,  2.+0.j,  3.+0.j])

Data-type consisting of more than one element:

>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])
>>> x['a']
array([1, 3])

Creating an array from sub-classes:

>>> np.array(np.mat('1 2; 3 4'))
array([[1, 2],
       [3, 4]])

>>> np.array(np.mat('1 2; 3 4'), subok=True)
matrix([[1, 2],
        [3, 4]])
*)

val asarray : ?dtype:Np.Dtype.t -> ?order:[`F | `C] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Convert the input to an array.

Parameters
----------
a : array_like
    Input data, in any form that can be converted to an array.  This
    includes lists, lists of tuples, tuples, tuples of tuples, tuples
    of lists and ndarrays.
dtype : data-type, optional
    By default, the data-type is inferred from the input data.
order : {'C', 'F'}, optional
    Whether to use row-major (C-style) or
    column-major (Fortran-style) memory representation.
    Defaults to 'C'.

Returns
-------
out : ndarray
    Array interpretation of `a`.  No copy is performed if the input
    is already an ndarray with matching dtype and order.  If `a` is a
    subclass of ndarray, a base class ndarray is returned.

See Also
--------
asanyarray : Similar function which passes through subclasses.
ascontiguousarray : Convert input to a contiguous array.
asfarray : Convert input to a floating point ndarray.
asfortranarray : Convert input to an ndarray with column-major
                 memory order.
asarray_chkfinite : Similar function which checks input for NaNs and Infs.
fromiter : Create an array from an iterator.
fromfunction : Construct an array by executing a function on grid
               positions.

Examples
--------
Convert a list into an array:

>>> a = [1, 2]
>>> np.asarray(a)
array([1, 2])

Existing arrays are not copied:

>>> a = np.array([1, 2])
>>> np.asarray(a) is a
True

If `dtype` is set, array is copied only if dtype does not match:

>>> a = np.array([1, 2], dtype=np.float32)
>>> np.asarray(a, dtype=np.float32) is a
True
>>> np.asarray(a, dtype=np.float64) is a
False

Contrary to `asanyarray`, ndarray subclasses are not passed through:

>>> issubclass(np.recarray, np.ndarray)
True
>>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)
>>> np.asarray(a) is a
False
>>> np.asanyarray(a) is a
True
*)

val atleast_1d : Py.Object.t list -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Convert inputs to arrays with at least one dimension.

Scalar inputs are converted to 1-dimensional arrays, whilst
higher-dimensional inputs are preserved.

Parameters
----------
arys1, arys2, ... : array_like
    One or more input arrays.

Returns
-------
ret : ndarray
    An array, or list of arrays, each with ``a.ndim >= 1``.
    Copies are made only if necessary.

See Also
--------
atleast_2d, atleast_3d

Examples
--------
>>> np.atleast_1d(1.0)
array([1.])

>>> x = np.arange(9.0).reshape(3,3)
>>> np.atleast_1d(x)
array([[0., 1., 2.],
       [3., 4., 5.],
       [6., 7., 8.]])
>>> np.atleast_1d(x) is x
True

>>> np.atleast_1d(1, [3, 4])
[array([1]), array([3, 4])]
*)

val bartlett : Py.Object.t list -> (float * float)
(**
Perform Bartlett's test for equal variances.

Bartlett's test tests the null hypothesis that all input samples
are from populations with equal variances.  For samples
from significantly non-normal populations, Levene's test
`levene` is more robust.

Parameters
----------
sample1, sample2,... : array_like
    arrays of sample data.  Only 1d arrays are accepted, they may have
    different lengths.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value of the test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
levene : A robust parametric test for equality of k variances

Notes
-----
Conover et al. (1981) examine many of the existing parametric and
nonparametric tests by extensive simulations and they conclude that the
tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be
superior in terms of robustness of departures from normality and power
([3]_).

References
----------
.. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm

.. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical
          Methods, Eighth Edition, Iowa State University Press.

.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.

.. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical
       Tests. Proceedings of the Royal Society of London. Series A,
       Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.
*)

val bayes_mvs : ?alpha:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Bayesian confidence intervals for the mean, var, and std.

Parameters
----------
data : array_like
    Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.
    Requires 2 or more data points.
alpha : float, optional
    Probability that the returned confidence interval contains
    the true parameter.

Returns
-------
mean_cntr, var_cntr, std_cntr : tuple
    The three results are for the mean, variance and standard deviation,
    respectively.  Each result is a tuple of the form::

        (center, (lower, upper))

    with `center` the mean of the conditional pdf of the value given the
    data, and `(lower, upper)` a confidence interval, centered on the
    median, containing the estimate to a probability ``alpha``.

See Also
--------
mvsdist

Notes
-----
Each tuple of mean, variance, and standard deviation estimates represent
the (center, (lower, upper)) with center the mean of the conditional pdf
of the value given the data and (lower, upper) is a confidence interval
centered on the median, containing the estimate to a probability
``alpha``.

Converts data to 1-D and assumes all data has the same mean and variance.
Uses Jeffrey's prior for variance and std.

Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``

References
----------
T.E. Oliphant, 'A Bayesian perspective on estimating mean, variance, and
standard-deviation from data', https://scholarsarchive.byu.edu/facpub/278,
2006.

Examples
--------
First a basic example to demonstrate the outputs:

>>> from scipy import stats
>>> data = [6, 9, 12, 7, 8, 8, 13]
>>> mean, var, std = stats.bayes_mvs(data)
>>> mean
Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))
>>> var
Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))
>>> std
Std_dev(statistic=2.9724954732045084, minmax=(1.7823367265645143, 4.945614605014631))

Now we generate some normally distributed random data, and get estimates of
mean and standard deviation with 95% confidence intervals for those
estimates:

>>> n_samples = 100000
>>> data = stats.norm.rvs(size=n_samples)
>>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)

>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.hist(data, bins=100, density=True, label='Histogram of data')
>>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')
>>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',
...            alpha=0.2, label=r'Estimated mean (95% limits)')
>>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')
>>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,
...            label=r'Estimated scale (95% limits)')

>>> ax.legend(fontsize=10)
>>> ax.set_xlim([-4, 4])
>>> ax.set_ylim([0, 0.5])
>>> plt.show()
*)

val binom_test : ?n:int -> ?p:float -> ?alternative:[`Two_sided | `Greater | `Less] -> x:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> unit -> Py.Object.t
(**
Perform a test that the probability of success is p.

This is an exact, two-sided test of the null hypothesis
that the probability of success in a Bernoulli experiment
is `p`.

Parameters
----------
x : int or array_like
    The number of successes, or if x has length 2, it is the
    number of successes and the number of failures.
n : int
    The number of trials.  This is ignored if x gives both the
    number of successes and failures.
p : float, optional
    The hypothesized probability of success.  ``0 <= p <= 1``. The
    default value is ``p = 0.5``.
alternative : {'two-sided', 'greater', 'less'}, optional
    Indicates the alternative hypothesis. The default value is
    'two-sided'.

Returns
-------
p-value : float
    The p-value of the hypothesis test.

References
----------
.. [1] https://en.wikipedia.org/wiki/Binomial_test

Examples
--------
>>> from scipy import stats

A car manufacturer claims that no more than 10% of their cars are unsafe.
15 cars are inspected for safety, 3 were found to be unsafe. Test the
manufacturer's claim:

>>> stats.binom_test(3, n=15, p=0.1, alternative='greater')
0.18406106910639114

The null hypothesis cannot be rejected at the 5% level of significance
because the returned p-value is greater than the critical value of 5%.
*)

val boxcox : ?lmbda:[`Bool of bool | `S of string | `I of int | `F of float] -> ?alpha:float -> x:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
Return a dataset transformed by a Box-Cox power transformation.

Parameters
----------
x : ndarray
    Input array.  Must be positive 1-dimensional.  Must not be constant.
lmbda : {None, scalar}, optional
    If `lmbda` is not None, do the transformation for that value.

    If `lmbda` is None, find the lambda that maximizes the log-likelihood
    function and return it as the second output argument.
alpha : {None, float}, optional
    If ``alpha`` is not None, return the ``100 * (1-alpha)%`` confidence
    interval for `lmbda` as the third output argument.
    Must be between 0.0 and 1.0.

Returns
-------
boxcox : ndarray
    Box-Cox power transformed array.
maxlog : float, optional
    If the `lmbda` parameter is None, the second returned argument is
    the lambda that maximizes the log-likelihood function.
(min_ci, max_ci) : tuple of float, optional
    If `lmbda` parameter is None and ``alpha`` is not None, this returned
    tuple of floats represents the minimum and maximum confidence limits
    given ``alpha``.

See Also
--------
probplot, boxcox_normplot, boxcox_normmax, boxcox_llf

Notes
-----
The Box-Cox transform is given by::

    y = (x**lmbda - 1) / lmbda,  for lmbda > 0
        log(x),                  for lmbda = 0

`boxcox` requires the input data to be positive.  Sometimes a Box-Cox
transformation provides a shift parameter to achieve this; `boxcox` does
not.  Such a shift parameter is equivalent to adding a positive constant to
`x` before calling `boxcox`.

The confidence limits returned when ``alpha`` is provided give the interval
where:

.. math::

    llf(\hat{\lambda}) - llf(\lambda) < \frac{1}{2}\chi^2(1 - \alpha, 1),

with ``llf`` the log-likelihood function and :math:`\chi^2` the chi-squared
function.

References
----------
G.E.P. Box and D.R. Cox, 'An Analysis of Transformations', Journal of the
Royal Statistical Society B, 26, 211-252 (1964).

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

We generate some random variates from a non-normal distribution and make a
probability plot for it, to show it is non-normal in the tails:

>>> fig = plt.figure()
>>> ax1 = fig.add_subplot(211)
>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)
>>> ax1.set_xlabel('')
>>> ax1.set_title('Probplot against normal distribution')

We now use `boxcox` to transform the data so it's closest to normal:

>>> ax2 = fig.add_subplot(212)
>>> xt, _ = stats.boxcox(x)
>>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)
>>> ax2.set_title('Probplot after Box-Cox transformation')

>>> plt.show()
*)

val boxcox_llf : lmb:[`F of float | `I of int | `Bool of bool | `S of string] -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
The boxcox log-likelihood function.

Parameters
----------
lmb : scalar
    Parameter for Box-Cox transformation.  See `boxcox` for details.
data : array_like
    Data to calculate Box-Cox log-likelihood for.  If `data` is
    multi-dimensional, the log-likelihood is calculated along the first
    axis.

Returns
-------
llf : float or ndarray
    Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,
    an array otherwise.

See Also
--------
boxcox, probplot, boxcox_normplot, boxcox_normmax

Notes
-----
The Box-Cox log-likelihood function is defined here as

.. math::

    llf = (\lambda - 1) \sum_i(\log(x_i)) -
          N/2 \log(\sum_i (y_i - \bar{y})^2 / N),

where ``y`` is the Box-Cox transformed input data ``x``.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes
>>> np.random.seed(1245)

Generate some random variates and calculate Box-Cox log-likelihood values
for them for a range of ``lmbda`` values:

>>> x = stats.loggamma.rvs(5, loc=10, size=1000)
>>> lmbdas = np.linspace(-2, 10)
>>> llf = np.zeros(lmbdas.shape, dtype=float)
>>> for ii, lmbda in enumerate(lmbdas):
...     llf[ii] = stats.boxcox_llf(lmbda, x)

Also find the optimal lmbda value with `boxcox`:

>>> x_most_normal, lmbda_optimal = stats.boxcox(x)

Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a
horizontal line to check that that's really the optimum:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(lmbdas, llf, 'b.-')
>>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')
>>> ax.set_xlabel('lmbda parameter')
>>> ax.set_ylabel('Box-Cox log-likelihood')

Now add some probability plots to show that where the log-likelihood is
maximized the data transformed with `boxcox` looks closest to normal:

>>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'
>>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):
...     xt = stats.boxcox(x, lmbda=lmbda)
...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)
...     ax_inset = inset_axes(ax, width='20%', height='20%', loc=loc)
...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')
...     ax_inset.set_xticklabels([])
...     ax_inset.set_yticklabels([])
...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda)

>>> plt.show()
*)

val boxcox_normmax : ?brack:Py.Object.t -> ?method_:string -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute optimal Box-Cox transform parameter for input data.

Parameters
----------
x : array_like
    Input array.
brack : 2-tuple, optional
    The starting interval for a downhill bracket search with
    `optimize.brent`.  Note that this is in most cases not critical; the
    final result is allowed to be outside this bracket.
method : str, optional
    The method to determine the optimal transform parameter (`boxcox`
    ``lmbda`` parameter). Options are:

    'pearsonr'  (default)
        Maximizes the Pearson correlation coefficient between
        ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be
        normally-distributed.

    'mle'
        Minimizes the log-likelihood `boxcox_llf`.  This is the method used
        in `boxcox`.

    'all'
        Use all optimization methods available, and return all results.
        Useful to compare different methods.

Returns
-------
maxlog : float or ndarray
    The optimal transform parameter found.  An array instead of a scalar
    for ``method='all'``.

See Also
--------
boxcox, boxcox_llf, boxcox_normplot

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234)  # make this example reproducible

Generate some data and determine optimal ``lmbda`` in various ways:

>>> x = stats.loggamma.rvs(5, size=30) + 5
>>> y, lmax_mle = stats.boxcox(x)
>>> lmax_pearsonr = stats.boxcox_normmax(x)

>>> lmax_mle
7.177...
>>> lmax_pearsonr
7.916...
>>> stats.boxcox_normmax(x, method='all')
array([ 7.91667384,  7.17718692])

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)
>>> ax.axvline(lmax_mle, color='r')
>>> ax.axvline(lmax_pearsonr, color='g', ls='--')

>>> plt.show()
*)

val boxcox_normplot : ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> la:Py.Object.t -> lb:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Compute parameters for a Box-Cox normality plot, optionally show it.

A Box-Cox normality plot shows graphically what the best transformation
parameter is to use in `boxcox` to obtain a distribution that is close
to normal.

Parameters
----------
x : array_like
    Input array.
la, lb : scalar
    The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`
    for Box-Cox transformations.  These are also the limits of the
    horizontal axis of the plot if that is generated.
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `la` to `lb`).

Returns
-------
lmbdas : ndarray
    The ``lmbda`` values for which a Box-Cox transform was done.
ppcc : ndarray
    Probability Plot Correlelation Coefficient, as obtained from `probplot`
    when fitting the Box-Cox transformed input `x` against a normal
    distribution.

See Also
--------
probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max

Notes
-----
Even if `plot` is given, the figure is not shown or saved by
`boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``
should be used after calling `probplot`.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

Generate some non-normally distributed data, and create a Box-Cox plot:

>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)

Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in
the same plot:

>>> _, maxlog = stats.boxcox(x)
>>> ax.axvline(maxlog, color='r')

>>> plt.show()
*)

val ceil : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
ceil(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Return the ceiling of the input, element-wise.

The ceil of the scalar `x` is the smallest integer `i`, such that
`i >= x`.  It is often denoted as :math:`\lceil x \rceil`.

Parameters
----------
x : array_like
    Input data.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray or scalar
    The ceiling of each element in `x`, with `float` dtype.
    This is a scalar if `x` is a scalar.

See Also
--------
floor, trunc, rint

Examples
--------
>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
>>> np.ceil(a)
array([-1., -1., -0.,  1.,  2.,  2.,  2.])
*)

val chi2_contingency : ?correction:bool -> ?lambda_:[`S of string | `F of float] -> observed:[>`Ndarray] Np.Obj.t -> unit -> (float * float * int * Py.Object.t)
(**
Chi-square test of independence of variables in a contingency table.

This function computes the chi-square statistic and p-value for the
hypothesis test of independence of the observed frequencies in the
contingency table [1]_ `observed`.  The expected frequencies are computed
based on the marginal sums under the assumption of independence; see
`scipy.stats.contingency.expected_freq`.  The number of degrees of
freedom is (expressed using numpy functions and attributes)::

    dof = observed.size - sum(observed.shape) + observed.ndim - 1


Parameters
----------
observed : array_like
    The contingency table. The table contains the observed frequencies
    (i.e. number of occurrences) in each category.  In the two-dimensional
    case, the table is often described as an 'R x C table'.
correction : bool, optional
    If True, *and* the degrees of freedom is 1, apply Yates' correction
    for continuity.  The effect of the correction is to adjust each
    observed value by 0.5 towards the corresponding expected value.
lambda_ : float or str, optional.
    By default, the statistic computed in this test is Pearson's
    chi-squared statistic [2]_.  `lambda_` allows a statistic from the
    Cressie-Read power divergence family [3]_ to be used instead.  See
    `power_divergence` for details.

Returns
-------
chi2 : float
    The test statistic.
p : float
    The p-value of the test
dof : int
    Degrees of freedom
expected : ndarray, same shape as `observed`
    The expected frequencies, based on the marginal sums of the table.

See Also
--------
contingency.expected_freq
fisher_exact
chisquare
power_divergence

Notes
-----
An often quoted guideline for the validity of this calculation is that
the test should be used only if the observed and expected frequencies
in each cell are at least 5.

This is a test for the independence of different categories of a
population. The test is only meaningful when the dimension of
`observed` is two or more.  Applying the test to a one-dimensional
table will always result in `expected` equal to `observed` and a
chi-square statistic equal to 0.

This function does not handle masked arrays, because the calculation
does not make sense with missing values.

Like stats.chisquare, this function computes a chi-square statistic;
the convenience this function provides is to figure out the expected
frequencies and degrees of freedom from the given contingency table.
If these were already known, and if the Yates' correction was not
required, one could use stats.chisquare.  That is, if one calls::

    chi2, p, dof, ex = chi2_contingency(obs, correction=False)

then the following is true::

    (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),
                                 ddof=obs.size - 1 - dof)

The `lambda_` argument was added in version 0.13.0 of scipy.

References
----------
.. [1] 'Contingency table',
       https://en.wikipedia.org/wiki/Contingency_table
.. [2] 'Pearson's chi-squared test',
       https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test
.. [3] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
A two-way example (2 x 3):

>>> from scipy.stats import chi2_contingency
>>> obs = np.array([[10, 10, 20], [20, 20, 20]])
>>> chi2_contingency(obs)
(2.7777777777777777,
 0.24935220877729619,
 2,
 array([[ 12.,  12.,  16.],
        [ 18.,  18.,  24.]]))

Perform the test using the log-likelihood ratio (i.e. the 'G-test')
instead of Pearson's chi-squared statistic.

>>> g, p, dof, expctd = chi2_contingency(obs, lambda_='log-likelihood')
>>> g, p
(2.7688587616781319, 0.25046668010954165)

A four-way example (2 x 2 x 2 x 2):

>>> obs = np.array(
...     [[[[12, 17],
...        [11, 16]],
...       [[11, 12],
...        [15, 16]]],
...      [[[23, 15],
...        [30, 22]],
...       [[14, 17],
...        [15, 16]]]])
>>> chi2_contingency(obs)
(8.7584514426741897,
 0.64417725029295503,
 11,
 array([[[[ 14.15462386,  14.15462386],
          [ 16.49423111,  16.49423111]],
         [[ 11.2461395 ,  11.2461395 ],
          [ 13.10500554,  13.10500554]]],
        [[[ 19.5591166 ,  19.5591166 ],
          [ 22.79202844,  22.79202844]],
         [[ 15.54012004,  15.54012004],
          [ 18.10873492,  18.10873492]]]]))
*)

val circmean : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular mean for samples in a range.

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular mean range.  Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular mean range.  Default is 0.
axis : int, optional
    Axis along which means are computed.  The default is to compute
    the mean of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circmean : float
    Circular mean.

Examples
--------
>>> from scipy.stats import circmean
>>> circmean([0.1, 2*np.pi+0.2, 6*np.pi+0.3])
0.2

>>> from scipy.stats import circmean
>>> circmean([0.2, 1.4, 2.6], high = 1, low = 0)
0.4
*)

val circstd : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular standard deviation for samples assumed to be in the
range [low to high].

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular standard deviation range.
    Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular standard deviation range.  Default is 0.
axis : int, optional
    Axis along which standard deviations are computed.  The default is
    to compute the standard deviation of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circstd : float
    Circular standard deviation.

Notes
-----
This uses a definition of circular standard deviation that in the limit of
small angles returns a number close to the 'linear' standard deviation.

Examples
--------
>>> from scipy.stats import circstd
>>> circstd([0, 0.1*np.pi/2, 0.001*np.pi, 0.03*np.pi/2])
0.063564063306
*)

val circvar : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular variance for samples assumed to be in a range.

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular variance range.  Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular variance range.  Default is 0.
axis : int, optional
    Axis along which variances are computed.  The default is to compute
    the variance of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circvar : float
    Circular variance.

Notes
-----
This uses a definition of circular variance that in the limit of small
angles returns a number close to the 'linear' variance.

Examples
--------
>>> from scipy.stats import circvar
>>> circvar([0, 2*np.pi/3, 5*np.pi/3])
2.19722457734
*)

val compress : ?axis:int -> ?out:[>`Ndarray] Np.Obj.t -> condition:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return selected slices of an array along given axis.

When working along a given axis, a slice along that axis is returned in
`output` for each index where `condition` evaluates to True. When
working on a 1-D array, `compress` is equivalent to `extract`.

Parameters
----------
condition : 1-D array of bools
    Array that selects which entries to return. If len(condition)
    is less than the size of `a` along the given axis, then output is
    truncated to the length of the condition array.
a : array_like
    Array from which to extract a part.
axis : int, optional
    Axis along which to take slices. If None (default), work on the
    flattened array.
out : ndarray, optional
    Output array.  Its type is preserved and it must be of the right
    shape to hold the output.

Returns
-------
compressed_array : ndarray
    A copy of `a` without the slices along axis for which `condition`
    is false.

See Also
--------
take, choose, diag, diagonal, select
ndarray.compress : Equivalent method in ndarray
np.extract: Equivalent method when working on 1-D arrays
ufuncs-output-type

Examples
--------
>>> a = np.array([[1, 2], [3, 4], [5, 6]])
>>> a
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.compress([0, 1], a, axis=0)
array([[3, 4]])
>>> np.compress([False, True, True], a, axis=0)
array([[3, 4],
       [5, 6]])
>>> np.compress([False, True], a, axis=1)
array([[2],
       [4],
       [6]])

Working on the flattened array does not return slices along an axis but
selects elements.

>>> np.compress([False, True], a)
array([2])
*)

val cos : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
cos(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Cosine element-wise.

Parameters
----------
x : array_like
    Input array in radians.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray
    The corresponding cosine values.
    This is a scalar if `x` is a scalar.

Notes
-----
If `out` is provided, the function writes the result into it,
and returns a reference to `out`.  (See Examples)

References
----------
M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions.
New York, NY: Dover, 1972.

Examples
--------
>>> np.cos(np.array([0, np.pi/2, np.pi]))
array([  1.00000000e+00,   6.12303177e-17,  -1.00000000e+00])
>>>
>>> # Example of providing the optional output parameter
>>> out1 = np.array([0], dtype='d')
>>> out2 = np.cos([0.1], out1)
>>> out2 is out1
True
>>>
>>> # Example of ValueError due to provision of shape mis-matched `out`
>>> np.cos(np.zeros((3,3)),np.zeros((2,2)))
Traceback (most recent call last):
  File '<stdin>', line 1, in <module>
ValueError: operands could not be broadcast together with shapes (3,3) (2,2)
*)

val count_nonzero : ?axis:[`Tuple of Py.Object.t | `I of int] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Counts the number of non-zero values in the array ``a``.

The word 'non-zero' is in reference to the Python 2.x
built-in method ``__nonzero__()`` (renamed ``__bool__()``
in Python 3.x) of Python objects that tests an object's
'truthfulness'. For example, any number is considered
truthful if it is nonzero, whereas any string is considered
truthful if it is not the empty string. Thus, this function
(recursively) counts how many elements in ``a`` (and in
sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``
method evaluated to ``True``.

Parameters
----------
a : array_like
    The array for which to count non-zeros.
axis : int or tuple, optional
    Axis or tuple of axes along which to count non-zeros.
    Default is None, meaning that non-zeros will be counted
    along a flattened version of ``a``.

    .. versionadded:: 1.12.0

Returns
-------
count : int or array of int
    Number of non-zero values in the array along a given axis.
    Otherwise, the total number of non-zero values in the array
    is returned.

See Also
--------
nonzero : Return the coordinates of all the non-zero values.

Examples
--------
>>> np.count_nonzero(np.eye(4))
4
>>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]])
5
>>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]], axis=0)
array([1, 1, 1, 1, 1])
>>> np.count_nonzero([[0,1,7,0,0],[3,0,0,2,19]], axis=1)
array([2, 3])
*)

val exp : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Calculate the exponential of all elements in the input array.

Parameters
----------
x : array_like
    Input values.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
out : ndarray or scalar
    Output array, element-wise exponential of `x`.
    This is a scalar if `x` is a scalar.

See Also
--------
expm1 : Calculate ``exp(x) - 1`` for all elements in the array.
exp2  : Calculate ``2**x`` for all elements in the array.

Notes
-----
The irrational number ``e`` is also known as Euler's number.  It is
approximately 2.718281, and is the base of the natural logarithm,
``ln`` (this means that, if :math:`x = \ln y = \log_e y`,
then :math:`e^x = y`. For real input, ``exp(x)`` is always positive.

For complex arguments, ``x = a + ib``, we can write
:math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already
known (it is the real argument, described above).  The second term,
:math:`e^{ib}`, is :math:`\cos b + i \sin b`, a function with
magnitude 1 and a periodic phase.

References
----------
.. [1] Wikipedia, 'Exponential function',
       https://en.wikipedia.org/wiki/Exponential_function
.. [2] M. Abramovitz and I. A. Stegun, 'Handbook of Mathematical Functions
       with Formulas, Graphs, and Mathematical Tables,' Dover, 1964, p. 69,
       http://www.math.sfu.ca/~cbm/aands/page_69.htm

Examples
--------
Plot the magnitude and phase of ``exp(x)`` in the complex plane:

>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-2*np.pi, 2*np.pi, 100)
>>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane
>>> out = np.exp(xx)

>>> plt.subplot(121)
>>> plt.imshow(np.abs(out),
...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')
>>> plt.title('Magnitude of exp(x)')

>>> plt.subplot(122)
>>> plt.imshow(np.angle(out),
...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')
>>> plt.title('Phase (angle) of exp(x)')
>>> plt.show()
*)

val find_repeats : [>`Ndarray] Np.Obj.t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find repeats and repeat counts.

Parameters
----------
arr : array_like
    Input array. This is cast to float64.

Returns
-------
values : ndarray
    The unique values from the (flattened) input that are repeated.

counts : ndarray
    Number of times the corresponding 'value' is repeated.

Notes
-----
In numpy >= 1.9 `numpy.unique` provides similar functionality. The main
difference is that `find_repeats` only returns repeated values.

Examples
--------
>>> from scipy import stats
>>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])
RepeatedResults(values=array([2.]), counts=array([4]))

>>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])
RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))
*)

val fligner : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Perform Fligner-Killeen test for equality of variance.

Fligner's test tests the null hypothesis that all input samples
are from populations with equal variances.  Fligner-Killeen's test is
distribution free when populations are identical [2]_.

Parameters
----------
sample1, sample2, ... : array_like
    Arrays of sample data.  Need not be the same length.
center : {'mean', 'median', 'trimmed'}, optional
    Keyword argument controlling which function of the data is used in
    computing the test statistic.  The default is 'median'.
proportiontocut : float, optional
    When `center` is 'trimmed', this gives the proportion of data points
    to cut from each end. (See `scipy.stats.trim_mean`.)
    Default is 0.05.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value for the hypothesis test.

See Also
--------
bartlett : A parametric test for equality of k variances in normal samples
levene : A robust parametric test for equality of k variances

Notes
-----
As with Levene's test there are three variants of Fligner's test that
differ by the measure of central tendency used in the test.  See `levene`
for more information.

Conover et al. (1981) examine many of the existing parametric and
nonparametric tests by extensive simulations and they conclude that the
tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be
superior in terms of robustness of departures from normality and power [3]_.

References
----------
.. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.
       https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf

.. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample
       tests for scale. 'Journal of the American Statistical Association.'
       71(353), 210-213.

.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.

.. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A
       comparative study of tests for homogeneity of variances, with
       applications to the outer continental shelf biding data.
       Technometrics, 23(4), 351-361.
*)

val floor : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
floor(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Return the floor of the input, element-wise.

The floor of the scalar `x` is the largest integer `i`, such that
`i <= x`.  It is often denoted as :math:`\lfloor x \rfloor`.

Parameters
----------
x : array_like
    Input data.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray or scalar
    The floor of each element in `x`.
    This is a scalar if `x` is a scalar.

See Also
--------
ceil, trunc, rint

Notes
-----
Some spreadsheet programs calculate the 'floor-towards-zero', in other
words ``floor(-2.5) == -2``.  NumPy instead uses the definition of
`floor` where `floor(-2.5) == -3`.

Examples
--------
>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
>>> np.floor(a)
array([-2., -2., -1.,  0.,  1.,  1.,  2.])
*)

val hypot : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:Py.Object.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
hypot(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Given the 'legs' of a right triangle, return its hypotenuse.

Equivalent to ``sqrt(x1**2 + x2**2)``, element-wise.  If `x1` or
`x2` is scalar_like (i.e., unambiguously cast-able to a scalar type),
it is broadcast for use with each element of the other argument.
(See Examples)

Parameters
----------
x1, x2 : array_like
    Leg of the triangle(s). If ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes the shape of the output).
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
z : ndarray
    The hypotenuse of the triangle(s).
    This is a scalar if both `x1` and `x2` are scalars.

Examples
--------
>>> np.hypot(3*np.ones((3, 3)), 4*np.ones((3, 3)))
array([[ 5.,  5.,  5.],
       [ 5.,  5.,  5.],
       [ 5.,  5.,  5.]])

Example showing broadcast of scalar_like argument:

>>> np.hypot(3*np.ones((3, 3)), [4])
array([[ 5.,  5.,  5.],
       [ 5.,  5.,  5.],
       [ 5.,  5.,  5.]])
*)

val isscalar : Py.Object.t -> bool
(**
Returns True if the type of `element` is a scalar type.

Parameters
----------
element : any
    Input argument, can be of any type and shape.

Returns
-------
val : bool
    True if `element` is a scalar type, False if it is not.

See Also
--------
ndim : Get the number of dimensions of an array

Notes
-----
If you need a stricter way to identify a *numerical* scalar, use
``isinstance(x, numbers.Number)``, as that returns ``False`` for most
non-numerical elements such as strings.

In most cases ``np.ndim(x) == 0`` should be used instead of this function,
as that will also return true for 0d arrays. This is how numpy overloads
functions in the style of the ``dx`` arguments to `gradient` and the ``bins``
argument to `histogram`. Some key differences:

+--------------------------------------+---------------+-------------------+
| x                                    |``isscalar(x)``|``np.ndim(x) == 0``|
+======================================+===============+===================+
| PEP 3141 numeric objects (including  | ``True``      | ``True``          |
| builtins)                            |               |                   |
+--------------------------------------+---------------+-------------------+
| builtin string and buffer objects    | ``True``      | ``True``          |
+--------------------------------------+---------------+-------------------+
| other builtin objects, like          | ``False``     | ``True``          |
| `pathlib.Path`, `Exception`,         |               |                   |
| the result of `re.compile`           |               |                   |
+--------------------------------------+---------------+-------------------+
| third-party objects like             | ``False``     | ``True``          |
| `matplotlib.figure.Figure`           |               |                   |
+--------------------------------------+---------------+-------------------+
| zero-dimensional numpy arrays        | ``False``     | ``True``          |
+--------------------------------------+---------------+-------------------+
| other numpy arrays                   | ``False``     | ``False``         |
+--------------------------------------+---------------+-------------------+
| `list`, `tuple`, and other sequence  | ``False``     | ``False``         |
| objects                              |               |                   |
+--------------------------------------+---------------+-------------------+

Examples
--------
>>> np.isscalar(3.1)
True
>>> np.isscalar(np.array(3.1))
False
>>> np.isscalar([3.1])
False
>>> np.isscalar(False)
True
>>> np.isscalar('numpy')
True

NumPy supports PEP 3141 numbers:

>>> from fractions import Fraction
>>> np.isscalar(Fraction(5, 17))
True
>>> from numbers import Number
>>> np.isscalar(Number())
True
*)

val kstat : ?n:[`Three | `I of int | `Two | `PyObject of Py.Object.t] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Return the nth k-statistic (1<=n<=4 so far).

The nth k-statistic k_n is the unique symmetric unbiased estimator of the
nth cumulant kappa_n.

Parameters
----------
data : array_like
    Input array. Note that n-D input gets flattened.
n : int, {1, 2, 3, 4}, optional
    Default is equal to 2.

Returns
-------
kstat : float
    The nth k-statistic.

See Also
--------
kstatvar: Returns an unbiased estimator of the variance of the k-statistic.
moment: Returns the n-th central moment about the mean for a sample.

Notes
-----
For a sample size n, the first few k-statistics are given by:

.. math::

    k_{1} = \mu
    k_{2} = \frac{n}{n-1} m_{2}
    k_{3} = \frac{ n^{2} } {(n-1) (n-2)} m_{3}
    k_{4} = \frac{ n^{2} [(n + 1)m_{4} - 3(n - 1) m^2_{2}]} {(n-1) (n-2) (n-3)}

where :math:`\mu` is the sample mean, :math:`m_2` is the sample
variance, and :math:`m_i` is the i-th sample central moment.

References
----------
http://mathworld.wolfram.com/k-Statistic.html

http://mathworld.wolfram.com/Cumulant.html

Examples
--------
>>> from scipy import stats
>>> rndm = np.random.RandomState(1234)

As sample size increases, n-th moment and n-th k-statistic converge to the
same number (although they aren't identical). In the case of the normal
distribution, they converge to zero.

>>> for n in [2, 3, 4, 5, 6, 7]:
...     x = rndm.normal(size=10**n)
...     m, k = stats.moment(x, 3), stats.kstat(x, 3)
...     print('%.3g %.3g %.3g' % (m, k, m-k))
-0.631 -0.651 0.0194
0.0282 0.0283 -8.49e-05
-0.0454 -0.0454 1.36e-05
7.53e-05 7.53e-05 -2.26e-09
0.00166 0.00166 -4.99e-09
-2.88e-06 -2.88e-06 8.63e-13
*)

val kstatvar : ?n:[`I of int | `PyObject of Py.Object.t] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Return an unbiased estimator of the variance of the k-statistic.

See `kstat` for more details of the k-statistic.

Parameters
----------
data : array_like
    Input array. Note that n-D input gets flattened.
n : int, {1, 2}, optional
    Default is equal to 2.

Returns
-------
kstatvar : float
    The nth k-statistic variance.

See Also
--------
kstat: Returns the n-th k-statistic.
moment: Returns the n-th central moment about the mean for a sample.

Notes
-----
The variances of the first few k-statistics are given by:

.. math::

    var(k_{1}) = \frac{\kappa^2}{n}
    var(k_{2}) = \frac{\kappa^4}{n} + \frac{2\kappa^2_{2}}{n - 1}
    var(k_{3}) = \frac{\kappa^6}{n} + \frac{9 \kappa_2 \kappa_4}{n - 1} +
                 \frac{9 \kappa^2_{3}}{n - 1} +
                 \frac{6 n \kappa^3_{2}}{(n-1) (n-2)}
    var(k_{4}) = \frac{\kappa^8}{n} + \frac{16 \kappa_2 \kappa_6}{n - 1} +
                 \frac{48 \kappa_{3} \kappa_5}{n - 1} +
                 \frac{34 \kappa^2_{4}}{n-1} + \frac{72 n \kappa^2_{2} \kappa_4}{(n - 1) (n - 2)} +
                 \frac{144 n \kappa_{2} \kappa^2_{3}}{(n - 1) (n - 2)} +
                 \frac{24 (n + 1) n \kappa^4_{2}}{(n - 1) (n - 2) (n - 3)}
*)

val levene : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Perform Levene test for equal variances.

The Levene test tests the null hypothesis that all input samples
are from populations with equal variances.  Levene's test is an
alternative to Bartlett's test `bartlett` in the case where
there are significant deviations from normality.

Parameters
----------
sample1, sample2, ... : array_like
    The sample data, possibly with different lengths. Only one-dimensional
    samples are accepted.
center : {'mean', 'median', 'trimmed'}, optional
    Which function of the data to use in the test.  The default
    is 'median'.
proportiontocut : float, optional
    When `center` is 'trimmed', this gives the proportion of data points
    to cut from each end. (See `scipy.stats.trim_mean`.)
    Default is 0.05.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value for the test.

Notes
-----
Three variations of Levene's test are possible.  The possibilities
and their recommended usages are:

  * 'median' : Recommended for skewed (non-normal) distributions>
  * 'mean' : Recommended for symmetric, moderate-tailed distributions.
  * 'trimmed' : Recommended for heavy-tailed distributions.

The test version using the mean was proposed in the original article
of Levene ([2]_) while the median and trimmed mean have been studied by
Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe
test.

References
----------
.. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm
.. [2]   Levene, H. (1960). In Contributions to Probability and Statistics:
           Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,
           Stanford University Press, pp. 278-292.
.. [3]  Brown, M. B. and Forsythe, A. B. (1974), Journal of the American
          Statistical Association, 69, 364-367
*)

val log : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
log(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Natural logarithm, element-wise.

The natural logarithm `log` is the inverse of the exponential function,
so that `log(exp(x)) = x`. The natural logarithm is logarithm in base
`e`.

Parameters
----------
x : array_like
    Input value.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray
    The natural logarithm of `x`, element-wise.
    This is a scalar if `x` is a scalar.

See Also
--------
log10, log2, log1p, emath.log

Notes
-----
Logarithm is a multivalued function: for each `x` there is an infinite
number of `z` such that `exp(z) = x`. The convention is to return the
`z` whose imaginary part lies in `[-pi, pi]`.

For real-valued input data types, `log` always returns real output. For
each value that cannot be expressed as a real number or infinity, it
yields ``nan`` and sets the `invalid` floating point error flag.

For complex-valued input, `log` is a complex analytical function that
has a branch cut `[-inf, 0]` and is continuous from above on it. `log`
handles the floating-point negative zero as an infinitesimal negative
number, conforming to the C99 standard.

References
----------
.. [1] M. Abramowitz and I.A. Stegun, 'Handbook of Mathematical Functions',
       10th printing, 1964, pp. 67. http://www.math.sfu.ca/~cbm/aands/
.. [2] Wikipedia, 'Logarithm'. https://en.wikipedia.org/wiki/Logarithm

Examples
--------
>>> np.log([1, np.e, np.e**2, 0])
array([  0.,   1.,   2., -Inf])
*)

val median_test : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float * float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Perform a Mood's median test.

Test that two or more samples come from populations with the same median.

Let ``n = len(args)`` be the number of samples.  The 'grand median' of
all the data is computed, and a contingency table is formed by
classifying the values in each sample as being above or below the grand
median.  The contingency table, along with `correction` and `lambda_`,
are passed to `scipy.stats.chi2_contingency` to compute the test statistic
and p-value.

Parameters
----------
sample1, sample2, ... : array_like
    The set of samples.  There must be at least two samples.
    Each sample must be a one-dimensional sequence containing at least
    one value.  The samples are not required to have the same length.
ties : str, optional
    Determines how values equal to the grand median are classified in
    the contingency table.  The string must be one of::

        'below':
            Values equal to the grand median are counted as 'below'.
        'above':
            Values equal to the grand median are counted as 'above'.
        'ignore':
            Values equal to the grand median are not counted.

    The default is 'below'.
correction : bool, optional
    If True, *and* there are just two samples, apply Yates' correction
    for continuity when computing the test statistic associated with
    the contingency table.  Default is True.
lambda_ : float or str, optional
    By default, the statistic computed in this test is Pearson's
    chi-squared statistic.  `lambda_` allows a statistic from the
    Cressie-Read power divergence family to be used instead.  See
    `power_divergence` for details.
    Default is 1 (Pearson's chi-squared statistic).
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
stat : float
    The test statistic.  The statistic that is returned is determined by
    `lambda_`.  The default is Pearson's chi-squared statistic.
p : float
    The p-value of the test.
m : float
    The grand median.
table : ndarray
    The contingency table.  The shape of the table is (2, n), where
    n is the number of samples.  The first row holds the counts of the
    values above the grand median, and the second row holds the counts
    of the values below the grand median.  The table allows further
    analysis with, for example, `scipy.stats.chi2_contingency`, or with
    `scipy.stats.fisher_exact` if there are two samples, without having
    to recompute the table.  If ``nan_policy`` is 'propagate' and there
    are nans in the input, the return value for ``table`` is ``None``.

See Also
--------
kruskal : Compute the Kruskal-Wallis H-test for independent samples.
mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.

Notes
-----
.. versionadded:: 0.15.0

References
----------
.. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill
    (1950), pp. 394-399.
.. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).
    See Sections 8.12 and 10.15.

Examples
--------
A biologist runs an experiment in which there are three groups of plants.
Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.
Each plant produces a number of seeds.  The seed counts for each group
are::

    Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49
    Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99
    Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84

The following code applies Mood's median test to these samples.

>>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]
>>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]
>>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]
>>> from scipy.stats import median_test
>>> stat, p, med, tbl = median_test(g1, g2, g3)

The median is

>>> med
34.0

and the contingency table is

>>> tbl
array([[ 5, 10,  7],
       [11,  5, 10]])

`p` is too large to conclude that the medians are not the same:

>>> p
0.12609082774093244

The 'G-test' can be performed by passing ``lambda_='log-likelihood'`` to
`median_test`.

>>> g, p, med, tbl = median_test(g1, g2, g3, lambda_='log-likelihood')
>>> p
0.12224779737117837

The median occurs several times in the data, so we'll get a different
result if, for example, ``ties='above'`` is used:

>>> stat, p, med, tbl = median_test(g1, g2, g3, ties='above')
>>> p
0.063873276069553273

>>> tbl
array([[ 5, 11,  9],
       [11,  4,  8]])

This example demonstrates that if the data set is not large and there
are values equal to the median, the p-value can be sensitive to the
choice of `ties`.
*)

val mood : ?axis:int -> x:Py.Object.t -> y:Py.Object.t -> unit -> Py.Object.t
(**
Perform Mood's test for equal scale parameters.

Mood's two-sample test for scale parameters is a non-parametric
test for the null hypothesis that two samples are drawn from the
same distribution with the same scale parameter.

Parameters
----------
x, y : array_like
    Arrays of sample data.
axis : int, optional
    The axis along which the samples are tested.  `x` and `y` can be of
    different length along `axis`.
    If `axis` is None, `x` and `y` are flattened and the test is done on
    all values in the flattened arrays.

Returns
-------
z : scalar or ndarray
    The z-score for the hypothesis test.  For 1-D inputs a scalar is
    returned.
p-value : scalar ndarray
    The p-value for the hypothesis test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
ansari : A non-parametric test for the equality of 2 variances
bartlett : A parametric test for equality of k variances in normal samples
levene : A parametric test for equality of k variances

Notes
-----
The data are assumed to be drawn from probability distributions ``f(x)``
and ``f(x/s) / s`` respectively, for some probability density function f.
The null hypothesis is that ``s == 1``.

For multi-dimensional arrays, if the inputs are of shapes
``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the
resulting z and p values will have shape ``(n0, n2, n3)``.  Note that
``n1`` and ``m1`` don't have to be equal, but the other dimensions do.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(1234)
>>> x2 = np.random.randn(2, 45, 6, 7)
>>> x1 = np.random.randn(2, 30, 6, 7)
>>> z, p = stats.mood(x1, x2, axis=1)
>>> p.shape
(2, 6, 7)

Find the number of points where the difference in scale is not significant:

>>> (p > 0.1).sum()
74

Perform the test with different scales:

>>> x1 = np.random.randn(2, 30)
>>> x2 = np.random.randn(2, 35) * 10.0
>>> stats.mood(x1, x2, axis=1)
(array([-5.7178125 , -5.25342163]), array([  1.07904114e-08,   1.49299218e-07]))
*)

val mvsdist : [>`Ndarray] Np.Obj.t -> (Py.Object.t * Py.Object.t * Py.Object.t)
(**
'Frozen' distributions for mean, variance, and standard deviation of data.

Parameters
----------
data : array_like
    Input array. Converted to 1-D using ravel.
    Requires 2 or more data-points.

Returns
-------
mdist : 'frozen' distribution object
    Distribution object representing the mean of the data.
vdist : 'frozen' distribution object
    Distribution object representing the variance of the data.
sdist : 'frozen' distribution object
    Distribution object representing the standard deviation of the data.

See Also
--------
bayes_mvs

Notes
-----
The return values from ``bayes_mvs(data)`` is equivalent to
``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.

In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``
on the three distribution objects returned from this function will give
the same results that are returned from `bayes_mvs`.

References
----------
T.E. Oliphant, 'A Bayesian perspective on estimating mean, variance, and
standard-deviation from data', https://scholarsarchive.byu.edu/facpub/278,
2006.

Examples
--------
>>> from scipy import stats
>>> data = [6, 9, 12, 7, 8, 8, 13]
>>> mean, var, std = stats.mvsdist(data)

We now have frozen distribution objects 'mean', 'var' and 'std' that we can
examine:

>>> mean.mean()
9.0
>>> mean.interval(0.95)
(6.6120585482655692, 11.387941451734431)
>>> mean.std()
1.1952286093343936
*)

val namedtuple : ?rename:Py.Object.t -> ?defaults:Py.Object.t -> ?module_:Py.Object.t -> typename:Py.Object.t -> field_names:Py.Object.t -> unit -> Py.Object.t
(**
Returns a new subclass of tuple with named fields.

>>> Point = namedtuple('Point', ['x', 'y'])
>>> Point.__doc__                   # docstring for the new class
'Point(x, y)'
>>> p = Point(11, y=22)             # instantiate with positional args or keywords
>>> p[0] + p[1]                     # indexable like a plain tuple
33
>>> x, y = p                        # unpack like a regular tuple
>>> x, y
(11, 22)
>>> p.x + p.y                       # fields also accessible by name
33
>>> d = p._asdict()                 # convert to a dictionary
>>> d['x']
11
>>> Point( **d)                      # convert from a dictionary
Point(x=11, y=22)
>>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields
Point(x=100, y=22)
*)

val ppcc_max : ?brack:Py.Object.t -> ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> x:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Calculate the shape parameter that maximizes the PPCC.

The probability plot correlation coefficient (PPCC) plot can be used to
determine the optimal shape parameter for a one-parameter family of
distributions.  ppcc_max returns the shape parameter that would maximize the
probability plot correlation coefficient for the given data to a
one-parameter family of distributions.

Parameters
----------
x : array_like
    Input array.
brack : tuple, optional
    Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)
    then they are assumed to be a starting interval for a downhill bracket
    search (see `scipy.optimize.brent`).
dist : str or stats.distributions instance, optional
    Distribution or distribution function name.  Objects that look enough
    like a stats.distributions instance (i.e. they have a ``ppf`` method)
    are also accepted.  The default is ``'tukeylambda'``.

Returns
-------
shape_value : float
    The shape parameter at which the probability plot correlation
    coefficient reaches its max value.

See Also
--------
ppcc_plot, probplot, boxcox

Notes
-----
The brack keyword serves as a starting point which is useful in corner
cases. One can use a plot to obtain a rough visual estimate of the location
for the maximum to start the search near it.

References
----------
.. [1] J.J. Filliben, 'The Probability Plot Correlation Coefficient Test for
       Normality', Technometrics, Vol. 17, pp. 111-117, 1975.

.. [2] https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm

Examples
--------
First we generate some random data from a Tukey-Lambda distribution,
with shape parameter -0.7:

>>> from scipy import stats
>>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000,
...                           random_state=1234567) + 1e4

Now we explore this data with a PPCC plot as well as the related
probability plot and Box-Cox normplot.  A red line is drawn where we
expect the PPCC value to be maximal (at the shape parameter -0.7 used
above):

>>> import matplotlib.pyplot as plt
>>> fig = plt.figure(figsize=(8, 6))
>>> ax = fig.add_subplot(111)
>>> res = stats.ppcc_plot(x, -5, 5, plot=ax)

We calculate the value where the shape should reach its maximum and a red
line is drawn there. The line should coincide with the highest point in the
ppcc_plot.

>>> max = stats.ppcc_max(x)
>>> ax.vlines(max, 0, 1, colors='r', label='Expected shape value')

>>> plt.show()
*)

val ppcc_plot : ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> a:Py.Object.t -> b:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Calculate and optionally plot probability plot correlation coefficient.

The probability plot correlation coefficient (PPCC) plot can be used to
determine the optimal shape parameter for a one-parameter family of
distributions.  It cannot be used for distributions without shape parameters
(like the normal distribution) or with multiple shape parameters.

By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A
Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed
distributions via an approximately normal one, and is therefore particularly
useful in practice.

Parameters
----------
x : array_like
    Input array.
a, b : scalar
    Lower and upper bounds of the shape parameter to use.
dist : str or stats.distributions instance, optional
    Distribution or distribution function name.  Objects that look enough
    like a stats.distributions instance (i.e. they have a ``ppf`` method)
    are also accepted.  The default is ``'tukeylambda'``.
plot : object, optional
    If given, plots PPCC against the shape parameter.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `a` to `b`).

Returns
-------
svals : ndarray
    The shape values for which `ppcc` was calculated.
ppcc : ndarray
    The calculated probability plot correlation coefficient values.

See Also
--------
ppcc_max, probplot, boxcox_normplot, tukeylambda

References
----------
J.J. Filliben, 'The Probability Plot Correlation Coefficient Test for
Normality', Technometrics, Vol. 17, pp. 111-117, 1975.

Examples
--------
First we generate some random data from a Tukey-Lambda distribution,
with shape parameter -0.7:

>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234567)
>>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000) + 1e4

Now we explore this data with a PPCC plot as well as the related
probability plot and Box-Cox normplot.  A red line is drawn where we
expect the PPCC value to be maximal (at the shape parameter -0.7 used
above):

>>> fig = plt.figure(figsize=(12, 4))
>>> ax1 = fig.add_subplot(131)
>>> ax2 = fig.add_subplot(132)
>>> ax3 = fig.add_subplot(133)
>>> res = stats.probplot(x, plot=ax1)
>>> res = stats.boxcox_normplot(x, -5, 5, plot=ax2)
>>> res = stats.ppcc_plot(x, -5, 5, plot=ax3)
>>> ax3.vlines(-0.7, 0, 1, colors='r', label='Expected shape value')
>>> plt.show()
*)

val probplot : ?sparams:Py.Object.t -> ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> ?fit:bool -> ?plot:Py.Object.t -> ?rvalue:Py.Object.t -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate quantiles for a probability plot, and optionally show the plot.

Generates a probability plot of sample data against the quantiles of a
specified theoretical distribution (the normal distribution by default).
`probplot` optionally calculates a best-fit line for the data and plots the
results using Matplotlib or a given plot function.

Parameters
----------
x : array_like
    Sample/response data from which `probplot` creates the plot.
sparams : tuple, optional
    Distribution-specific shape parameters (shape parameters plus location
    and scale).
dist : str or stats.distributions instance, optional
    Distribution or distribution function name. The default is 'norm' for a
    normal probability plot.  Objects that look enough like a
    stats.distributions instance (i.e. they have a ``ppf`` method) are also
    accepted.
fit : bool, optional
    Fit a least-squares regression (best-fit) line to the sample data if
    True (default).
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.

Returns
-------
(osm, osr) : tuple of ndarrays
    Tuple of theoretical quantiles (osm, or order statistic medians) and
    ordered responses (osr).  `osr` is simply sorted input `x`.
    For details on how `osm` is calculated see the Notes section.
(slope, intercept, r) : tuple of floats, optional
    Tuple  containing the result of the least-squares fit, if that is
    performed by `probplot`. `r` is the square root of the coefficient of
    determination.  If ``fit=False`` and ``plot=None``, this tuple is not
    returned.

Notes
-----
Even if `plot` is given, the figure is not shown or saved by `probplot`;
``plt.show()`` or ``plt.savefig('figname.png')`` should be used after
calling `probplot`.

`probplot` generates a probability plot, which should not be confused with
a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this
type, see ``statsmodels.api.ProbPlot``.

The formula used for the theoretical quantiles (horizontal axis of the
probability plot) is Filliben's estimate::

    quantiles = dist.ppf(val), for

            0.5**(1/n),                  for i = n
      val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1
            1 - 0.5**(1/n),              for i = 1

where ``i`` indicates the i-th ordered value and ``n`` is the total number
of values.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> nsample = 100
>>> np.random.seed(7654321)

A t distribution with small degrees of freedom:

>>> ax1 = plt.subplot(221)
>>> x = stats.t.rvs(3, size=nsample)
>>> res = stats.probplot(x, plot=plt)

A t distribution with larger degrees of freedom:

>>> ax2 = plt.subplot(222)
>>> x = stats.t.rvs(25, size=nsample)
>>> res = stats.probplot(x, plot=plt)

A mixture of two normal distributions with broadcasting:

>>> ax3 = plt.subplot(223)
>>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],
...                    size=(nsample//2,2)).ravel()
>>> res = stats.probplot(x, plot=plt)

A standard normal distribution:

>>> ax4 = plt.subplot(224)
>>> x = stats.norm.rvs(loc=0, scale=1, size=nsample)
>>> res = stats.probplot(x, plot=plt)

Produce a new figure with a loggamma distribution, using the ``dist`` and
``sparams`` keywords:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> x = stats.loggamma.rvs(c=2.5, size=500)
>>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)
>>> ax.set_title('Probplot for loggamma dist with shape parameter 2.5')

Show the results with Matplotlib:

>>> plt.show()
*)

val ravel : ?order:[`C | `F | `A | `K] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return a contiguous flattened array.

A 1-D array, containing the elements of the input, is returned.  A copy is
made only if needed.

As of NumPy 1.10, the returned array will have the same type as the input
array. (for example, a masked array will be returned for a masked array
input)

Parameters
----------
a : array_like
    Input array.  The elements in `a` are read in the order specified by
    `order`, and packed as a 1-D array.
order : {'C','F', 'A', 'K'}, optional

    The elements of `a` are read using this index order. 'C' means
    to index the elements in row-major, C-style order,
    with the last axis index changing fastest, back to the first
    axis index changing slowest.  'F' means to index the elements
    in column-major, Fortran-style order, with the
    first index changing fastest, and the last index changing
    slowest. Note that the 'C' and 'F' options take no account of
    the memory layout of the underlying array, and only refer to
    the order of axis indexing.  'A' means to read the elements in
    Fortran-like index order if `a` is Fortran *contiguous* in
    memory, C-like order otherwise.  'K' means to read the
    elements in the order they occur in memory, except for
    reversing the data when strides are negative.  By default, 'C'
    index order is used.

Returns
-------
y : array_like
    y is an array of the same subtype as `a`, with shape ``(a.size,)``.
    Note that matrices are special cased for backward compatibility, if `a`
    is a matrix, then y is a 1-D ndarray.

See Also
--------
ndarray.flat : 1-D iterator over an array.
ndarray.flatten : 1-D array copy of the elements of an array
                  in row-major order.
ndarray.reshape : Change the shape of an array without changing its data.

Notes
-----
In row-major, C-style order, in two dimensions, the row index
varies the slowest, and the column index the quickest.  This can
be generalized to multiple dimensions, where row-major order
implies that the index along the first axis varies slowest, and
the index along the last quickest.  The opposite holds for
column-major, Fortran-style index ordering.

When a view is desired in as many cases as possible, ``arr.reshape(-1)``
may be preferable.

Examples
--------
It is equivalent to ``reshape(-1, order=order)``.

>>> x = np.array([[1, 2, 3], [4, 5, 6]])
>>> np.ravel(x)
array([1, 2, 3, 4, 5, 6])

>>> x.reshape(-1)
array([1, 2, 3, 4, 5, 6])

>>> np.ravel(x, order='F')
array([1, 4, 2, 5, 3, 6])

When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:

>>> np.ravel(x.T)
array([1, 4, 2, 5, 3, 6])
>>> np.ravel(x.T, order='A')
array([1, 2, 3, 4, 5, 6])

When ``order`` is 'K', it will preserve orderings that are neither 'C'
nor 'F', but won't reverse axes:

>>> a = np.arange(3)[::-1]; a
array([2, 1, 0])
>>> a.ravel(order='C')
array([2, 1, 0])
>>> a.ravel(order='K')
array([2, 1, 0])

>>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a
array([[[ 0,  2,  4],
        [ 1,  3,  5]],
       [[ 6,  8, 10],
        [ 7,  9, 11]]])
>>> a.ravel(order='C')
array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])
>>> a.ravel(order='K')
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
*)

val shapiro : [>`Ndarray] Np.Obj.t -> float
(**
Perform the Shapiro-Wilk test for normality.

The Shapiro-Wilk test tests the null hypothesis that the
data was drawn from a normal distribution.

Parameters
----------
x : array_like
    Array of sample data.

Returns
-------
W : float
    The test statistic.
p-value : float
    The p-value for the hypothesis test.

See Also
--------
anderson : The Anderson-Darling test for normality
kstest : The Kolmogorov-Smirnov test for goodness of fit.

Notes
-----
The algorithm used is described in [4]_ but censoring parameters as
described are not implemented. For N > 5000 the W test statistic is accurate
but the p-value may not be.

The chance of rejecting the null hypothesis when it is true is close to 5%
regardless of sample size.

References
----------
.. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm
.. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for
       normality (complete samples), Biometrika, Vol. 52, pp. 591-611.
.. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,
       Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of
       Statistical Modeling and Analytics, Vol. 2, pp. 21-33.
.. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)
>>> x = stats.norm.rvs(loc=5, scale=3, size=100)
>>> stats.shapiro(x)
(0.9772805571556091, 0.08144091814756393)
*)

val sin : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Trigonometric sine, element-wise.

Parameters
----------
x : array_like
    Angle, in radians (:math:`2 \pi` rad equals 360 degrees).
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : array_like
    The sine of each element of x.
    This is a scalar if `x` is a scalar.

See Also
--------
arcsin, sinh, cos

Notes
-----
The sine is one of the fundamental functions of trigonometry (the
mathematical study of triangles).  Consider a circle of radius 1
centered on the origin.  A ray comes in from the :math:`+x` axis, makes
an angle at the origin (measured counter-clockwise from that axis), and
departs from the origin.  The :math:`y` coordinate of the outgoing
ray's intersection with the unit circle is the sine of that angle.  It
ranges from -1 for :math:`x=3\pi / 2` to +1 for :math:`\pi / 2.`  The
function has zeroes where the angle is a multiple of :math:`\pi`.
Sines of angles between :math:`\pi` and :math:`2\pi` are negative.
The numerous properties of the sine and related functions are included
in any standard trigonometry text.

Examples
--------
Print sine of one angle:

>>> np.sin(np.pi/2.)
1.0

Print sines of an array of angles given in degrees:

>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )
array([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])

Plot the sine function:

>>> import matplotlib.pylab as plt
>>> x = np.linspace(-np.pi, np.pi, 201)
>>> plt.plot(x, np.sin(x))
>>> plt.xlabel('Angle [rad]')
>>> plt.ylabel('sin(x)')
>>> plt.axis('tight')
>>> plt.show()
*)

val sort : ?axis:[`I of int | `None] -> ?kind:[`Stable | `Quicksort | `Heapsort | `Mergesort] -> ?order:[`StringList of string list | `S of string] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return a sorted copy of an array.

Parameters
----------
a : array_like
    Array to be sorted.
axis : int or None, optional
    Axis along which to sort. If None, the array is flattened before
    sorting. The default is -1, which sorts along the last axis.
kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional
    Sorting algorithm. The default is 'quicksort'. Note that both 'stable'
    and 'mergesort' use timsort or radix sort under the covers and, in general,
    the actual implementation will vary with data type. The 'mergesort' option
    is retained for backwards compatibility.

    .. versionchanged:: 1.15.0.
       The 'stable' option was added.

order : str or list of str, optional
    When `a` is an array with fields defined, this argument specifies
    which fields to compare first, second, etc.  A single field can
    be specified as a string, and not all fields need be specified,
    but unspecified fields will still be used, in the order in which
    they come up in the dtype, to break ties.

Returns
-------
sorted_array : ndarray
    Array of the same type and shape as `a`.

See Also
--------
ndarray.sort : Method to sort an array in-place.
argsort : Indirect sort.
lexsort : Indirect stable sort on multiple keys.
searchsorted : Find elements in a sorted array.
partition : Partial sort.

Notes
-----
The various sorting algorithms are characterized by their average speed,
worst case performance, work space size, and whether they are stable. A
stable sort keeps items with the same key in the same relative
order. The four algorithms implemented in NumPy have the following
properties:

=========== ======= ============= ============ ========
   kind      speed   worst case    work space   stable
=========== ======= ============= ============ ========
'quicksort'    1     O(n^2)            0          no
'heapsort'     3     O(n*log(n))       0          no
'mergesort'    2     O(n*log(n))      ~n/2        yes
'timsort'      2     O(n*log(n))      ~n/2        yes
=========== ======= ============= ============ ========

.. note:: The datatype determines which of 'mergesort' or 'timsort'
   is actually used, even if 'mergesort' is specified. User selection
   at a finer scale is not currently available.

All the sort algorithms make temporary copies of the data when
sorting along any but the last axis.  Consequently, sorting along
the last axis is faster and uses less space than sorting along
any other axis.

The sort order for complex numbers is lexicographic. If both the real
and imaginary parts are non-nan then the order is determined by the
real parts except when they are equal, in which case the order is
determined by the imaginary parts.

Previous to numpy 1.4.0 sorting real and complex arrays containing nan
values led to undefined behaviour. In numpy versions >= 1.4.0 nan
values are sorted to the end. The extended sort order is:

  * Real: [R, nan]
  * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]

where R is a non-nan real value. Complex values with the same nan
placements are sorted according to the non-nan part if it exists.
Non-nan values are sorted as before.

.. versionadded:: 1.12.0

quicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.
When sorting does not make enough progress it switches to
`heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.
This implementation makes quicksort O(n*log(n)) in the worst case.

'stable' automatically chooses the best stable sorting algorithm
for the data type being sorted.
It, along with 'mergesort' is currently mapped to
`timsort <https://en.wikipedia.org/wiki/Timsort>`_
or `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_
depending on the data type.
API forward compatibility currently limits the
ability to select the implementation and it is hardwired for the different
data types.

.. versionadded:: 1.17.0

Timsort is added for better performance on already or nearly
sorted data. On random data timsort is almost identical to
mergesort. It is now used for stable sort while quicksort is still the
default sort if none is chosen. For timsort details, refer to
`CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.
'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an
O(n) sort instead of O(n log n).

.. versionchanged:: 1.17.0

NaT now sorts to the end of arrays for consistency with NaN.

Examples
--------
>>> a = np.array([[1,4],[3,1]])
>>> np.sort(a)                # sort along the last axis
array([[1, 4],
       [1, 3]])
>>> np.sort(a, axis=None)     # sort the flattened array
array([1, 1, 3, 4])
>>> np.sort(a, axis=0)        # sort along the first axis
array([[1, 1],
       [3, 4]])

Use the `order` keyword to specify a field to use when sorting a
structured array:

>>> dtype = [('name', 'S10'), ('height', float), ('age', int)]
>>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),
...           ('Galahad', 1.7, 38)]
>>> a = np.array(values, dtype=dtype)       # create a structured array
>>> np.sort(a, order='height')                        # doctest: +SKIP
array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),
       ('Lancelot', 1.8999999999999999, 38)],
      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])

Sort by age, then height if ages are equal:

>>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP
array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),
       ('Arthur', 1.8, 41)],
      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])
*)

val sqrt : ?out:[`Ndarray of [>`Ndarray] Np.Obj.t | `Tuple_of_ndarray_and_None of Py.Object.t] -> ?where:[>`Ndarray] Np.Obj.t -> x:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])

Return the non-negative square-root of an array, element-wise.

Parameters
----------
x : array_like
    The values whose square-roots are required.
out : ndarray, None, or tuple of ndarray and None, optional
    A location into which the result is stored. If provided, it must have
    a shape that the inputs broadcast to. If not provided or None,
    a freshly-allocated array is returned. A tuple (possible only as a
    keyword argument) must have length equal to the number of outputs.
where : array_like, optional
    This condition is broadcast over the input. At locations where the
    condition is True, the `out` array will be set to the ufunc result.
    Elsewhere, the `out` array will retain its original value.
    Note that if an uninitialized `out` array is created via the default
    ``out=None``, locations within it where the condition is False will
    remain uninitialized.
**kwargs
    For other keyword-only arguments, see the
    :ref:`ufunc docs <ufuncs.kwargs>`.

Returns
-------
y : ndarray
    An array of the same shape as `x`, containing the positive
    square-root of each element in `x`.  If any element in `x` is
    complex, a complex array is returned (and the square-roots of
    negative reals are calculated).  If all of the elements in `x`
    are real, so is `y`, with negative elements returning ``nan``.
    If `out` was provided, `y` is a reference to it.
    This is a scalar if `x` is a scalar.

See Also
--------
lib.scimath.sqrt
    A version which returns complex numbers when given negative reals.

Notes
-----
*sqrt* has--consistent with common convention--as its branch cut the
real 'interval' [`-inf`, 0), and is continuous from above on it.
A branch cut is a curve in the complex plane across which a given
complex function fails to be continuous.

Examples
--------
>>> np.sqrt([1,4,9])
array([ 1.,  2.,  3.])

>>> np.sqrt([4, -1, -3+4J])
array([ 2.+0.j,  0.+1.j,  1.+2.j])

>>> np.sqrt([4, -1, np.inf])
array([ 2., nan, inf])
*)

val unique : ?return_index:bool -> ?return_inverse:bool -> ?return_counts:bool -> ?axis:int -> ar:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find the unique elements of an array.

Returns the sorted unique elements of an array. There are three optional
outputs in addition to the unique elements:

* the indices of the input array that give the unique values
* the indices of the unique array that reconstruct the input array
* the number of times each unique value comes up in the input array

Parameters
----------
ar : array_like
    Input array. Unless `axis` is specified, this will be flattened if it
    is not already 1-D.
return_index : bool, optional
    If True, also return the indices of `ar` (along the specified axis,
    if provided, or in the flattened array) that result in the unique array.
return_inverse : bool, optional
    If True, also return the indices of the unique array (for the specified
    axis, if provided) that can be used to reconstruct `ar`.
return_counts : bool, optional
    If True, also return the number of times each unique item appears
    in `ar`.

    .. versionadded:: 1.9.0

axis : int or None, optional
    The axis to operate on. If None, `ar` will be flattened. If an integer,
    the subarrays indexed by the given axis will be flattened and treated
    as the elements of a 1-D array with the dimension of the given axis,
    see the notes for more details.  Object arrays or structured arrays
    that contain objects are not supported if the `axis` kwarg is used. The
    default is None.

    .. versionadded:: 1.13.0

Returns
-------
unique : ndarray
    The sorted unique values.
unique_indices : ndarray, optional
    The indices of the first occurrences of the unique values in the
    original array. Only provided if `return_index` is True.
unique_inverse : ndarray, optional
    The indices to reconstruct the original array from the
    unique array. Only provided if `return_inverse` is True.
unique_counts : ndarray, optional
    The number of times each of the unique values comes up in the
    original array. Only provided if `return_counts` is True.

    .. versionadded:: 1.9.0

See Also
--------
numpy.lib.arraysetops : Module with a number of other functions for
                        performing set operations on arrays.

Notes
-----
When an axis is specified the subarrays indexed by the axis are sorted.
This is done by making the specified axis the first dimension of the array
(move the axis to the first dimension to keep the order of the other axes)
and then flattening the subarrays in C order. The flattened subarrays are
then viewed as a structured type with each element given a label, with the
effect that we end up with a 1-D array of structured types that can be
treated in the same way as any other 1-D array. The result is that the
flattened subarrays are sorted in lexicographic order starting with the
first element.

Examples
--------
>>> np.unique([1, 1, 2, 2, 3, 3])
array([1, 2, 3])
>>> a = np.array([[1, 1], [2, 3]])
>>> np.unique(a)
array([1, 2, 3])

Return the unique rows of a 2D array

>>> a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])
>>> np.unique(a, axis=0)
array([[1, 0, 0], [2, 3, 4]])

Return the indices of the original array that give the unique values:

>>> a = np.array(['a', 'b', 'b', 'c', 'a'])
>>> u, indices = np.unique(a, return_index=True)
>>> u
array(['a', 'b', 'c'], dtype='<U1')
>>> indices
array([0, 1, 3])
>>> a[indices]
array(['a', 'b', 'c'], dtype='<U1')

Reconstruct the input array from the unique values:

>>> a = np.array([1, 2, 6, 4, 2, 3, 2])
>>> u, indices = np.unique(a, return_inverse=True)
>>> u
array([1, 2, 3, 4, 6])
>>> indices
array([0, 1, 4, ..., 1, 2, 1])
>>> u[indices]
array([1, 2, 6, ..., 2, 3, 2])
*)

val wilcoxon : ?y:[>`Ndarray] Np.Obj.t -> ?zero_method:[`Pratt | `Wilcox | `Zsplit] -> ?correction:bool -> ?alternative:[`Two_sided | `Greater | `Less] -> x:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculate the Wilcoxon signed-rank test.

The Wilcoxon signed-rank test tests the null hypothesis that two
related paired samples come from the same distribution. In particular,
it tests whether the distribution of the differences x - y is symmetric
about zero. It is a non-parametric version of the paired T-test.

Parameters
----------
x : array_like
    Either the first set of measurements (in which case `y` is the second
    set of measurements), or the differences between two sets of
    measurements (in which case `y` is not to be specified.)  Must be
    one-dimensional.
y : array_like, optional
    Either the second set of measurements (if `x` is the first set of
    measurements), or not specified (if `x` is the differences between
    two sets of measurements.)  Must be one-dimensional.
zero_method : {'pratt', 'wilcox', 'zsplit'}, optional
    The following options are available (default is 'wilcox'):
 
      * 'pratt': Includes zero-differences in the ranking process,
        but drops the ranks of the zeros, see [4]_, (more conservative).
      * 'wilcox': Discards all zero-differences, the default.
      * 'zsplit': Includes zero-differences in the ranking process and 
        split the zero rank between positive and negative ones.
correction : bool, optional
    If True, apply continuity correction by adjusting the Wilcoxon rank
    statistic by 0.5 towards the mean value when computing the
    z-statistic.  Default is False.
alternative : {'two-sided', 'greater', 'less'}, optional
    The alternative hypothesis to be tested, see Notes. Default is
    'two-sided'.

Returns
-------
statistic : float
    If `alternative` is 'two-sided', the sum of the ranks of the
    differences above or below zero, whichever is smaller.
    Otherwise the sum of the ranks of the differences above zero.
pvalue : float
    The p-value for the test depending on `alternative`.

See Also
--------
kruskal, mannwhitneyu

Notes
-----
The test has been introduced in [4]_. Given n independent samples
(xi, yi) from a bivariate distribution (i.e. paired samples),
it computes the differences di = xi - yi. One assumption of the test
is that the differences are symmetric, see [2]_.
The two-sided test has the null hypothesis that the median of the
differences is zero against the alternative that it is different from
zero. The one-sided test has the null hypothesis that the median is 
positive against the alternative that it is negative 
(``alternative == 'less'``), or vice versa (``alternative == 'greater.'``).

The test uses a normal approximation to derive the p-value (if
``zero_method == 'pratt'``, the approximation is adjusted as in [5]_).
A typical rule is to require that n > 20 ([2]_, p. 383). For smaller n,
exact tables can be used to find critical values.

References
----------
.. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
.. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.
.. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed
   Rank Procedures, Journal of the American Statistical Association,
   Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`
.. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,
   Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`
.. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank
   Sampling Distribution When Zero Differences are Present,
   Journal of the American Statistical Association, Vol. 62, 1967,
   pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`

Examples
--------
In [4]_, the differences in height between cross- and self-fertilized
corn plants is given as follows:

>>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]

Cross-fertilized plants appear to be be higher. To test the null
hypothesis that there is no height difference, we can apply the
two-sided test:

>>> from scipy.stats import wilcoxon
>>> w, p = wilcoxon(d)
>>> w, p
(24.0, 0.04088813291185591)

Hence, we would reject the null hypothesis at a confidence level of 5%,
concluding that there is a difference in height between the groups.
To confirm that the median of the differences can be assumed to be
positive, we use:

>>> w, p = wilcoxon(d, alternative='greater')
>>> w, p
(96.0, 0.020444066455927955)

This shows that the null hypothesis that the median is negative can be
rejected at a confidence level of 5% in favor of the alternative that
the median is greater than zero. The p-value based on the approximation
is within the range of 0.019 and 0.054 given in [2]_.
Note that the statistic changed to 96 in the one-sided case (the sum
of ranks of positive differences) whereas it is 24 in the two-sided
case (the minimum of sum of ranks above and below zero).
*)

val yeojohnson : ?lmbda:float -> x:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
Return a dataset transformed by a Yeo-Johnson power transformation.

Parameters
----------
x : ndarray
    Input array.  Should be 1-dimensional.
lmbda : float, optional
    If ``lmbda`` is ``None``, find the lambda that maximizes the
    log-likelihood function and return it as the second output argument.
    Otherwise the transformation is done for the given value.

Returns
-------
yeojohnson: ndarray
    Yeo-Johnson power transformed array.
maxlog : float, optional
    If the `lmbda` parameter is None, the second returned argument is
    the lambda that maximizes the log-likelihood function.

See Also
--------
probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox

Notes
-----
The Yeo-Johnson transform is given by::

    y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0
        log(x + 1),                                  for x >= 0, lmbda = 0
        -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2
        -log(-x + 1),                                for x < 0, lmbda = 2

Unlike `boxcox`, `yeojohnson` does not require the input data to be
positive.

.. versionadded:: 1.2.0


References
----------
I. Yeo and R.A. Johnson, 'A New Family of Power Transformations to
Improve Normality or Symmetry', Biometrika 87.4 (2000):


Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

We generate some random variates from a non-normal distribution and make a
probability plot for it, to show it is non-normal in the tails:

>>> fig = plt.figure()
>>> ax1 = fig.add_subplot(211)
>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)
>>> ax1.set_xlabel('')
>>> ax1.set_title('Probplot against normal distribution')

We now use `yeojohnson` to transform the data so it's closest to normal:

>>> ax2 = fig.add_subplot(212)
>>> xt, lmbda = stats.yeojohnson(x)
>>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)
>>> ax2.set_title('Probplot after Yeo-Johnson transformation')

>>> plt.show()
*)

val yeojohnson_llf : lmb:[`F of float | `I of int | `Bool of bool | `S of string] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
The yeojohnson log-likelihood function.

Parameters
----------
lmb : scalar
    Parameter for Yeo-Johnson transformation. See `yeojohnson` for
    details.
data : array_like
    Data to calculate Yeo-Johnson log-likelihood for. If `data` is
    multi-dimensional, the log-likelihood is calculated along the first
    axis.

Returns
-------
llf : float
    Yeo-Johnson log-likelihood of `data` given `lmb`.

See Also
--------
yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax

Notes
-----
The Yeo-Johnson log-likelihood function is defined here as

.. math::

    llf = N/2 \log(\hat{\sigma}^2) + (\lambda - 1)
          \sum_i \text{ sign }(x_i)\log(|x_i| + 1)

where :math:`\hat{\sigma}^2` is estimated variance of the the Yeo-Johnson
transformed input data ``x``.

.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes
>>> np.random.seed(1245)

Generate some random variates and calculate Yeo-Johnson log-likelihood
values for them for a range of ``lmbda`` values:

>>> x = stats.loggamma.rvs(5, loc=10, size=1000)
>>> lmbdas = np.linspace(-2, 10)
>>> llf = np.zeros(lmbdas.shape, dtype=float)
>>> for ii, lmbda in enumerate(lmbdas):
...     llf[ii] = stats.yeojohnson_llf(lmbda, x)

Also find the optimal lmbda value with `yeojohnson`:

>>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)

Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a
horizontal line to check that that's really the optimum:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(lmbdas, llf, 'b.-')
>>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')
>>> ax.set_xlabel('lmbda parameter')
>>> ax.set_ylabel('Yeo-Johnson log-likelihood')

Now add some probability plots to show that where the log-likelihood is
maximized the data transformed with `yeojohnson` looks closest to normal:

>>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'
>>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):
...     xt = stats.yeojohnson(x, lmbda=lmbda)
...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)
...     ax_inset = inset_axes(ax, width='20%', height='20%', loc=loc)
...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')
...     ax_inset.set_xticklabels([])
...     ax_inset.set_yticklabels([])
...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda)

>>> plt.show()
*)

val yeojohnson_normmax : ?brack:Py.Object.t -> x:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute optimal Yeo-Johnson transform parameter.

Compute optimal Yeo-Johnson transform parameter for input data, using
maximum likelihood estimation.

Parameters
----------
x : array_like
    Input array.
brack : 2-tuple, optional
    The starting interval for a downhill bracket search with
    `optimize.brent`. Note that this is in most cases not critical; the
    final result is allowed to be outside this bracket.

Returns
-------
maxlog : float
    The optimal transform parameter found.

See Also
--------
yeojohnson, yeojohnson_llf, yeojohnson_normplot

Notes
-----
.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234)  # make this example reproducible

Generate some data and determine optimal ``lmbda``

>>> x = stats.loggamma.rvs(5, size=30) + 5
>>> lmax = stats.yeojohnson_normmax(x)

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)
>>> ax.axvline(lmax, color='r')

>>> plt.show()
*)

val yeojohnson_normplot : ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> la:Py.Object.t -> lb:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Compute parameters for a Yeo-Johnson normality plot, optionally show it.

A Yeo-Johnson normality plot shows graphically what the best
transformation parameter is to use in `yeojohnson` to obtain a
distribution that is close to normal.

Parameters
----------
x : array_like
    Input array.
la, lb : scalar
    The lower and upper bounds for the ``lmbda`` values to pass to
    `yeojohnson` for Yeo-Johnson transformations. These are also the
    limits of the horizontal axis of the plot if that is generated.
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `la` to `lb`).

Returns
-------
lmbdas : ndarray
    The ``lmbda`` values for which a Yeo-Johnson transform was done.
ppcc : ndarray
    Probability Plot Correlelation Coefficient, as obtained from `probplot`
    when fitting the Box-Cox transformed input `x` against a normal
    distribution.

See Also
--------
probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max

Notes
-----
Even if `plot` is given, the figure is not shown or saved by
`boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``
should be used after calling `probplot`.

.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

Generate some non-normally distributed data, and create a Yeo-Johnson plot:

>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)

Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in
the same plot:

>>> _, maxlog = stats.yeojohnson(x)
>>> ax.axvline(maxlog, color='r')

>>> plt.show()
*)

val zeros : ?dtype:Np.Dtype.t -> ?order:[`C | `F] -> shape:int list -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
zeros(shape, dtype=float, order='C')

Return a new array of given shape and type, filled with zeros.

Parameters
----------
shape : int or tuple of ints
    Shape of the new array, e.g., ``(2, 3)`` or ``2``.
dtype : data-type, optional
    The desired data-type for the array, e.g., `numpy.int8`.  Default is
    `numpy.float64`.
order : {'C', 'F'}, optional, default: 'C'
    Whether to store multi-dimensional data in row-major
    (C-style) or column-major (Fortran-style) order in
    memory.

Returns
-------
out : ndarray
    Array of zeros with the given shape, dtype, and order.

See Also
--------
zeros_like : Return an array of zeros with shape and type of input.
empty : Return a new uninitialized array.
ones : Return a new array setting values to one.
full : Return a new array of given shape filled with value.

Examples
--------
>>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])

>>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])

>>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])

>>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])

>>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
*)


end

module Mstats : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

val argstoarray : Py.Object.t list -> Py.Object.t
(**
Constructs a 2D array from a group of sequences.

Sequences are filled with missing values to match the length of the longest
sequence.

Parameters
----------
args : sequences
    Group of sequences.

Returns
-------
argstoarray : MaskedArray
    A ( `m` x `n` ) masked array, where `m` is the number of arguments and
    `n` the length of the longest argument.

Notes
-----
`numpy.ma.row_stack` has identical behavior, but is called with a sequence
of sequences.
*)

val brunnermunzel : ?alternative:[`Less | `Two_sided | `Greater] -> ?distribution:[`T | `Normal] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes the Brunner-Munzel test on samples x and y

Missing values in `x` and/or `y` are discarded.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
alternative :  'less', 'two-sided', or 'greater', optional
    Whether to get the p-value for the one-sided hypothesis ('less'
    or 'greater') or for the two-sided hypothesis ('two-sided').
    Defaults value is 'two-sided' .
distribution: 't' or 'normal', optional
    Whether to get the p-value by t-distribution or by standard normal
    distribution.
    Defaults value is 't' .

Returns
-------
statistic : float
    The Brunner-Munzer W statistic.
pvalue : float
    p-value assuming an t distribution. One-sided or
    two-sided, depending on the choice of `alternative` and `distribution`.

See Also
--------
mannwhitneyu : Mann-Whitney rank test on two samples.

Notes
-------
For more details on `brunnermunzel`, see `stats.brunnermunzel`.
*)

val chisquare : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate a one-way chi-square test.

The chi-square test tests the null hypothesis that the categorical data
has the given frequencies.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.

Returns
-------
chisq : float or ndarray
    The chi-squared test statistic.  The value is a float if `axis` is
    None or `f_obs` and `f_exp` are 1-D.
p : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `chisq` are scalars.

See Also
--------
scipy.stats.power_divergence

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not chi-square, in which case this test
is not appropriate.

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test

Examples
--------
When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.

>>> from scipy.stats import chisquare
>>> chisquare([16, 18, 16, 14, 12, 12])
(2.0, 0.84914503608460956)

With `f_exp` the expected frequencies can be given.

>>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])
(3.5, 0.62338762774958223)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> chisquare(obs)
(array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> chisquare(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> chisquare(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
chi-squared statistic with `ddof`.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we use ``axis=1``:

>>> chisquare([16, 18, 16, 14, 12, 12],
...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],
...           axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val compare_medians_ms : ?axis:int -> group_1:[>`Ndarray] Np.Obj.t -> group_2:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compares the medians from two independent groups along the given axis.

The comparison is performed using the McKean-Schrader estimate of the
standard error of the medians.

Parameters
----------
group_1 : array_like
    First dataset.  Has to be of size >=7.
group_2 : array_like
    Second dataset.  Has to be of size >=7.
axis : int, optional
    Axis along which the medians are estimated. If None, the arrays are
    flattened.  If `axis` is not None, then `group_1` and `group_2`
    should have the same shape.

Returns
-------
compare_medians_ms : {float, ndarray}
    If `axis` is None, then returns a float, otherwise returns a 1-D
    ndarray of floats with a length equal to the length of `group_1`
    along `axis`.
*)

val count_tied_groups : ?use_missing:bool -> x:Py.Object.t -> unit -> Py.Object.t
(**
Counts the number of tied values.

Parameters
----------
x : sequence
    Sequence of data on which to counts the ties
use_missing : bool, optional
    Whether to consider missing values as tied.

Returns
-------
count_tied_groups : dict
    Returns a dictionary (nb of ties: nb of groups).

Examples
--------
>>> from scipy.stats import mstats
>>> z = [0, 0, 0, 2, 2, 2, 3, 3, 4, 5, 6]
>>> mstats.count_tied_groups(z)
{2: 1, 3: 2}

In the above example, the ties were 0 (3x), 2 (3x) and 3 (2x).

>>> z = np.ma.array([0, 0, 1, 2, 2, 2, 3, 3, 4, 5, 6])
>>> mstats.count_tied_groups(z)
{2: 2, 3: 1}
>>> z[[1,-1]] = np.ma.masked
>>> mstats.count_tied_groups(z, use_missing=True)
{2: 2, 3: 1}
*)

val describe : ?axis:[`I of int | `None] -> ?ddof:int -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> (int * (int * int) * float * float * float * float)
(**
Computes several descriptive statistics of the passed array.

Parameters
----------
a : array_like
    Data array
axis : int or None, optional
    Axis along which to calculate statistics. Default 0. If None,
    compute over the whole array `a`.
ddof : int, optional
    degree of freedom (default 0); note that default ddof is different
    from the same routine in stats.describe
bias : bool, optional
    If False, then the skewness and kurtosis calculations are corrected for
    statistical bias.

Returns
-------
nobs : int
    (size of the data (discarding missing values)

minmax : (int, int)
    min, max

mean : float
    arithmetic mean

variance : float
    unbiased variance

skewness : float
    biased skewness

kurtosis : float
    biased kurtosis

Examples
--------
>>> from scipy.stats.mstats import describe
>>> ma = np.ma.array(range(6), mask=[0, 0, 0, 1, 1, 1])
>>> describe(ma)
DescribeResult(nobs=3, minmax=(masked_array(data=0,
             mask=False,
       fill_value=999999), masked_array(data=2,
             mask=False,
       fill_value=999999)), mean=1.0, variance=0.6666666666666666,
       skewness=masked_array(data=0., mask=False, fill_value=1e+20),
        kurtosis=-1.5)
*)

val f_oneway : Py.Object.t list -> (float * float)
(**
Performs a 1-way ANOVA, returning an F-value and probability given
any number of groups.  From Heiman, pp.394-7.

Usage: ``f_oneway( *args)``, where ``*args`` is 2 or more arrays,
one per treatment group.

Returns
-------
statistic : float
    The computed F-value of the test.
pvalue : float
    The associated p-value from the F-distribution.
*)

val find_repeats : Py.Object.t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find repeats in arr and return a tuple (repeats, repeat_count).

The input is cast to float64. Masked values are discarded.

Parameters
----------
arr : sequence
    Input array. The array is flattened if it is not 1D.

Returns
-------
repeats : ndarray
    Array of repeated values.
counts : ndarray
    Array of counts.
*)

val friedmanchisquare : Py.Object.t list -> (float * float)
(**
Friedman Chi-Square is a non-parametric, one-way within-subjects ANOVA.
This function calculates the Friedman Chi-square test for repeated measures
and returns the result, along with the associated probability value.

Each input is considered a given group. Ideally, the number of treatments
among each group should be equal. If this is not the case, only the first
n treatments are taken into account, where n is the number of treatments
of the smallest group.
If a group has some missing values, the corresponding treatments are masked
in the other groups.
The test statistic is corrected for ties.

Masked values in one group are propagated to the other groups.

Returns
-------
statistic : float
    the test statistic.
pvalue : float
    the associated p-value.
*)

val gmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the geometric mean along the specified axis.

Return the geometric average of the array elements.
That is:  n-th root of (x1 * x2 * ... * xn)

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the geometric mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If dtype is not specified, it defaults to the
    dtype of a, unless a has an integer dtype with a precision less than
    that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
gmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
hmean : Harmonic mean

Notes
-----
The geometric average is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity because masked
arrays automatically mask any non-finite values.

Examples
--------
>>> from scipy.stats import gmean
>>> gmean([1, 4])
2.0
>>> gmean([1, 2, 3, 4, 5, 6, 7])
3.3800151591412964
*)

val hdmedian : ?axis:int -> ?var:bool -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns the Harrell-Davis estimate of the median along the given axis.

Parameters
----------
data : ndarray
    Data array.
axis : int, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
var : bool, optional
    Whether to return the variance of the estimate.

Returns
-------
hdmedian : MaskedArray
    The median values.  If ``var=True``, the variance is returned inside
    the masked array.  E.g. for a 1-D array the shape change from (1,) to
    (2,).
*)

val hdquantiles : ?prob:Py.Object.t -> ?axis:int -> ?var:bool -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes quantile estimates with the Harrell-Davis method.

The quantile estimates are calculated as a weighted linear combination
of order statistics.

Parameters
----------
data : array_like
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
var : bool, optional
    Whether to return the variance of the estimate.

Returns
-------
hdquantiles : MaskedArray
    A (p,) array of quantiles (if `var` is False), or a (2,p) array of
    quantiles and variances (if `var` is True), where ``p`` is the
    number of quantiles.

See Also
--------
hdquantiles_sd
*)

val hdquantiles_sd : ?prob:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
The standard error of the Harrell-Davis quantile estimates by jackknife.

Parameters
----------
data : array_like
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.

Returns
-------
hdquantiles_sd : MaskedArray
    Standard error of the Harrell-Davis quantile estimates.

See Also
--------
hdquantiles
*)

val hmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the harmonic mean along the specified axis.

That is:  n / (1/x1 + 1/x2 + ... + 1/xn)

Parameters
----------
a : array_like
    Input array, masked array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the harmonic mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If `dtype` is not specified, it defaults to the
    dtype of `a`, unless `a` has an integer `dtype` with a precision less
    than that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
hmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
gmean : Geometric mean

Notes
-----
The harmonic mean is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity.

Examples
--------
>>> from scipy.stats import hmean
>>> hmean([1, 4])
1.6000000000000001
>>> hmean([1, 2, 3, 4, 5, 6, 7])
2.6997245179063363
*)

val idealfourths : ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns an estimate of the lower and upper quartiles.

Uses the ideal fourths algorithm.

Parameters
----------
data : array_like
    Input array.
axis : int, optional
    Axis along which the quartiles are estimated. If None, the arrays are
    flattened.

Returns
-------
idealfourths : {list of floats, masked array}
    Returns the two internal values that divide `data` into four parts
    using the ideal fourths algorithm either along the flattened array
    (if `axis` is None) or along `axis` of `data`.
*)

val kendalltau : ?use_ties:bool -> ?use_missing:bool -> ?method_:[`Auto | `Asymptotic | `Exact] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes Kendall's rank correlation tau on two variables *x* and *y*.

Parameters
----------
x : sequence
    First data list (for example, time).
y : sequence
    Second data list.
use_ties : {True, False}, optional
    Whether ties correction should be performed.
use_missing : {False, True}, optional
    Whether missing data should be allocated a rank of 0 (False) or the
    average rank (True)
method: {'auto', 'asymptotic', 'exact'}, optional
    Defines which method is used to calculate the p-value [1]_.
    'asymptotic' uses a normal approximation valid for large samples.
    'exact' computes the exact p-value, but can only be used if no ties
    are present. 'auto' is the default and selects the appropriate
    method based on a trade-off between speed and accuracy.

Returns
-------
correlation : float
    Kendall tau
pvalue : float
    Approximate 2-side p-value.

References
----------
.. [1] Maurice G. Kendall, 'Rank Correlation Methods' (4th Edition),
       Charles Griffin & Co., 1970.
*)

val kendalltau_seasonal : Py.Object.t -> Py.Object.t
(**
Computes a multivariate Kendall's rank correlation tau, for seasonal data.

Parameters
----------
x : 2-D ndarray
    Array of seasonal data, with seasons in columns.
*)

val kruskal : Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution

Notes
-----
For more details on `kruskal`, see `stats.kruskal`.

Examples
--------
>>> from scipy.stats.mstats import kruskal

Random samples from three different brands of batteries were tested
to see how long the charge lasted. Results were as follows:

>>> a = [6.3, 5.4, 5.7, 5.2, 5.0]
>>> b = [6.9, 7.0, 6.1, 7.9]
>>> c = [7.2, 6.9, 6.1, 6.5]

Test the hypotesis that the distribution functions for all of the brands'
durations are identical. Use 5% level of significance.

>>> kruskal(a, b, c)
KruskalResult(statistic=7.113812154696133, pvalue=0.028526948491942164)

The null hypothesis is rejected at the 5% level of significance
because the returned p-value is less than the critical value of 5%.
*)

val kruskalwallis : Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution

Notes
-----
For more details on `kruskal`, see `stats.kruskal`.

Examples
--------
>>> from scipy.stats.mstats import kruskal

Random samples from three different brands of batteries were tested
to see how long the charge lasted. Results were as follows:

>>> a = [6.3, 5.4, 5.7, 5.2, 5.0]
>>> b = [6.9, 7.0, 6.1, 7.9]
>>> c = [7.2, 6.9, 6.1, 6.5]

Test the hypotesis that the distribution functions for all of the brands'
durations are identical. Use 5% level of significance.

>>> kruskal(a, b, c)
KruskalResult(statistic=7.113812154696133, pvalue=0.028526948491942164)

The null hypothesis is rejected at the 5% level of significance
because the returned p-value is less than the critical value of 5%.
*)

val ks_2samp : ?alternative:[`Two_sided | `Less | `Greater] -> data1:[>`Ndarray] Np.Obj.t -> data2:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Kolmogorov-Smirnov test on two samples.

Missing values are discarded.

Parameters
----------
data1 : array_like
    First data set
data2 : array_like
    Second data set
alternative : {'two-sided', 'less', 'greater'}, optional
    Indicates the alternative hypothesis.  Default is 'two-sided'.

Returns
-------
d : float
    Value of the Kolmogorov Smirnov test
p : float
    Corresponding p-value.
*)

val ks_twosamp : ?alternative:[`Two_sided | `Less | `Greater] -> data1:[>`Ndarray] Np.Obj.t -> data2:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Kolmogorov-Smirnov test on two samples.

Missing values are discarded.

Parameters
----------
data1 : array_like
    First data set
data2 : array_like
    Second data set
alternative : {'two-sided', 'less', 'greater'}, optional
    Indicates the alternative hypothesis.  Default is 'two-sided'.

Returns
-------
d : float
    Value of the Kolmogorov Smirnov test
p : float
    Corresponding p-value.
*)

val kurtosis : ?axis:[`I of int | `None] -> ?fisher:bool -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the kurtosis (Fisher or Pearson) of a dataset.

Kurtosis is the fourth central moment divided by the square of the
variance. If Fisher's definition is used, then 3.0 is subtracted from
the result to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics to
eliminate bias coming from biased moment estimators

Use `kurtosistest` to see if result is close enough to normal.

Parameters
----------
a : array
    data for which the kurtosis is calculated
axis : int or None, optional
    Axis along which the kurtosis is calculated. Default is 0.
    If None, compute over the whole array `a`.
fisher : bool, optional
    If True, Fisher's definition is used (normal ==> 0.0). If False,
    Pearson's definition is used (normal ==> 3.0).
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.

Returns
-------
kurtosis : array
    The kurtosis of values along an axis. If all values are equal,
    return -3 for Fisher's definition and 0 for Pearson's definition.

Notes
-----
For more details about `kurtosis`, see `stats.kurtosis`.
*)

val kurtosistest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Tests whether a dataset has normal kurtosis

Parameters
----------
a : array
    array of the sample data
axis : int or None, optional
   Axis along which to compute test. Default is 0. If None,
   compute over the whole array `a`.

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    The 2-sided p-value for the hypothesis test

Notes
-----
For more details about `kurtosistest`, see `stats.kurtosistest`.
*)

val linregress : ?y:Py.Object.t -> x:Py.Object.t -> unit -> Py.Object.t
(**
Linear regression calculation

Note that the non-masked version is used, and that this docstring is
replaced by the non-masked docstring + some info on missing data.
*)

val mannwhitneyu : ?use_continuity:bool -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes the Mann-Whitney statistic

Missing values in `x` and/or `y` are discarded.

Parameters
----------
x : sequence
    Input
y : sequence
    Input
use_continuity : {True, False}, optional
    Whether a continuity correction (1/2.) should be taken into account.

Returns
-------
statistic : float
    The Mann-Whitney statistics
pvalue : float
    Approximate p-value assuming a normal distribution.
*)

val median_cihs : ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes the alpha-level confidence interval for the median of the data.

Uses the Hettmasperger-Sheather method.

Parameters
----------
data : array_like
    Input data. Masked values are discarded. The input should be 1D only,
    or `axis` should be set to None.
alpha : float, optional
    Confidence level of the intervals.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.

Returns
-------
median_cihs
    Alpha level confidence interval.
*)

val meppf : ?alpha:float -> ?beta:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns plotting positions (or empirical percentile points) for the data.

Plotting positions are defined as ``(i-alpha)/(n+1-alpha-beta)``, where:
    - i is the rank order statistics
    - n is the number of unmasked values along the given axis
    - `alpha` and `beta` are two parameters.

Typical values for `alpha` and `beta` are:
    - (0,1)    : ``p(k) = k/n``, linear interpolation of cdf (R, type 4)
    - (.5,.5)  : ``p(k) = (k-1/2.)/n``, piecewise linear function
      (R, type 5)
    - (0,0)    : ``p(k) = k/(n+1)``, Weibull (R type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``, in this case,
      ``p(k) = mode[F(x[k])]``. That's R default (R type 7)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``, then
      ``p(k) ~ median[F(x[k])]``.
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x. (R type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``, Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed (R type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM
    - (.3175, .3175): used in scipy.stats.probplot

Parameters
----------
data : array_like
    Input data, as a sequence or array of dimension at most 2.
alpha : float, optional
    Plotting positions parameter. Default is 0.4.
beta : float, optional
    Plotting positions parameter. Default is 0.4.

Returns
-------
positions : MaskedArray
    The calculated plotting positions.
*)

val mjci : ?prob:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns the Maritz-Jarrett estimators of the standard error of selected
experimental quantiles of the data.

Parameters
----------
data : ndarray
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
*)

val mode : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Returns an array of the modal (most common) value in the passed array.

Parameters
----------
a : array_like
    n-dimensional array of which to find mode(s).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
mode : ndarray
    Array of modal values.
count : ndarray
    Array of counts for each mode.

Notes
-----
For more details, see `stats.mode`.

Examples
--------
>>> from scipy import stats
>>> from scipy.stats import mstats
>>> m_arr = np.ma.array([1, 1, 0, 0, 0, 0], mask=[0, 0, 1, 1, 1, 0])
>>> stats.mode(m_arr)
ModeResult(mode=array([0]), count=array([4]))
>>> mstats.mode(m_arr)
ModeResult(mode=array([1.]), count=array([2.]))
*)

val moment : ?moment:int -> ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculates the nth moment about the mean for a sample.

Parameters
----------
a : array_like
   data
moment : int, optional
   order of central moment that is returned
axis : int or None, optional
   Axis along which the central moment is computed. Default is 0.
   If None, compute over the whole array `a`.

Returns
-------
n-th central moment : ndarray or float
   The appropriate moment along the given axis or over all values if axis
   is None. The denominator for the moment calculation is the number of
   observations, no degrees of freedom correction is done.

Notes
-----
For more details about `moment`, see `stats.moment`.
*)

val mquantiles : ?prob:[>`Ndarray] Np.Obj.t -> ?alphap:float -> ?betap:float -> ?axis:int -> ?limit:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes empirical quantiles for a data array.

Samples quantile are defined by ``Q(p) = (1-gamma)*x[j] + gamma*x[j+1]``,
where ``x[j]`` is the j-th order statistic, and gamma is a function of
``j = floor(n*p + m)``, ``m = alphap + p*(1 - alphap - betap)`` and
``g = n*p + m - j``.

Reinterpreting the above equations to compare to **R** lead to the
equation: ``p(k) = (k - alphap)/(n + 1 - alphap - betap)``

Typical values of (alphap,betap) are:
    - (0,1)    : ``p(k) = k/n`` : linear interpolation of cdf
      ( **R** type 4)
    - (.5,.5)  : ``p(k) = (k - 1/2.)/n`` : piecewise linear function
      ( **R** type 5)
    - (0,0)    : ``p(k) = k/(n+1)`` :
      ( **R** type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``: p(k) = mode[F(x[k])].
      ( **R** type 7, **R** default)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``: Then p(k) ~ median[F(x[k])].
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x.
      ( **R** type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``: Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed
      ( **R** type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM

Parameters
----------
a : array_like
    Input data, as a sequence or array of dimension at most 2.
prob : array_like, optional
    List of quantiles to compute.
alphap : float, optional
    Plotting positions parameter, default is 0.4.
betap : float, optional
    Plotting positions parameter, default is 0.4.
axis : int, optional
    Axis along which to perform the trimming.
    If None (default), the input array is first flattened.
limit : tuple, optional
    Tuple of (lower, upper) values.
    Values of `a` outside this open interval are ignored.

Returns
-------
mquantiles : MaskedArray
    An array containing the calculated quantiles.

Notes
-----
This formulation is very similar to **R** except the calculation of
``m`` from ``alphap`` and ``betap``, where in **R** ``m`` is defined
with each type.

References
----------
.. [1] *R* statistical software: https://www.r-project.org/
.. [2] *R* ``quantile`` function:
        http://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html

Examples
--------
>>> from scipy.stats.mstats import mquantiles
>>> a = np.array([6., 47., 49., 15., 42., 41., 7., 39., 43., 40., 36.])
>>> mquantiles(a)
array([ 19.2,  40. ,  42.8])

Using a 2D array, specifying axis and limit.

>>> data = np.array([[   6.,    7.,    1.],
...                  [  47.,   15.,    2.],
...                  [  49.,   36.,    3.],
...                  [  15.,   39.,    4.],
...                  [  42.,   40., -999.],
...                  [  41.,   41., -999.],
...                  [   7., -999., -999.],
...                  [  39., -999., -999.],
...                  [  43., -999., -999.],
...                  [  40., -999., -999.],
...                  [  36., -999., -999.]])
>>> print(mquantiles(data, axis=0, limit=(0, 50)))
[[19.2  14.6   1.45]
 [40.   37.5   2.5 ]
 [42.8  40.05  3.55]]

>>> data[:, 2] = -999.
>>> print(mquantiles(data, axis=0, limit=(0, 50)))
[[19.200000000000003 14.6 --]
 [40.0 37.5 --]
 [42.800000000000004 40.05 --]]
*)

val mquantiles_cimj : ?prob:Py.Object.t -> ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Computes the alpha confidence interval for the selected quantiles of the
data, with Maritz-Jarrett estimators.

Parameters
----------
data : ndarray
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
alpha : float, optional
    Confidence level of the intervals.
axis : int or None, optional
    Axis along which to compute the quantiles.
    If None, use a flattened array.

Returns
-------
ci_lower : ndarray
    The lower boundaries of the confidence interval.  Of the same length as
    `prob`.
ci_upper : ndarray
    The upper boundaries of the confidence interval.  Of the same length as
    `prob`.
*)

val msign : Py.Object.t -> Py.Object.t
(**
Returns the sign of x, or 0 if x is masked.
*)

val normaltest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Tests whether a sample differs from a normal distribution.

Parameters
----------
a : array_like
    The array containing the data to be tested.
axis : int or None, optional
    Axis along which to compute test. Default is 0. If None,
    compute over the whole array `a`.

Returns
-------
statistic : float or array
    ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and
    ``k`` is the z-score returned by `kurtosistest`.
pvalue : float or array
   A 2-sided chi squared probability for the hypothesis test.

Notes
-----
For more details about `normaltest`, see `stats.normaltest`.
*)

val obrientransform : Py.Object.t list -> Py.Object.t
(**
Computes a transform on input data (any number of columns).  Used to
test for homogeneity of variance prior to running one-way stats.  Each
array in ``*args`` is one level of a factor.  If an `f_oneway()` run on
the transformed data and found significant, variances are unequal.   From
Maxwell and Delaney, p.112.

Returns: transformed data for use in an ANOVA
*)

val pearsonr : x:Py.Object.t -> y:Py.Object.t -> unit -> float
(**
Calculates a Pearson correlation coefficient and the p-value for testing
non-correlation.

The Pearson correlation coefficient measures the linear relationship
between two datasets. Strictly speaking, Pearson's correlation requires
that each dataset be normally distributed. Like other correlation
coefficients, this one varies between -1 and +1 with 0 implying no
correlation. Correlations of -1 or +1 imply an exact linear
relationship. Positive correlations imply that as `x` increases, so does
`y`. Negative correlations imply that as `x` increases, `y` decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Pearson correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
x : 1-D array_like
    Input
y : 1-D array_like
    Input

Returns
-------
pearsonr : float
    Pearson's correlation coefficient, 2-tailed p-value.

References
----------
http://www.statsoft.com/textbook/glosp.html#Pearson%20Correlation
*)

val plotting_positions : ?alpha:float -> ?beta:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns plotting positions (or empirical percentile points) for the data.

Plotting positions are defined as ``(i-alpha)/(n+1-alpha-beta)``, where:
    - i is the rank order statistics
    - n is the number of unmasked values along the given axis
    - `alpha` and `beta` are two parameters.

Typical values for `alpha` and `beta` are:
    - (0,1)    : ``p(k) = k/n``, linear interpolation of cdf (R, type 4)
    - (.5,.5)  : ``p(k) = (k-1/2.)/n``, piecewise linear function
      (R, type 5)
    - (0,0)    : ``p(k) = k/(n+1)``, Weibull (R type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``, in this case,
      ``p(k) = mode[F(x[k])]``. That's R default (R type 7)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``, then
      ``p(k) ~ median[F(x[k])]``.
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x. (R type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``, Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed (R type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM
    - (.3175, .3175): used in scipy.stats.probplot

Parameters
----------
data : array_like
    Input data, as a sequence or array of dimension at most 2.
alpha : float, optional
    Plotting positions parameter. Default is 0.4.
beta : float, optional
    Plotting positions parameter. Default is 0.4.

Returns
-------
positions : MaskedArray
    The calculated plotting positions.
*)

val pointbiserialr : x:Py.Object.t -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculates a point biserial correlation coefficient and its p-value.

Parameters
----------
x : array_like of bools
    Input array.
y : array_like
    Input array.

Returns
-------
correlation : float
    R value
pvalue : float
    2-tailed p-value

Notes
-----
Missing values are considered pair-wise: if a value is missing in x,
the corresponding value in y is masked.

For more details on `pointbiserialr`, see `stats.pointbiserialr`.
*)

val rankdata : ?axis:int -> ?use_missing:bool -> data:Py.Object.t -> unit -> Py.Object.t
(**
Returns the rank (also known as order statistics) of each data point
along the given axis.

If some values are tied, their rank is averaged.
If some values are masked, their rank is set to 0 if use_missing is False,
or set to the average rank of the unmasked values if use_missing is True.

Parameters
----------
data : sequence
    Input data. The data is transformed to a masked array
axis : {None,int}, optional
    Axis along which to perform the ranking.
    If None, the array is first flattened. An exception is raised if
    the axis is specified for arrays with a dimension larger than 2
use_missing : bool, optional
    Whether the masked values have a rank of 0 (False) or equal to the
    average rank of the unmasked values (True).
*)

val rsh : ?points:Py.Object.t -> data:Py.Object.t -> unit -> Py.Object.t
(**
Evaluates Rosenblatt's shifted histogram estimators for each data point.

Rosenblatt's estimator is a centered finite-difference approximation to the
derivative of the empirical cumulative distribution function.

Parameters
----------
data : sequence
    Input data, should be 1-D. Masked values are ignored.
points : sequence or None, optional
    Sequence of points where to evaluate Rosenblatt shifted histogram.
    If None, use the data.
*)

val scoreatpercentile : ?limit:Py.Object.t -> ?alphap:Py.Object.t -> ?betap:Py.Object.t -> data:Py.Object.t -> per:Py.Object.t -> unit -> Py.Object.t
(**
Calculate the score at the given 'per' percentile of the
sequence a.  For example, the score at per=50 is the median.

This function is a shortcut to mquantile
*)

val sem : ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculates the standard error of the mean of the input array.

Also sometimes called standard error of measurement.

Parameters
----------
a : array_like
    An array containing the values for which the standard error is
    returned.
axis : int or None, optional
    If axis is None, ravel `a` first. If axis is an integer, this will be
    the axis over which to operate. Defaults to 0.
ddof : int, optional
    Delta degrees-of-freedom. How many degrees of freedom to adjust
    for bias in limited samples relative to the population estimate
    of variance. Defaults to 1.

Returns
-------
s : ndarray or float
    The standard error of the mean in the sample(s), along the input axis.

Notes
-----
The default value for `ddof` changed in scipy 0.15.0 to be consistent with
`stats.sem` as well as with the most common definition used (like in the R
documentation).

Examples
--------
Find standard error along the first axis:

>>> from scipy import stats
>>> a = np.arange(20).reshape(5,4)
>>> print(stats.mstats.sem(a))
[2.8284271247461903 2.8284271247461903 2.8284271247461903
 2.8284271247461903]

Find standard error across the whole array, using n degrees of freedom:

>>> print(stats.mstats.sem(a, axis=None, ddof=0))
1.2893796958227628
*)

val sen_seasonal_slopes : Py.Object.t -> Py.Object.t
(**
None
*)

val siegelslopes : ?x:[>`Ndarray] Np.Obj.t -> ?method_:[`Hierarchical | `Separate] -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Siegel estimator for a set of points (x, y).

`siegelslopes` implements a method for robust linear regression
using repeated medians to fit a line to the points (x, y).
The method is robust to outliers with an asymptotic breakdown point
of 50%.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
method : {'hierarchical', 'separate'}
    If 'hierarchical', estimate the intercept using the estimated
    slope ``medslope`` (default option).
    If 'separate', estimate the intercept independent of the estimated
    slope. See Notes for details.

Returns
-------
medslope : float
    Estimate of the slope of the regression line.
medintercept : float
    Estimate of the intercept of the regression line.

See also
--------
theilslopes : a similar technique without repeated medians

Notes
-----
For more details on `siegelslopes`, see `scipy.stats.siegelslopes`.
*)

val skew : ?axis:[`I of int | `None] -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the skewness of a data set.

Parameters
----------
a : ndarray
    data
axis : int or None, optional
    Axis along which skewness is calculated. Default is 0.
    If None, compute over the whole array `a`.
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.

Returns
-------
skewness : ndarray
    The skewness of values along an axis, returning 0 where all values are
    equal.

Notes
-----
For more details about `skew`, see `stats.skew`.
*)

val skewtest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Tests whether the skew is different from the normal distribution.

Parameters
----------
a : array
    The data to be tested
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    a 2-sided p-value for the hypothesis test

Notes
-----
For more details about `skewtest`, see `stats.skewtest`.
*)

val spearmanr : ?y:Py.Object.t -> ?use_ties:bool -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:Py.Object.t -> unit -> (float * float)
(**
Calculates a Spearman rank-order correlation coefficient and the p-value
to test for non-correlation.

The Spearman correlation is a nonparametric measure of the linear
relationship between two datasets. Unlike the Pearson correlation, the
Spearman correlation does not assume that both datasets are normally
distributed. Like other correlation coefficients, this one varies
between -1 and +1 with 0 implying no correlation. Correlations of -1 or
+1 imply a monotonic relationship. Positive correlations imply that
as `x` increases, so does `y`. Negative correlations imply that as `x`
increases, `y` decreases.

Missing values are discarded pair-wise: if a value is missing in `x`, the
corresponding value in `y` is masked.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Spearman correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
x, y : 1D or 2D array_like, y is optional
    One or two 1-D or 2-D arrays containing multiple variables and
    observations. When these are 1-D, each represents a vector of
    observations of a single variable. For the behavior in the 2-D case,
    see under ``axis``, below.
use_ties : bool, optional
    DO NOT USE.  Does not do anything, keyword is only left in place for
    backwards compatibility reasons.
axis : int or None, optional
    If axis=0 (default), then each column represents a variable, with
    observations in the rows. If axis=1, the relationship is transposed:
    each row represents a variable, while the columns contain observations.
    If axis=None, then both arrays will be raveled.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
correlation : float
    Spearman correlation coefficient
pvalue : float
    2-tailed p-value.

References
----------
[CRCProbStat2000] section 14.7
*)

val theilslopes : ?x:[>`Ndarray] Np.Obj.t -> ?alpha:float -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float * float * float)
(**
Computes the Theil-Sen estimator for a set of points (x, y).

`theilslopes` implements a method for robust linear regression.  It
computes the slope as the median of all slopes between paired values.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
alpha : float, optional
    Confidence degree between 0 and 1. Default is 95% confidence.
    Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are
    interpreted as 'find the 90% confidence interval'.

Returns
-------
medslope : float
    Theil slope.
medintercept : float
    Intercept of the Theil line, as ``median(y) - medslope*median(x)``.
lo_slope : float
    Lower bound of the confidence interval on `medslope`.
up_slope : float
    Upper bound of the confidence interval on `medslope`.

See also
--------
siegelslopes : a similar technique with repeated medians


Notes
-----
For more details on `theilslopes`, see `stats.theilslopes`.
*)

val tmax : ?upperlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed maximum

This function computes the maximum value of an array along a given axis,
while ignoring values larger than a specified upper limit.

Parameters
----------
a : array_like
    array of values
upperlimit : None or float, optional
    Values in the input array greater than the given limit will be ignored.
    When upperlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the upper limit
    are included.  The default value is True.

Returns
-------
tmax : float, int or ndarray

Notes
-----
For more details on `tmax`, see `stats.tmax`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 9, 1, 2],
...               [8, 7, 8, 2],
...               [5, 6, 0, 2],
...               [4, 5, 5, 2]])
...
...
>>> mstats.tmax(a, 4)
masked_array(data=[4, --, 3, 2],
             mask=[False,  True, False, False],
       fill_value=999999)
*)

val tmean : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed mean.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored.  When limits is None (default), then all
    values are used.  Either of the limit values in the tuple can also be
    None representing a half-open interval.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is None.

Returns
-------
tmean : float

Notes
-----
For more details on `tmean`, see `stats.tmean`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 9, 1, 2],
...               [8, 7, 8, 2],
...               [5, 6, 0, 2],
...               [4, 5, 5, 2]])
...
...
>>> mstats.tmean(a, (2,5))
3.3
>>> mstats.tmean(a, (2,5), axis=0)
masked_array(data=[4.0, 5.0, 4.0, 2.0],
             mask=[False, False, False, False],
       fill_value=1e+20)
*)

val tmin : ?lowerlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed minimum

Parameters
----------
a : array_like
    array of values
lowerlimit : None or float, optional
    Values in the input array less than the given limit will be ignored.
    When lowerlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the lower limit
    are included.  The default value is True.

Returns
-------
tmin : float, int or ndarray

Notes
-----
For more details on `tmin`, see `stats.tmin`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 2, 1, 2],
...               [8, 1, 8, 2],
...               [5, 3, 0, 2],
...               [4, 7, 5, 2]])
...
>>> mstats.tmin(a, 5)
masked_array(data=[5, 7, 5, --],
             mask=[False, False, False,  True],
       fill_value=999999)
*)

val trim : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Trims an array by masking the data outside some given limits.

Returns a masked version of the input array.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.


Examples
--------
>>> from scipy.stats.mstats import trim
>>> z = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10]
>>> print(trim(z,(3,8)))
[-- -- 3 4 5 6 7 8 -- --]
>>> print(trim(z,(0.1,0.2),relative=True))
[-- 2 3 4 5 6 7 8 -- --]
*)

val trima : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Trims an array by masking the data outside some given limits.

Returns a masked version of the input array.

Parameters
----------
a : array_like
    Input array.
limits : {None, tuple}, optional
    Tuple of (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit
    will be masked.  A limit is None indicates an open interval.
inclusive : (bool, bool) tuple, optional
    Tuple of (lower flag, upper flag), indicating whether values exactly
    equal to the lower (upper) limit are allowed.

Examples
--------
>>> from scipy.stats.mstats import trima

>>> a = np.arange(10)

The interval is left-closed and right-open, i.e., `[2, 8)`.
Trim the array by keeping only values in the interval.

>>> trima(a, limits=(2, 8), inclusive=(True, False))
masked_array(data=[--, --, 2, 3, 4, 5, 6, 7, --, --],
             mask=[ True,  True, False, False, False, False, False, False,
                    True,  True],
       fill_value=999999)
*)

val trimboth : ?proportiontocut:float -> ?inclusive:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Trims the smallest and largest data values.

Trims the `data` by masking the ``int(proportiontocut * n)`` smallest and
``int(proportiontocut * n)`` largest values of data along the given axis,
where n is the number of unmasked values before trimming.

Parameters
----------
data : ndarray
    Data to trim.
proportiontocut : float, optional
    Percentage of trimming (as a float between 0 and 1).
    If n is the number of unmasked values before trimming, the number of
    values after trimming is ``(1 - 2*proportiontocut) * n``.
    Default is 0.2.
inclusive : {(bool, bool) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).
axis : int, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.
*)

val trimmed_mean : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed mean of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.
*)

val trimmed_mean_ci : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Selected confidence interval of the trimmed mean along the given axis.

Parameters
----------
data : array_like
    Input data.
limits : {None, tuple}, optional
    None or a two item tuple.
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1. If ``n``
    is the number of unmasked data before trimming, then
    (``n * limits[0]``)th smallest data and (``n * limits[1]``)th
    largest data are masked.  The total number of unmasked data after
    trimming is ``n * (1. - sum(limits))``.
    The value of one limit can be set to None to indicate an open interval.

    Defaults to (0.2, 0.2).
inclusive : (2,) tuple of boolean, optional
    If relative==False, tuple indicating whether values exactly equal to
    the absolute limits are allowed.
    If relative==True, tuple indicating whether the number of data being
    masked on each side should be rounded (True) or truncated (False).

    Defaults to (True, True).
alpha : float, optional
    Confidence level of the intervals.

    Defaults to 0.05.
axis : int, optional
    Axis along which to cut. If None, uses a flattened version of `data`.

    Defaults to None.

Returns
-------
trimmed_mean_ci : (2,) ndarray
    The lower and upper confidence intervals of the trimmed data.
*)

val trimmed_std : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> ?ddof:[`Zero | `I of int] -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed standard deviation of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.

ddof : {0,integer}, optional
    Means Delta Degrees of Freedom. The denominator used during computations
    is (n-ddof). DDOF=0 corresponds to a biased estimate, DDOF=1 to an un-
    biased estimate of the variance.
*)

val trimmed_stde : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the standard error of the trimmed mean along the given axis.

Parameters
----------
a : sequence
    Input array
limits : {(0.1,0.1), tuple of float}, optional
    tuple (lower percentage, upper percentage) to cut  on each side of the
    array, with respect to the number of unmasked data.

    If n is the number of unmasked data before trimming, the values
    smaller than ``n * limits[0]`` and the values larger than
    ``n * `limits[1]`` are masked, and the total number of unmasked
    data after trimming is ``n * (1.-sum(limits))``.  In each case,
    the value of one limit can be set to None to indicate an open interval.
    If `limits` is None, no trimming is performed.
inclusive : {(bool, bool) tuple} optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).
axis : int, optional
    Axis along which to trim.

Returns
-------
trimmed_stde : scalar or ndarray
*)

val trimmed_var : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> ?ddof:[`Zero | `I of int] -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed variance of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.

ddof : {0,integer}, optional
    Means Delta Degrees of Freedom. The denominator used during computations
    is (n-ddof). DDOF=0 corresponds to a biased estimate, DDOF=1 to an un-
    biased estimate of the variance.
*)

val trimr : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Trims an array by masking some proportion of the data on each end.
Returns a masked version of the input array.

Parameters
----------
a : sequence
    Input array.
limits : {None, tuple}, optional
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1.
    Noting n the number of unmasked data before trimming, the
    (n*limits[0])th smallest data and the (n*limits[1])th largest data are
    masked, and the total number of unmasked data after trimming is
    n*(1.-sum(limits)).  The value of one limit can be set to None to
    indicate an open interval.
inclusive : {(True,True) tuple}, optional
    Tuple of flags indicating whether the number of data being masked on
    the left (right) end should be truncated (True) or rounded (False) to
    integers.
axis : {None,int}, optional
    Axis along which to trim. If None, the whole array is trimmed, but its
    shape is maintained.
*)

val trimtail : ?proportiontocut:float -> ?tail:[`Left | `Right] -> ?inclusive:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Trims the data by masking values from one tail.

Parameters
----------
data : array_like
    Data to trim.
proportiontocut : float, optional
    Percentage of trimming. If n is the number of unmasked values
    before trimming, the number of values after trimming is
    ``(1 - proportiontocut) * n``.  Default is 0.2.
tail : {'left','right'}, optional
    If 'left' the `proportiontocut` lowest values will be masked.
    If 'right' the `proportiontocut` highest values will be masked.
    Default is 'left'.
inclusive : {(bool, bool) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).  Default is
    (True, True).
axis : int, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.  Default is None.

Returns
-------
trimtail : ndarray
    Returned array of same shape as `data` with masked tail values.
*)

val tsem : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed standard error of the mean.

This function finds the standard error of the mean for given
values, ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    array of values
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is zero.
ddof : int, optional
    Delta degrees of freedom. Default is 1.

Returns
-------
tsem : float

Notes
-----
For more details on `tsem`, see `stats.tsem`.
*)

val ttest_1samp : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the mean of ONE group of scores.

Parameters
----------
a : array_like
    sample observation
popmean : float or array_like
    expected value in null hypothesis, if array_like than it must have the
    same shape as `a` excluding the axis dimension
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_1samp`, see `stats.ttest_1samp`.
*)

val ttest_ind : ?axis:[`I of int | `None] -> ?equal_var:bool -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the means of TWO INDEPENDENT samples of scores.

Parameters
----------
a, b : array_like
    The arrays must have the same shape, except in the dimension
    corresponding to `axis` (the first, by default).
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
equal_var : bool, optional
    If True, perform a standard independent 2 sample test that assumes equal
    population variances.
    If False, perform Welch's t-test, which does not assume equal population
    variance.

    .. versionadded:: 0.17.0

Returns
-------
statistic : float or array
    The calculated t-statistic.
pvalue : float or array
    The two-tailed p-value.

Notes
-----
For more details on `ttest_ind`, see `stats.ttest_ind`.
*)

val ttest_onesamp : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the mean of ONE group of scores.

Parameters
----------
a : array_like
    sample observation
popmean : float or array_like
    expected value in null hypothesis, if array_like than it must have the
    same shape as `a` excluding the axis dimension
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_1samp`, see `stats.ttest_1samp`.
*)

val ttest_rel : ?axis:[`I of int | `None] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test on TWO RELATED samples of scores, a and b.

Parameters
----------
a, b : array_like
    The arrays must have the same shape.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_rel`, see `stats.ttest_rel`.
*)

val tvar : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed variance

This function computes the sample variance of an array of values,
while ignoring values which are outside of given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is zero.
ddof : int, optional
    Delta degrees of freedom. Default is 1.

Returns
-------
tvar : float
    Trimmed variance.

Notes
-----
For more details on `tvar`, see `stats.tvar`.
*)

val variation : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the coefficient of variation, the ratio of the biased standard
deviation to the mean.

Parameters
----------
a : array_like
    Input array.
axis : int or None, optional
    Axis along which to calculate the coefficient of variation. Default
    is 0. If None, compute over the whole array `a`.

Returns
-------
variation : ndarray
    The calculated variation along the requested axis.

Notes
-----
For more details about `variation`, see `stats.variation`.

Examples
--------
>>> from scipy.stats.mstats import variation
>>> a = np.array([2,8,4])
>>> variation(a)
0.5345224838248487
>>> b = np.array([2,8,3,4])
>>> c = np.ma.masked_array(b, mask=[0,0,1,0])
>>> variation(c)
0.5345224838248487

In the example above, it can be seen that this works the same as
`stats.variation` except 'stats.mstats.variation' ignores masked 
array elements.
*)

val winsorize : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?inplace:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns a Winsorized version of the input array.

The (limits[0])th lowest values are set to the (limits[0])th percentile,
and the (limits[1])th highest values are set to the (1 - limits[1])th
percentile.
Masked values are skipped.


Parameters
----------
a : sequence
    Input array.
limits : {None, tuple of float}, optional
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1.
    Noting n the number of unmasked data before trimming, the
    (n*limits[0])th smallest data and the (n*limits[1])th largest data are
    masked, and the total number of unmasked data after trimming
    is n*(1.-sum(limits)) The value of one limit can be set to None to
    indicate an open interval.
inclusive : {(True, True) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be truncated (True) or rounded (False).
inplace : {False, True}, optional
    Whether to winsorize in place (True) or to use a copy (False)
axis : {None, int}, optional
    Axis along which to trim. If None, the whole array is trimmed, but its
    shape is maintained.

Notes
-----
This function is applied to reduce the effect of possibly spurious outliers
by limiting the extreme values.

Examples
--------
>>> from scipy.stats.mstats import winsorize

A shuffled array contains integers from 1 to 10.

>>> a = np.array([10, 4, 9, 8, 5, 3, 7, 2, 1, 6])

The 10% of the lowest value (i.e., `1`) and the 20% of the highest
values (i.e., `9` and `10`) are replaced.

>>> winsorize(a, limits=[0.1, 0.2])
masked_array(data=[8, 4, 8, 8, 5, 3, 7, 2, 2, 6],
             mask=False,
       fill_value=999999)
*)

val zmap : ?axis:[`I of int | `None] -> ?ddof:int -> scores:[>`Ndarray] Np.Obj.t -> compare:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the relative z-scores.

Return an array of z-scores, i.e., scores that are standardized to
zero mean and unit variance, where mean and variance are calculated
from the comparison array.

Parameters
----------
scores : array_like
    The input for which z-scores are calculated.
compare : array_like
    The input from which the mean and standard deviation of the
    normalization are taken; assumed to have the same dimension as
    `scores`.
axis : int or None, optional
    Axis over which mean and variance of `compare` are calculated.
    Default is 0. If None, compute over the whole array `scores`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.

Returns
-------
zscore : array_like
    Z-scores, in the same shape as `scores`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> from scipy.stats import zmap
>>> a = [0.5, 2.0, 2.5, 3]
>>> b = [0, 1, 2, 3, 4]
>>> zmap(a, b)
array([-1.06066017,  0.        ,  0.35355339,  0.70710678])
*)

val zscore : ?axis:[`I of int | `None] -> ?ddof:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the z score.

Compute the z score of each value in the sample, relative to the
sample mean and standard deviation.

Parameters
----------
a : array_like
    An array like object containing the sample data.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
zscore : array_like
    The z-scores, standardized by mean and standard deviation of
    input array `a`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,
...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])
>>> from scipy import stats
>>> stats.zscore(a)
array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,
        0.6748, -1.1488, -1.3324])

Computing along a specified axis, using n-1 degrees of freedom
(``ddof=1``) to calculate the standard deviation:

>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],
...               [ 0.7149,  0.0775,  0.6072,  0.9656],
...               [ 0.6341,  0.1403,  0.9759,  0.4064],
...               [ 0.5918,  0.6948,  0.904 ,  0.3721],
...               [ 0.0921,  0.2481,  0.1188,  0.1366]])
>>> stats.zscore(b, axis=1, ddof=1)
array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],
       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],
       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],
       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],
       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])
*)


end

module Mstats_basic : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module BrunnerMunzelResult : sig
type tag = [`BrunnerMunzelResult]
type t = [`BrunnerMunzelResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
BrunnerMunzelResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module DescribeResult : sig
type tag = [`DescribeResult]
type t = [`DescribeResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : nobs:Py.Object.t -> minmax:Py.Object.t -> mean:Py.Object.t -> variance:Py.Object.t -> skewness:Py.Object.t -> kurtosis:Py.Object.t -> unit -> t
(**
DescribeResult(nobs, minmax, mean, variance, skewness, kurtosis)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module F_onewayResult : sig
type tag = [`F_onewayResult]
type t = [`F_onewayResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
F_onewayResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module FriedmanchisquareResult : sig
type tag = [`FriedmanchisquareResult]
type t = [`FriedmanchisquareResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
FriedmanchisquareResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KendalltauResult : sig
type tag = [`KendalltauResult]
type t = [`KendalltauResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KendalltauResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KruskalResult : sig
type tag = [`KruskalResult]
type t = [`KruskalResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KruskalResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KurtosistestResult : sig
type tag = [`KurtosistestResult]
type t = [`KurtosistestResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KurtosistestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module LinregressResult : sig
type tag = [`LinregressResult]
type t = [`LinregressResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : slope:Py.Object.t -> intercept:Py.Object.t -> rvalue:Py.Object.t -> pvalue:Py.Object.t -> stderr:Py.Object.t -> unit -> t
(**
LinregressResult(slope, intercept, rvalue, pvalue, stderr)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module MannwhitneyuResult : sig
type tag = [`MannwhitneyuResult]
type t = [`MannwhitneyuResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
MannwhitneyuResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module ModeResult : sig
type tag = [`ModeResult]
type t = [`ModeResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : mode:Py.Object.t -> count:Py.Object.t -> unit -> t
(**
ModeResult(mode, count)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module NormaltestResult : sig
type tag = [`NormaltestResult]
type t = [`NormaltestResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
NormaltestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module PointbiserialrResult : sig
type tag = [`PointbiserialrResult]
type t = [`Object | `PointbiserialrResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
PointbiserialrResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module SkewtestResult : sig
type tag = [`SkewtestResult]
type t = [`Object | `SkewtestResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
SkewtestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module SpearmanrResult : sig
type tag = [`SpearmanrResult]
type t = [`Object | `SpearmanrResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
SpearmanrResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_1sampResult : sig
type tag = [`Ttest_1sampResult]
type t = [`Object | `Ttest_1sampResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_1sampResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_indResult : sig
type tag = [`Ttest_indResult]
type t = [`Object | `Ttest_indResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_indResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_relResult : sig
type tag = [`Ttest_relResult]
type t = [`Object | `Ttest_relResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_relResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

val argstoarray : Py.Object.t list -> Py.Object.t
(**
Constructs a 2D array from a group of sequences.

Sequences are filled with missing values to match the length of the longest
sequence.

Parameters
----------
args : sequences
    Group of sequences.

Returns
-------
argstoarray : MaskedArray
    A ( `m` x `n` ) masked array, where `m` is the number of arguments and
    `n` the length of the longest argument.

Notes
-----
`numpy.ma.row_stack` has identical behavior, but is called with a sequence
of sequences.
*)

val brunnermunzel : ?alternative:[`Less | `Two_sided | `Greater] -> ?distribution:[`T | `Normal] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes the Brunner-Munzel test on samples x and y

Missing values in `x` and/or `y` are discarded.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
alternative :  'less', 'two-sided', or 'greater', optional
    Whether to get the p-value for the one-sided hypothesis ('less'
    or 'greater') or for the two-sided hypothesis ('two-sided').
    Defaults value is 'two-sided' .
distribution: 't' or 'normal', optional
    Whether to get the p-value by t-distribution or by standard normal
    distribution.
    Defaults value is 't' .

Returns
-------
statistic : float
    The Brunner-Munzer W statistic.
pvalue : float
    p-value assuming an t distribution. One-sided or
    two-sided, depending on the choice of `alternative` and `distribution`.

See Also
--------
mannwhitneyu : Mann-Whitney rank test on two samples.

Notes
-------
For more details on `brunnermunzel`, see `stats.brunnermunzel`.
*)

val count_tied_groups : ?use_missing:bool -> x:Py.Object.t -> unit -> Py.Object.t
(**
Counts the number of tied values.

Parameters
----------
x : sequence
    Sequence of data on which to counts the ties
use_missing : bool, optional
    Whether to consider missing values as tied.

Returns
-------
count_tied_groups : dict
    Returns a dictionary (nb of ties: nb of groups).

Examples
--------
>>> from scipy.stats import mstats
>>> z = [0, 0, 0, 2, 2, 2, 3, 3, 4, 5, 6]
>>> mstats.count_tied_groups(z)
{2: 1, 3: 2}

In the above example, the ties were 0 (3x), 2 (3x) and 3 (2x).

>>> z = np.ma.array([0, 0, 1, 2, 2, 2, 3, 3, 4, 5, 6])
>>> mstats.count_tied_groups(z)
{2: 2, 3: 1}
>>> z[[1,-1]] = np.ma.masked
>>> mstats.count_tied_groups(z, use_missing=True)
{2: 2, 3: 1}
*)

val describe : ?axis:[`I of int | `None] -> ?ddof:int -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> (int * (int * int) * float * float * float * float)
(**
Computes several descriptive statistics of the passed array.

Parameters
----------
a : array_like
    Data array
axis : int or None, optional
    Axis along which to calculate statistics. Default 0. If None,
    compute over the whole array `a`.
ddof : int, optional
    degree of freedom (default 0); note that default ddof is different
    from the same routine in stats.describe
bias : bool, optional
    If False, then the skewness and kurtosis calculations are corrected for
    statistical bias.

Returns
-------
nobs : int
    (size of the data (discarding missing values)

minmax : (int, int)
    min, max

mean : float
    arithmetic mean

variance : float
    unbiased variance

skewness : float
    biased skewness

kurtosis : float
    biased kurtosis

Examples
--------
>>> from scipy.stats.mstats import describe
>>> ma = np.ma.array(range(6), mask=[0, 0, 0, 1, 1, 1])
>>> describe(ma)
DescribeResult(nobs=3, minmax=(masked_array(data=0,
             mask=False,
       fill_value=999999), masked_array(data=2,
             mask=False,
       fill_value=999999)), mean=1.0, variance=0.6666666666666666,
       skewness=masked_array(data=0., mask=False, fill_value=1e+20),
        kurtosis=-1.5)
*)

val f_oneway : Py.Object.t list -> (float * float)
(**
Performs a 1-way ANOVA, returning an F-value and probability given
any number of groups.  From Heiman, pp.394-7.

Usage: ``f_oneway( *args)``, where ``*args`` is 2 or more arrays,
one per treatment group.

Returns
-------
statistic : float
    The computed F-value of the test.
pvalue : float
    The associated p-value from the F-distribution.
*)

val find_repeats : Py.Object.t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find repeats in arr and return a tuple (repeats, repeat_count).

The input is cast to float64. Masked values are discarded.

Parameters
----------
arr : sequence
    Input array. The array is flattened if it is not 1D.

Returns
-------
repeats : ndarray
    Array of repeated values.
counts : ndarray
    Array of counts.
*)

val friedmanchisquare : Py.Object.t list -> (float * float)
(**
Friedman Chi-Square is a non-parametric, one-way within-subjects ANOVA.
This function calculates the Friedman Chi-square test for repeated measures
and returns the result, along with the associated probability value.

Each input is considered a given group. Ideally, the number of treatments
among each group should be equal. If this is not the case, only the first
n treatments are taken into account, where n is the number of treatments
of the smallest group.
If a group has some missing values, the corresponding treatments are masked
in the other groups.
The test statistic is corrected for ties.

Masked values in one group are propagated to the other groups.

Returns
-------
statistic : float
    the test statistic.
pvalue : float
    the associated p-value.
*)

val iteritems : Py.Object.t -> Py.Object.t
(**
Return an iterator over the (key, value) pairs of a dictionary.
*)

val kendalltau : ?use_ties:bool -> ?use_missing:bool -> ?method_:[`Auto | `Asymptotic | `Exact] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes Kendall's rank correlation tau on two variables *x* and *y*.

Parameters
----------
x : sequence
    First data list (for example, time).
y : sequence
    Second data list.
use_ties : {True, False}, optional
    Whether ties correction should be performed.
use_missing : {False, True}, optional
    Whether missing data should be allocated a rank of 0 (False) or the
    average rank (True)
method: {'auto', 'asymptotic', 'exact'}, optional
    Defines which method is used to calculate the p-value [1]_.
    'asymptotic' uses a normal approximation valid for large samples.
    'exact' computes the exact p-value, but can only be used if no ties
    are present. 'auto' is the default and selects the appropriate
    method based on a trade-off between speed and accuracy.

Returns
-------
correlation : float
    Kendall tau
pvalue : float
    Approximate 2-side p-value.

References
----------
.. [1] Maurice G. Kendall, 'Rank Correlation Methods' (4th Edition),
       Charles Griffin & Co., 1970.
*)

val kendalltau_seasonal : Py.Object.t -> Py.Object.t
(**
Computes a multivariate Kendall's rank correlation tau, for seasonal data.

Parameters
----------
x : 2-D ndarray
    Array of seasonal data, with seasons in columns.
*)

val kruskal : Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution

Notes
-----
For more details on `kruskal`, see `stats.kruskal`.

Examples
--------
>>> from scipy.stats.mstats import kruskal

Random samples from three different brands of batteries were tested
to see how long the charge lasted. Results were as follows:

>>> a = [6.3, 5.4, 5.7, 5.2, 5.0]
>>> b = [6.9, 7.0, 6.1, 7.9]
>>> c = [7.2, 6.9, 6.1, 6.5]

Test the hypotesis that the distribution functions for all of the brands'
durations are identical. Use 5% level of significance.

>>> kruskal(a, b, c)
KruskalResult(statistic=7.113812154696133, pvalue=0.028526948491942164)

The null hypothesis is rejected at the 5% level of significance
because the returned p-value is less than the critical value of 5%.
*)

val kruskalwallis : Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution

Notes
-----
For more details on `kruskal`, see `stats.kruskal`.

Examples
--------
>>> from scipy.stats.mstats import kruskal

Random samples from three different brands of batteries were tested
to see how long the charge lasted. Results were as follows:

>>> a = [6.3, 5.4, 5.7, 5.2, 5.0]
>>> b = [6.9, 7.0, 6.1, 7.9]
>>> c = [7.2, 6.9, 6.1, 6.5]

Test the hypotesis that the distribution functions for all of the brands'
durations are identical. Use 5% level of significance.

>>> kruskal(a, b, c)
KruskalResult(statistic=7.113812154696133, pvalue=0.028526948491942164)

The null hypothesis is rejected at the 5% level of significance
because the returned p-value is less than the critical value of 5%.
*)

val ks_2samp : ?alternative:[`Two_sided | `Less | `Greater] -> data1:[>`Ndarray] Np.Obj.t -> data2:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Kolmogorov-Smirnov test on two samples.

Missing values are discarded.

Parameters
----------
data1 : array_like
    First data set
data2 : array_like
    Second data set
alternative : {'two-sided', 'less', 'greater'}, optional
    Indicates the alternative hypothesis.  Default is 'two-sided'.

Returns
-------
d : float
    Value of the Kolmogorov Smirnov test
p : float
    Corresponding p-value.
*)

val ks_twosamp : ?alternative:[`Two_sided | `Less | `Greater] -> data1:[>`Ndarray] Np.Obj.t -> data2:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Kolmogorov-Smirnov test on two samples.

Missing values are discarded.

Parameters
----------
data1 : array_like
    First data set
data2 : array_like
    Second data set
alternative : {'two-sided', 'less', 'greater'}, optional
    Indicates the alternative hypothesis.  Default is 'two-sided'.

Returns
-------
d : float
    Value of the Kolmogorov Smirnov test
p : float
    Corresponding p-value.
*)

val kurtosis : ?axis:[`I of int | `None] -> ?fisher:bool -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the kurtosis (Fisher or Pearson) of a dataset.

Kurtosis is the fourth central moment divided by the square of the
variance. If Fisher's definition is used, then 3.0 is subtracted from
the result to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics to
eliminate bias coming from biased moment estimators

Use `kurtosistest` to see if result is close enough to normal.

Parameters
----------
a : array
    data for which the kurtosis is calculated
axis : int or None, optional
    Axis along which the kurtosis is calculated. Default is 0.
    If None, compute over the whole array `a`.
fisher : bool, optional
    If True, Fisher's definition is used (normal ==> 0.0). If False,
    Pearson's definition is used (normal ==> 3.0).
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.

Returns
-------
kurtosis : array
    The kurtosis of values along an axis. If all values are equal,
    return -3 for Fisher's definition and 0 for Pearson's definition.

Notes
-----
For more details about `kurtosis`, see `stats.kurtosis`.
*)

val kurtosistest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Tests whether a dataset has normal kurtosis

Parameters
----------
a : array
    array of the sample data
axis : int or None, optional
   Axis along which to compute test. Default is 0. If None,
   compute over the whole array `a`.

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    The 2-sided p-value for the hypothesis test

Notes
-----
For more details about `kurtosistest`, see `stats.kurtosistest`.
*)

val linregress : ?y:Py.Object.t -> x:Py.Object.t -> unit -> Py.Object.t
(**
Linear regression calculation

Note that the non-masked version is used, and that this docstring is
replaced by the non-masked docstring + some info on missing data.
*)

val mannwhitneyu : ?use_continuity:bool -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Computes the Mann-Whitney statistic

Missing values in `x` and/or `y` are discarded.

Parameters
----------
x : sequence
    Input
y : sequence
    Input
use_continuity : {True, False}, optional
    Whether a continuity correction (1/2.) should be taken into account.

Returns
-------
statistic : float
    The Mann-Whitney statistics
pvalue : float
    Approximate p-value assuming a normal distribution.
*)

val meppf : ?alpha:float -> ?beta:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns plotting positions (or empirical percentile points) for the data.

Plotting positions are defined as ``(i-alpha)/(n+1-alpha-beta)``, where:
    - i is the rank order statistics
    - n is the number of unmasked values along the given axis
    - `alpha` and `beta` are two parameters.

Typical values for `alpha` and `beta` are:
    - (0,1)    : ``p(k) = k/n``, linear interpolation of cdf (R, type 4)
    - (.5,.5)  : ``p(k) = (k-1/2.)/n``, piecewise linear function
      (R, type 5)
    - (0,0)    : ``p(k) = k/(n+1)``, Weibull (R type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``, in this case,
      ``p(k) = mode[F(x[k])]``. That's R default (R type 7)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``, then
      ``p(k) ~ median[F(x[k])]``.
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x. (R type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``, Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed (R type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM
    - (.3175, .3175): used in scipy.stats.probplot

Parameters
----------
data : array_like
    Input data, as a sequence or array of dimension at most 2.
alpha : float, optional
    Plotting positions parameter. Default is 0.4.
beta : float, optional
    Plotting positions parameter. Default is 0.4.

Returns
-------
positions : MaskedArray
    The calculated plotting positions.
*)

val mode : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Returns an array of the modal (most common) value in the passed array.

Parameters
----------
a : array_like
    n-dimensional array of which to find mode(s).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
mode : ndarray
    Array of modal values.
count : ndarray
    Array of counts for each mode.

Notes
-----
For more details, see `stats.mode`.

Examples
--------
>>> from scipy import stats
>>> from scipy.stats import mstats
>>> m_arr = np.ma.array([1, 1, 0, 0, 0, 0], mask=[0, 0, 1, 1, 1, 0])
>>> stats.mode(m_arr)
ModeResult(mode=array([0]), count=array([4]))
>>> mstats.mode(m_arr)
ModeResult(mode=array([1.]), count=array([2.]))
*)

val moment : ?moment:int -> ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculates the nth moment about the mean for a sample.

Parameters
----------
a : array_like
   data
moment : int, optional
   order of central moment that is returned
axis : int or None, optional
   Axis along which the central moment is computed. Default is 0.
   If None, compute over the whole array `a`.

Returns
-------
n-th central moment : ndarray or float
   The appropriate moment along the given axis or over all values if axis
   is None. The denominator for the moment calculation is the number of
   observations, no degrees of freedom correction is done.

Notes
-----
For more details about `moment`, see `stats.moment`.
*)

val mquantiles : ?prob:[>`Ndarray] Np.Obj.t -> ?alphap:float -> ?betap:float -> ?axis:int -> ?limit:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes empirical quantiles for a data array.

Samples quantile are defined by ``Q(p) = (1-gamma)*x[j] + gamma*x[j+1]``,
where ``x[j]`` is the j-th order statistic, and gamma is a function of
``j = floor(n*p + m)``, ``m = alphap + p*(1 - alphap - betap)`` and
``g = n*p + m - j``.

Reinterpreting the above equations to compare to **R** lead to the
equation: ``p(k) = (k - alphap)/(n + 1 - alphap - betap)``

Typical values of (alphap,betap) are:
    - (0,1)    : ``p(k) = k/n`` : linear interpolation of cdf
      ( **R** type 4)
    - (.5,.5)  : ``p(k) = (k - 1/2.)/n`` : piecewise linear function
      ( **R** type 5)
    - (0,0)    : ``p(k) = k/(n+1)`` :
      ( **R** type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``: p(k) = mode[F(x[k])].
      ( **R** type 7, **R** default)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``: Then p(k) ~ median[F(x[k])].
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x.
      ( **R** type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``: Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed
      ( **R** type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM

Parameters
----------
a : array_like
    Input data, as a sequence or array of dimension at most 2.
prob : array_like, optional
    List of quantiles to compute.
alphap : float, optional
    Plotting positions parameter, default is 0.4.
betap : float, optional
    Plotting positions parameter, default is 0.4.
axis : int, optional
    Axis along which to perform the trimming.
    If None (default), the input array is first flattened.
limit : tuple, optional
    Tuple of (lower, upper) values.
    Values of `a` outside this open interval are ignored.

Returns
-------
mquantiles : MaskedArray
    An array containing the calculated quantiles.

Notes
-----
This formulation is very similar to **R** except the calculation of
``m`` from ``alphap`` and ``betap``, where in **R** ``m`` is defined
with each type.

References
----------
.. [1] *R* statistical software: https://www.r-project.org/
.. [2] *R* ``quantile`` function:
        http://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html

Examples
--------
>>> from scipy.stats.mstats import mquantiles
>>> a = np.array([6., 47., 49., 15., 42., 41., 7., 39., 43., 40., 36.])
>>> mquantiles(a)
array([ 19.2,  40. ,  42.8])

Using a 2D array, specifying axis and limit.

>>> data = np.array([[   6.,    7.,    1.],
...                  [  47.,   15.,    2.],
...                  [  49.,   36.,    3.],
...                  [  15.,   39.,    4.],
...                  [  42.,   40., -999.],
...                  [  41.,   41., -999.],
...                  [   7., -999., -999.],
...                  [  39., -999., -999.],
...                  [  43., -999., -999.],
...                  [  40., -999., -999.],
...                  [  36., -999., -999.]])
>>> print(mquantiles(data, axis=0, limit=(0, 50)))
[[19.2  14.6   1.45]
 [40.   37.5   2.5 ]
 [42.8  40.05  3.55]]

>>> data[:, 2] = -999.
>>> print(mquantiles(data, axis=0, limit=(0, 50)))
[[19.200000000000003 14.6 --]
 [40.0 37.5 --]
 [42.800000000000004 40.05 --]]
*)

val msign : Py.Object.t -> Py.Object.t
(**
Returns the sign of x, or 0 if x is masked.
*)

val namedtuple : ?rename:Py.Object.t -> ?defaults:Py.Object.t -> ?module_:Py.Object.t -> typename:Py.Object.t -> field_names:Py.Object.t -> unit -> Py.Object.t
(**
Returns a new subclass of tuple with named fields.

>>> Point = namedtuple('Point', ['x', 'y'])
>>> Point.__doc__                   # docstring for the new class
'Point(x, y)'
>>> p = Point(11, y=22)             # instantiate with positional args or keywords
>>> p[0] + p[1]                     # indexable like a plain tuple
33
>>> x, y = p                        # unpack like a regular tuple
>>> x, y
(11, 22)
>>> p.x + p.y                       # fields also accessible by name
33
>>> d = p._asdict()                 # convert to a dictionary
>>> d['x']
11
>>> Point( **d)                      # convert from a dictionary
Point(x=11, y=22)
>>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields
Point(x=100, y=22)
*)

val normaltest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Tests whether a sample differs from a normal distribution.

Parameters
----------
a : array_like
    The array containing the data to be tested.
axis : int or None, optional
    Axis along which to compute test. Default is 0. If None,
    compute over the whole array `a`.

Returns
-------
statistic : float or array
    ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and
    ``k`` is the z-score returned by `kurtosistest`.
pvalue : float or array
   A 2-sided chi squared probability for the hypothesis test.

Notes
-----
For more details about `normaltest`, see `stats.normaltest`.
*)

val obrientransform : Py.Object.t list -> Py.Object.t
(**
Computes a transform on input data (any number of columns).  Used to
test for homogeneity of variance prior to running one-way stats.  Each
array in ``*args`` is one level of a factor.  If an `f_oneway()` run on
the transformed data and found significant, variances are unequal.   From
Maxwell and Delaney, p.112.

Returns: transformed data for use in an ANOVA
*)

val pearsonr : x:Py.Object.t -> y:Py.Object.t -> unit -> float
(**
Calculates a Pearson correlation coefficient and the p-value for testing
non-correlation.

The Pearson correlation coefficient measures the linear relationship
between two datasets. Strictly speaking, Pearson's correlation requires
that each dataset be normally distributed. Like other correlation
coefficients, this one varies between -1 and +1 with 0 implying no
correlation. Correlations of -1 or +1 imply an exact linear
relationship. Positive correlations imply that as `x` increases, so does
`y`. Negative correlations imply that as `x` increases, `y` decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Pearson correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
x : 1-D array_like
    Input
y : 1-D array_like
    Input

Returns
-------
pearsonr : float
    Pearson's correlation coefficient, 2-tailed p-value.

References
----------
http://www.statsoft.com/textbook/glosp.html#Pearson%20Correlation
*)

val plotting_positions : ?alpha:float -> ?beta:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns plotting positions (or empirical percentile points) for the data.

Plotting positions are defined as ``(i-alpha)/(n+1-alpha-beta)``, where:
    - i is the rank order statistics
    - n is the number of unmasked values along the given axis
    - `alpha` and `beta` are two parameters.

Typical values for `alpha` and `beta` are:
    - (0,1)    : ``p(k) = k/n``, linear interpolation of cdf (R, type 4)
    - (.5,.5)  : ``p(k) = (k-1/2.)/n``, piecewise linear function
      (R, type 5)
    - (0,0)    : ``p(k) = k/(n+1)``, Weibull (R type 6)
    - (1,1)    : ``p(k) = (k-1)/(n-1)``, in this case,
      ``p(k) = mode[F(x[k])]``. That's R default (R type 7)
    - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``, then
      ``p(k) ~ median[F(x[k])]``.
      The resulting quantile estimates are approximately median-unbiased
      regardless of the distribution of x. (R type 8)
    - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``, Blom.
      The resulting quantile estimates are approximately unbiased
      if x is normally distributed (R type 9)
    - (.4,.4)  : approximately quantile unbiased (Cunnane)
    - (.35,.35): APL, used with PWM
    - (.3175, .3175): used in scipy.stats.probplot

Parameters
----------
data : array_like
    Input data, as a sequence or array of dimension at most 2.
alpha : float, optional
    Plotting positions parameter. Default is 0.4.
beta : float, optional
    Plotting positions parameter. Default is 0.4.

Returns
-------
positions : MaskedArray
    The calculated plotting positions.
*)

val pointbiserialr : x:Py.Object.t -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculates a point biserial correlation coefficient and its p-value.

Parameters
----------
x : array_like of bools
    Input array.
y : array_like
    Input array.

Returns
-------
correlation : float
    R value
pvalue : float
    2-tailed p-value

Notes
-----
Missing values are considered pair-wise: if a value is missing in x,
the corresponding value in y is masked.

For more details on `pointbiserialr`, see `stats.pointbiserialr`.
*)

val rankdata : ?axis:int -> ?use_missing:bool -> data:Py.Object.t -> unit -> Py.Object.t
(**
Returns the rank (also known as order statistics) of each data point
along the given axis.

If some values are tied, their rank is averaged.
If some values are masked, their rank is set to 0 if use_missing is False,
or set to the average rank of the unmasked values if use_missing is True.

Parameters
----------
data : sequence
    Input data. The data is transformed to a masked array
axis : {None,int}, optional
    Axis along which to perform the ranking.
    If None, the array is first flattened. An exception is raised if
    the axis is specified for arrays with a dimension larger than 2
use_missing : bool, optional
    Whether the masked values have a rank of 0 (False) or equal to the
    average rank of the unmasked values (True).
*)

val scoreatpercentile : ?limit:Py.Object.t -> ?alphap:Py.Object.t -> ?betap:Py.Object.t -> data:Py.Object.t -> per:Py.Object.t -> unit -> Py.Object.t
(**
Calculate the score at the given 'per' percentile of the
sequence a.  For example, the score at per=50 is the median.

This function is a shortcut to mquantile
*)

val sem : ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculates the standard error of the mean of the input array.

Also sometimes called standard error of measurement.

Parameters
----------
a : array_like
    An array containing the values for which the standard error is
    returned.
axis : int or None, optional
    If axis is None, ravel `a` first. If axis is an integer, this will be
    the axis over which to operate. Defaults to 0.
ddof : int, optional
    Delta degrees-of-freedom. How many degrees of freedom to adjust
    for bias in limited samples relative to the population estimate
    of variance. Defaults to 1.

Returns
-------
s : ndarray or float
    The standard error of the mean in the sample(s), along the input axis.

Notes
-----
The default value for `ddof` changed in scipy 0.15.0 to be consistent with
`stats.sem` as well as with the most common definition used (like in the R
documentation).

Examples
--------
Find standard error along the first axis:

>>> from scipy import stats
>>> a = np.arange(20).reshape(5,4)
>>> print(stats.mstats.sem(a))
[2.8284271247461903 2.8284271247461903 2.8284271247461903
 2.8284271247461903]

Find standard error across the whole array, using n degrees of freedom:

>>> print(stats.mstats.sem(a, axis=None, ddof=0))
1.2893796958227628
*)

val sen_seasonal_slopes : Py.Object.t -> Py.Object.t
(**
None
*)

val siegelslopes : ?x:[>`Ndarray] Np.Obj.t -> ?method_:[`Hierarchical | `Separate] -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Siegel estimator for a set of points (x, y).

`siegelslopes` implements a method for robust linear regression
using repeated medians to fit a line to the points (x, y).
The method is robust to outliers with an asymptotic breakdown point
of 50%.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
method : {'hierarchical', 'separate'}
    If 'hierarchical', estimate the intercept using the estimated
    slope ``medslope`` (default option).
    If 'separate', estimate the intercept independent of the estimated
    slope. See Notes for details.

Returns
-------
medslope : float
    Estimate of the slope of the regression line.
medintercept : float
    Estimate of the intercept of the regression line.

See also
--------
theilslopes : a similar technique without repeated medians

Notes
-----
For more details on `siegelslopes`, see `scipy.stats.siegelslopes`.
*)

val skew : ?axis:[`I of int | `None] -> ?bias:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the skewness of a data set.

Parameters
----------
a : ndarray
    data
axis : int or None, optional
    Axis along which skewness is calculated. Default is 0.
    If None, compute over the whole array `a`.
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.

Returns
-------
skewness : ndarray
    The skewness of values along an axis, returning 0 where all values are
    equal.

Notes
-----
For more details about `skew`, see `stats.skew`.
*)

val skewtest : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Tests whether the skew is different from the normal distribution.

Parameters
----------
a : array
    The data to be tested
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    a 2-sided p-value for the hypothesis test

Notes
-----
For more details about `skewtest`, see `stats.skewtest`.
*)

val spearmanr : ?y:Py.Object.t -> ?use_ties:bool -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:Py.Object.t -> unit -> (float * float)
(**
Calculates a Spearman rank-order correlation coefficient and the p-value
to test for non-correlation.

The Spearman correlation is a nonparametric measure of the linear
relationship between two datasets. Unlike the Pearson correlation, the
Spearman correlation does not assume that both datasets are normally
distributed. Like other correlation coefficients, this one varies
between -1 and +1 with 0 implying no correlation. Correlations of -1 or
+1 imply a monotonic relationship. Positive correlations imply that
as `x` increases, so does `y`. Negative correlations imply that as `x`
increases, `y` decreases.

Missing values are discarded pair-wise: if a value is missing in `x`, the
corresponding value in `y` is masked.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Spearman correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
x, y : 1D or 2D array_like, y is optional
    One or two 1-D or 2-D arrays containing multiple variables and
    observations. When these are 1-D, each represents a vector of
    observations of a single variable. For the behavior in the 2-D case,
    see under ``axis``, below.
use_ties : bool, optional
    DO NOT USE.  Does not do anything, keyword is only left in place for
    backwards compatibility reasons.
axis : int or None, optional
    If axis=0 (default), then each column represents a variable, with
    observations in the rows. If axis=1, the relationship is transposed:
    each row represents a variable, while the columns contain observations.
    If axis=None, then both arrays will be raveled.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
correlation : float
    Spearman correlation coefficient
pvalue : float
    2-tailed p-value.

References
----------
[CRCProbStat2000] section 14.7
*)

val stats_linregress : ?y:Py.Object.t -> x:Py.Object.t -> unit -> (float * float * float * float * float)
(**
Calculate a linear least-squares regression for two sets of measurements.

Parameters
----------
x, y : array_like
    Two sets of measurements.  Both arrays should have the same length.  If
    only `x` is given (and ``y=None``), then it must be a two-dimensional
    array where one dimension has length 2.  The two sets of measurements
    are then found by splitting the array along the length-2 dimension.  In
    the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is
    equivalent to ``linregress(x[0], x[1])``.

Returns
-------
slope : float
    Slope of the regression line.
intercept : float
    Intercept of the regression line.
rvalue : float
    Correlation coefficient.
pvalue : float
    Two-sided p-value for a hypothesis test whose null hypothesis is
    that the slope is zero, using Wald Test with t-distribution of
    the test statistic.
stderr : float
    Standard error of the estimated gradient.

See also
--------
:func:`scipy.optimize.curve_fit` : Use non-linear
 least squares to fit a function to data.
:func:`scipy.optimize.leastsq` : Minimize the sum of
 squares of a set of equations.

Notes
-----
Missing values are considered pair-wise: if a value is missing in `x`,
the corresponding value in `y` is masked.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats

Generate some data:

>>> np.random.seed(12345678)
>>> x = np.random.random(10)
>>> y = 1.6*x + np.random.random(10)

Perform the linear regression:

>>> slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
>>> print('slope: %f    intercept: %f' % (slope, intercept))
slope: 1.944864    intercept: 0.268578

To get coefficient of determination (R-squared):

>>> print('R-squared: %f' % r_value**2)
R-squared: 0.735498

Plot the data along with the fitted line:

>>> plt.plot(x, y, 'o', label='original data')
>>> plt.plot(x, intercept + slope*x, 'r', label='fitted line')
>>> plt.legend()
>>> plt.show()

Example for the case where only x is provided as a 2x2 array:

>>> x = np.array([[0, 1], [0, 2]])
>>> r = stats.linregress(x)
>>> r.slope, r.intercept
(2.0, 0.0)
*)

val stats_siegelslopes : ?x:[>`Ndarray] Np.Obj.t -> ?method_:[`Hierarchical | `Separate] -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Siegel estimator for a set of points (x, y).

`siegelslopes` implements a method for robust linear regression
using repeated medians (see [1]_) to fit a line to the points (x, y).
The method is robust to outliers with an asymptotic breakdown point
of 50%.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
method : {'hierarchical', 'separate'}
    If 'hierarchical', estimate the intercept using the estimated
    slope ``medslope`` (default option).
    If 'separate', estimate the intercept independent of the estimated
    slope. See Notes for details.

Returns
-------
medslope : float
    Estimate of the slope of the regression line.
medintercept : float
    Estimate of the intercept of the regression line.

See also
--------
theilslopes : a similar technique without repeated medians

Notes
-----
With ``n = len(y)``, compute ``m_j`` as the median of
the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.
``medslope`` is then the median of all slopes ``m_j``.
Two ways are given to estimate the intercept in [1]_ which can be chosen
via the parameter ``method``.
The hierarchical approach uses the estimated slope ``medslope``
and computes ``medintercept`` as the median of ``y - medslope*x``.
The other approach estimates the intercept separately as follows: for
each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`
lines through the remaining points and take the median ``i_j``.
``medintercept`` is the median of the ``i_j``.

The implementation computes `n` times the median of a vector of size `n`
which can be slow for large vectors. There are more efficient algorithms
(see [2]_) which are not implemented here.

References
----------
.. [1] A. Siegel, 'Robust Regression Using Repeated Medians',
       Biometrika, Vol. 69, pp. 242-244, 1982.

.. [2] A. Stein and M. Werman, 'Finding the repeated median regression
       line', Proceedings of the Third Annual ACM-SIAM Symposium on
       Discrete Algorithms, pp. 409-413, 1992.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope and intercept.  For comparison, also compute the
least-squares fit with `linregress`:

>>> res = stats.siegelslopes(y, x)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Siegel regression line is shown in red. The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val stats_theilslopes : ?x:[>`Ndarray] Np.Obj.t -> ?alpha:float -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float * float * float)
(**
Computes the Theil-Sen estimator for a set of points (x, y).

`theilslopes` implements a method for robust linear regression.  It
computes the slope as the median of all slopes between paired values.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
alpha : float, optional
    Confidence degree between 0 and 1. Default is 95% confidence.
    Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are
    interpreted as 'find the 90% confidence interval'.

Returns
-------
medslope : float
    Theil slope.
medintercept : float
    Intercept of the Theil line, as ``median(y) - medslope*median(x)``.
lo_slope : float
    Lower bound of the confidence interval on `medslope`.
up_slope : float
    Upper bound of the confidence interval on `medslope`.

See also
--------
siegelslopes : a similar technique using repeated medians

Notes
-----
The implementation of `theilslopes` follows [1]_. The intercept is
not defined in [1]_, and here it is defined as ``median(y) -
medslope*median(x)``, which is given in [3]_. Other definitions of
the intercept exist in the literature. A confidence interval for
the intercept is not given as this question is not addressed in
[1]_.

References
----------
.. [1] P.K. Sen, 'Estimates of the regression coefficient based on Kendall's tau',
       J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.
.. [2] H. Theil, 'A rank-invariant method of linear and polynomial
       regression analysis I, II and III',  Nederl. Akad. Wetensch., Proc.
       53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.
.. [3] W.L. Conover, 'Practical nonparametric statistics', 2nd ed.,
       John Wiley and Sons, New York, pp. 493.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope, intercept and 90% confidence interval.  For comparison,
also compute the least-squares fit with `linregress`:

>>> res = stats.theilslopes(y, x, 0.90)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Theil-Sen regression line is shown in red, with the
dashed red lines illustrating the confidence interval of the slope (note
that the dashed red lines are not the confidence interval of the regression
as the confidence interval of the intercept is not included). The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, res[1] + res[2] * x, 'r--')
>>> ax.plot(x, res[1] + res[3] * x, 'r--')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val stde_median : ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns the McKean-Schrader estimate of the standard error of the sample
median along the given axis. masked values are discarded.

Parameters
----------
data : ndarray
    Data to trim.
axis : {None,int}, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.
*)

val theilslopes : ?x:[>`Ndarray] Np.Obj.t -> ?alpha:float -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float * float * float)
(**
Computes the Theil-Sen estimator for a set of points (x, y).

`theilslopes` implements a method for robust linear regression.  It
computes the slope as the median of all slopes between paired values.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
alpha : float, optional
    Confidence degree between 0 and 1. Default is 95% confidence.
    Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are
    interpreted as 'find the 90% confidence interval'.

Returns
-------
medslope : float
    Theil slope.
medintercept : float
    Intercept of the Theil line, as ``median(y) - medslope*median(x)``.
lo_slope : float
    Lower bound of the confidence interval on `medslope`.
up_slope : float
    Upper bound of the confidence interval on `medslope`.

See also
--------
siegelslopes : a similar technique with repeated medians


Notes
-----
For more details on `theilslopes`, see `stats.theilslopes`.
*)

val tmax : ?upperlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed maximum

This function computes the maximum value of an array along a given axis,
while ignoring values larger than a specified upper limit.

Parameters
----------
a : array_like
    array of values
upperlimit : None or float, optional
    Values in the input array greater than the given limit will be ignored.
    When upperlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the upper limit
    are included.  The default value is True.

Returns
-------
tmax : float, int or ndarray

Notes
-----
For more details on `tmax`, see `stats.tmax`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 9, 1, 2],
...               [8, 7, 8, 2],
...               [5, 6, 0, 2],
...               [4, 5, 5, 2]])
...
...
>>> mstats.tmax(a, 4)
masked_array(data=[4, --, 3, 2],
             mask=[False,  True, False, False],
       fill_value=999999)
*)

val tmean : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed mean.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored.  When limits is None (default), then all
    values are used.  Either of the limit values in the tuple can also be
    None representing a half-open interval.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is None.

Returns
-------
tmean : float

Notes
-----
For more details on `tmean`, see `stats.tmean`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 9, 1, 2],
...               [8, 7, 8, 2],
...               [5, 6, 0, 2],
...               [4, 5, 5, 2]])
...
...
>>> mstats.tmean(a, (2,5))
3.3
>>> mstats.tmean(a, (2,5), axis=0)
masked_array(data=[4.0, 5.0, 4.0, 2.0],
             mask=[False, False, False, False],
       fill_value=1e+20)
*)

val tmin : ?lowerlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed minimum

Parameters
----------
a : array_like
    array of values
lowerlimit : None or float, optional
    Values in the input array less than the given limit will be ignored.
    When lowerlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the lower limit
    are included.  The default value is True.

Returns
-------
tmin : float, int or ndarray

Notes
-----
For more details on `tmin`, see `stats.tmin`.

Examples
--------
>>> from scipy.stats import mstats
>>> a = np.array([[6, 8, 3, 0],
...               [3, 2, 1, 2],
...               [8, 1, 8, 2],
...               [5, 3, 0, 2],
...               [4, 7, 5, 2]])
...
>>> mstats.tmin(a, 5)
masked_array(data=[5, 7, 5, --],
             mask=[False, False, False,  True],
       fill_value=999999)
*)

val trim : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Trims an array by masking the data outside some given limits.

Returns a masked version of the input array.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.


Examples
--------
>>> from scipy.stats.mstats import trim
>>> z = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10]
>>> print(trim(z,(3,8)))
[-- -- 3 4 5 6 7 8 -- --]
>>> print(trim(z,(0.1,0.2),relative=True))
[-- 2 3 4 5 6 7 8 -- --]
*)

val trim1 : ?proportiontocut:float -> ?tail:[`Left | `Right] -> ?inclusive:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Trims the data by masking values from one tail.

Parameters
----------
data : array_like
    Data to trim.
proportiontocut : float, optional
    Percentage of trimming. If n is the number of unmasked values
    before trimming, the number of values after trimming is
    ``(1 - proportiontocut) * n``.  Default is 0.2.
tail : {'left','right'}, optional
    If 'left' the `proportiontocut` lowest values will be masked.
    If 'right' the `proportiontocut` highest values will be masked.
    Default is 'left'.
inclusive : {(bool, bool) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).  Default is
    (True, True).
axis : int, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.  Default is None.

Returns
-------
trimtail : ndarray
    Returned array of same shape as `data` with masked tail values.
*)

val trima : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Trims an array by masking the data outside some given limits.

Returns a masked version of the input array.

Parameters
----------
a : array_like
    Input array.
limits : {None, tuple}, optional
    Tuple of (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit
    will be masked.  A limit is None indicates an open interval.
inclusive : (bool, bool) tuple, optional
    Tuple of (lower flag, upper flag), indicating whether values exactly
    equal to the lower (upper) limit are allowed.

Examples
--------
>>> from scipy.stats.mstats import trima

>>> a = np.arange(10)

The interval is left-closed and right-open, i.e., `[2, 8)`.
Trim the array by keeping only values in the interval.

>>> trima(a, limits=(2, 8), inclusive=(True, False))
masked_array(data=[--, --, 2, 3, 4, 5, 6, 7, --, --],
             mask=[ True,  True, False, False, False, False, False, False,
                    True,  True],
       fill_value=999999)
*)

val trimboth : ?proportiontocut:float -> ?inclusive:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Trims the smallest and largest data values.

Trims the `data` by masking the ``int(proportiontocut * n)`` smallest and
``int(proportiontocut * n)`` largest values of data along the given axis,
where n is the number of unmasked values before trimming.

Parameters
----------
data : ndarray
    Data to trim.
proportiontocut : float, optional
    Percentage of trimming (as a float between 0 and 1).
    If n is the number of unmasked values before trimming, the number of
    values after trimming is ``(1 - 2*proportiontocut) * n``.
    Default is 0.2.
inclusive : {(bool, bool) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).
axis : int, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.
*)

val trimmed_mean : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed mean of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.
*)

val trimmed_std : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> ?ddof:[`Zero | `I of int] -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed standard deviation of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.

ddof : {0,integer}, optional
    Means Delta Degrees of Freedom. The denominator used during computations
    is (n-ddof). DDOF=0 corresponds to a biased estimate, DDOF=1 to an un-
    biased estimate of the variance.
*)

val trimmed_stde : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the standard error of the trimmed mean along the given axis.

Parameters
----------
a : sequence
    Input array
limits : {(0.1,0.1), tuple of float}, optional
    tuple (lower percentage, upper percentage) to cut  on each side of the
    array, with respect to the number of unmasked data.

    If n is the number of unmasked data before trimming, the values
    smaller than ``n * limits[0]`` and the values larger than
    ``n * `limits[1]`` are masked, and the total number of unmasked
    data after trimming is ``n * (1.-sum(limits))``.  In each case,
    the value of one limit can be set to None to indicate an open interval.
    If `limits` is None, no trimming is performed.
inclusive : {(bool, bool) tuple} optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).
axis : int, optional
    Axis along which to trim.

Returns
-------
trimmed_stde : scalar or ndarray
*)

val trimmed_var : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?relative:bool -> ?axis:int -> ?ddof:[`Zero | `I of int] -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns the trimmed variance of the data along the given axis.


Parameters
----------
a : sequence
    Input array
limits : {None, tuple}, optional
    If `relative` is False, tuple (lower limit, upper limit) in absolute values.
    Values of the input array lower (greater) than the lower (upper) limit are
    masked.

    If `relative` is True, tuple (lower percentage, upper percentage) to cut
    on each side of the  array, with respect to the number of unmasked data.

    Noting n the number of unmasked data before trimming, the (n*limits[0])th
    smallest data and the (n*limits[1])th largest data are masked, and the
    total number of unmasked data after trimming is n*(1.-sum(limits))
    In each case, the value of one limit can be set to None to indicate an
    open interval.

    If limits is None, no trimming is performed
inclusive : {(bool, bool) tuple}, optional
    If `relative` is False, tuple indicating whether values exactly equal
    to the absolute limits are allowed.
    If `relative` is True, tuple indicating whether the number of data
    being masked on each side should be rounded (True) or truncated
    (False).
relative : bool, optional
    Whether to consider the limits as absolute values (False) or proportions
    to cut (True).
axis : int, optional
    Axis along which to trim.

ddof : {0,integer}, optional
    Means Delta Degrees of Freedom. The denominator used during computations
    is (n-ddof). DDOF=0 corresponds to a biased estimate, DDOF=1 to an un-
    biased estimate of the variance.
*)

val trimr : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Trims an array by masking some proportion of the data on each end.
Returns a masked version of the input array.

Parameters
----------
a : sequence
    Input array.
limits : {None, tuple}, optional
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1.
    Noting n the number of unmasked data before trimming, the
    (n*limits[0])th smallest data and the (n*limits[1])th largest data are
    masked, and the total number of unmasked data after trimming is
    n*(1.-sum(limits)).  The value of one limit can be set to None to
    indicate an open interval.
inclusive : {(True,True) tuple}, optional
    Tuple of flags indicating whether the number of data being masked on
    the left (right) end should be truncated (True) or rounded (False) to
    integers.
axis : {None,int}, optional
    Axis along which to trim. If None, the whole array is trimmed, but its
    shape is maintained.
*)

val trimtail : ?proportiontocut:float -> ?tail:[`Left | `Right] -> ?inclusive:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Trims the data by masking values from one tail.

Parameters
----------
data : array_like
    Data to trim.
proportiontocut : float, optional
    Percentage of trimming. If n is the number of unmasked values
    before trimming, the number of values after trimming is
    ``(1 - proportiontocut) * n``.  Default is 0.2.
tail : {'left','right'}, optional
    If 'left' the `proportiontocut` lowest values will be masked.
    If 'right' the `proportiontocut` highest values will be masked.
    Default is 'left'.
inclusive : {(bool, bool) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be rounded (True) or truncated (False).  Default is
    (True, True).
axis : int, optional
    Axis along which to perform the trimming.
    If None, the input array is first flattened.  Default is None.

Returns
-------
trimtail : ndarray
    Returned array of same shape as `data` with masked tail values.
*)

val tsem : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed standard error of the mean.

This function finds the standard error of the mean for given
values, ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    array of values
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is zero.
ddof : int, optional
    Delta degrees of freedom. Default is 1.

Returns
-------
tsem : float

Notes
-----
For more details on `tsem`, see `stats.tsem`.
*)

val ttest_1samp : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the mean of ONE group of scores.

Parameters
----------
a : array_like
    sample observation
popmean : float or array_like
    expected value in null hypothesis, if array_like than it must have the
    same shape as `a` excluding the axis dimension
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_1samp`, see `stats.ttest_1samp`.
*)

val ttest_ind : ?axis:[`I of int | `None] -> ?equal_var:bool -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the means of TWO INDEPENDENT samples of scores.

Parameters
----------
a, b : array_like
    The arrays must have the same shape, except in the dimension
    corresponding to `axis` (the first, by default).
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
equal_var : bool, optional
    If True, perform a standard independent 2 sample test that assumes equal
    population variances.
    If False, perform Welch's t-test, which does not assume equal population
    variance.

    .. versionadded:: 0.17.0

Returns
-------
statistic : float or array
    The calculated t-statistic.
pvalue : float or array
    The two-tailed p-value.

Notes
-----
For more details on `ttest_ind`, see `stats.ttest_ind`.
*)

val ttest_onesamp : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test for the mean of ONE group of scores.

Parameters
----------
a : array_like
    sample observation
popmean : float or array_like
    expected value in null hypothesis, if array_like than it must have the
    same shape as `a` excluding the axis dimension
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_1samp`, see `stats.ttest_1samp`.
*)

val ttest_rel : ?axis:[`I of int | `None] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculates the T-test on TWO RELATED samples of scores, a and b.

Parameters
----------
a, b : array_like
    The arrays must have the same shape.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.

Returns
-------
statistic : float or array
    t-statistic
pvalue : float or array
    two-tailed p-value

Notes
-----
For more details on `ttest_rel`, see `stats.ttest_rel`.
*)

val tvar : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed variance

This function computes the sample variance of an array of values,
while ignoring values which are outside of given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. If None, compute over the
    whole array. Default is zero.
ddof : int, optional
    Delta degrees of freedom. Default is 1.

Returns
-------
tvar : float
    Trimmed variance.

Notes
-----
For more details on `tvar`, see `stats.tvar`.
*)

val variation : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Computes the coefficient of variation, the ratio of the biased standard
deviation to the mean.

Parameters
----------
a : array_like
    Input array.
axis : int or None, optional
    Axis along which to calculate the coefficient of variation. Default
    is 0. If None, compute over the whole array `a`.

Returns
-------
variation : ndarray
    The calculated variation along the requested axis.

Notes
-----
For more details about `variation`, see `stats.variation`.

Examples
--------
>>> from scipy.stats.mstats import variation
>>> a = np.array([2,8,4])
>>> variation(a)
0.5345224838248487
>>> b = np.array([2,8,3,4])
>>> c = np.ma.masked_array(b, mask=[0,0,1,0])
>>> variation(c)
0.5345224838248487

In the example above, it can be seen that this works the same as
`stats.variation` except 'stats.mstats.variation' ignores masked 
array elements.
*)

val winsorize : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?inplace:bool -> ?axis:int -> a:Py.Object.t -> unit -> Py.Object.t
(**
Returns a Winsorized version of the input array.

The (limits[0])th lowest values are set to the (limits[0])th percentile,
and the (limits[1])th highest values are set to the (1 - limits[1])th
percentile.
Masked values are skipped.


Parameters
----------
a : sequence
    Input array.
limits : {None, tuple of float}, optional
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1.
    Noting n the number of unmasked data before trimming, the
    (n*limits[0])th smallest data and the (n*limits[1])th largest data are
    masked, and the total number of unmasked data after trimming
    is n*(1.-sum(limits)) The value of one limit can be set to None to
    indicate an open interval.
inclusive : {(True, True) tuple}, optional
    Tuple indicating whether the number of data being masked on each side
    should be truncated (True) or rounded (False).
inplace : {False, True}, optional
    Whether to winsorize in place (True) or to use a copy (False)
axis : {None, int}, optional
    Axis along which to trim. If None, the whole array is trimmed, but its
    shape is maintained.

Notes
-----
This function is applied to reduce the effect of possibly spurious outliers
by limiting the extreme values.

Examples
--------
>>> from scipy.stats.mstats import winsorize

A shuffled array contains integers from 1 to 10.

>>> a = np.array([10, 4, 9, 8, 5, 3, 7, 2, 1, 6])

The 10% of the lowest value (i.e., `1`) and the 20% of the highest
values (i.e., `9` and `10`) are replaced.

>>> winsorize(a, limits=[0.1, 0.2])
masked_array(data=[8, 4, 8, 8, 5, 3, 7, 2, 2, 6],
             mask=False,
       fill_value=999999)
*)


end

module Mstats_extras : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module MaskedArray : sig
type tag = [`MaskedArray]
type t = [`ArrayLike | `MaskedArray | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : ?data:[>`Ndarray] Np.Obj.t -> ?mask:Py.Object.t -> ?dtype:Np.Dtype.t -> ?copy:bool -> ?subok:bool -> ?ndmin:int -> ?fill_value:[`Bool of bool | `S of string | `I of int | `F of float] -> ?keep_mask:bool -> ?hard_mask:bool -> ?shrink:bool -> ?order:[`F | `A | `C] -> ?options:(string * Py.Object.t) list -> unit -> t
(**
An array class with possibly masked values.

Masked values of True exclude the corresponding element from any
computation.

Construction::

  x = MaskedArray(data, mask=nomask, dtype=None, copy=False, subok=True,
                  ndmin=0, fill_value=None, keep_mask=True, hard_mask=None,
                  shrink=True, order=None)

Parameters
----------
data : array_like
    Input data.
mask : sequence, optional
    Mask. Must be convertible to an array of booleans with the same
    shape as `data`. True indicates a masked (i.e. invalid) data.
dtype : dtype, optional
    Data type of the output.
    If `dtype` is None, the type of the data argument (``data.dtype``)
    is used. If `dtype` is not None and different from ``data.dtype``,
    a copy is performed.
copy : bool, optional
    Whether to copy the input data (True), or to use a reference instead.
    Default is False.
subok : bool, optional
    Whether to return a subclass of `MaskedArray` if possible (True) or a
    plain `MaskedArray`. Default is True.
ndmin : int, optional
    Minimum number of dimensions. Default is 0.
fill_value : scalar, optional
    Value used to fill in the masked values when necessary.
    If None, a default based on the data-type is used.
keep_mask : bool, optional
    Whether to combine `mask` with the mask of the input data, if any
    (True), or to use only `mask` for the output (False). Default is True.
hard_mask : bool, optional
    Whether to use a hard mask or not. With a hard mask, masked values
    cannot be unmasked. Default is False.
shrink : bool, optional
    Whether to force compression of an empty mask. Default is True.
order : {'C', 'F', 'A'}, optional
    Specify the order of the array.  If order is 'C', then the array
    will be in C-contiguous order (last-index varies the fastest).
    If order is 'F', then the returned array will be in
    Fortran-contiguous order (first-index varies the fastest).
    If order is 'A' (default), then the returned array may be
    in any order (either C-, Fortran-contiguous, or even discontiguous),
    unless a copy is required, in which case it will be C-contiguous.
*)

val __getitem__ : indx:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
x.__getitem__(y) <==> x[y]

Return the item described by i, as a masked array.
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val __setitem__ : indx:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
x.__setitem__(i, y) <==> x[i]=y

Set item described by index. If value is masked, masks those
locations.
*)

val all : ?axis:Py.Object.t -> ?out:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Returns True if all elements evaluate to True.

The output array is masked where all the values along the given axis
are masked: if the output would have been a scalar and that all the
values are masked, then the output is `masked`.

Refer to `numpy.all` for full documentation.

See Also
--------
numpy.ndarray.all : corresponding function for ndarrays
numpy.all : equivalent function

Examples
--------
>>> np.ma.array([1,2,3]).all()
True
>>> a = np.ma.array([1,2,3], mask=True)
>>> (a.all() is np.ma.masked)
True
*)

val anom : ?axis:int -> ?dtype:Np.Dtype.t -> [> tag] Obj.t -> Py.Object.t
(**
Compute the anomalies (deviations from the arithmetic mean)
along the given axis.

Returns an array of anomalies, with the same shape as the input and
where the arithmetic mean is computed along the given axis.

Parameters
----------
axis : int, optional
    Axis over which the anomalies are taken.
    The default is to use the mean of the flattened array as reference.
dtype : dtype, optional
    Type to use in computing the variance. For arrays of integer type
     the default is float32; for arrays of float types it is the same as
     the array type.

See Also
--------
mean : Compute the mean of the array.

Examples
--------
>>> a = np.ma.array([1,2,3])
>>> a.anom()
masked_array(data=[-1.,  0.,  1.],
             mask=False,
       fill_value=1e+20)
*)

val any : ?axis:Py.Object.t -> ?out:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Returns True if any of the elements of `a` evaluate to True.

Masked values are considered as False during computation.

Refer to `numpy.any` for full documentation.

See Also
--------
numpy.ndarray.any : corresponding function for ndarrays
numpy.any : equivalent function
*)

val argmax : ?axis:int -> ?fill_value:Py.Object.t -> ?out:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Returns array of indices of the maximum values along the given axis.
Masked values are treated as if they had the value fill_value.

Parameters
----------
axis : {None, integer}
    If None, the index is into the flattened array, otherwise along
    the specified axis
fill_value : {var}, optional
    Value used to fill in the masked values.  If None, the output of
    maximum_fill_value(self._data) is used instead.
out : {None, array}, optional
    Array into which the result can be placed. Its type is preserved
    and it must be of the right shape to hold the output.

Returns
-------
index_array : {integer_array}

Examples
--------
>>> a = np.arange(6).reshape(2,3)
>>> a.argmax()
5
>>> a.argmax(0)
array([1, 1, 1])
>>> a.argmax(1)
array([2, 2])
*)

val argmin : ?axis:int -> ?fill_value:Py.Object.t -> ?out:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Return array of indices to the minimum values along the given axis.

Parameters
----------
axis : {None, integer}
    If None, the index is into the flattened array, otherwise along
    the specified axis
fill_value : {var}, optional
    Value used to fill in the masked values.  If None, the output of
    minimum_fill_value(self._data) is used instead.
out : {None, array}, optional
    Array into which the result can be placed. Its type is preserved
    and it must be of the right shape to hold the output.

Returns
-------
ndarray or scalar
    If multi-dimension input, returns a new ndarray of indices to the
    minimum values along the given axis.  Otherwise, returns a scalar
    of index to the minimum values along the given axis.

Examples
--------
>>> x = np.ma.array(np.arange(4), mask=[1,1,0,0])
>>> x.shape = (2,2)
>>> x
masked_array(
  data=[[--, --],
        [2, 3]],
  mask=[[ True,  True],
        [False, False]],
  fill_value=999999)
>>> x.argmin(axis=0, fill_value=-1)
array([0, 0])
>>> x.argmin(axis=0, fill_value=9)
array([1, 1])
*)

val argpartition : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.argpartition(kth, axis=-1, kind='introselect', order=None)

Returns the indices that would partition this array.

Refer to `numpy.argpartition` for full documentation.

.. versionadded:: 1.8.0

See Also
--------
numpy.argpartition : equivalent function
*)

val argsort : ?axis:int -> ?kind:[`Stable | `Quicksort | `Heapsort | `Mergesort] -> ?order:[>`Ndarray] Np.Obj.t -> ?endwith:bool -> ?fill_value:Py.Object.t -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return an ndarray of indices that sort the array along the
specified axis.  Masked values are filled beforehand to
`fill_value`.

Parameters
----------
axis : int, optional
    Axis along which to sort. If None, the default, the flattened array
    is used.

    ..  versionchanged:: 1.13.0
        Previously, the default was documented to be -1, but that was
        in error. At some future date, the default will change to -1, as
        originally intended.
        Until then, the axis should be given explicitly when
        ``arr.ndim > 1``, to avoid a FutureWarning.
kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional
    The sorting algorithm used.
order : list, optional
    When `a` is an array with fields defined, this argument specifies
    which fields to compare first, second, etc.  Not all fields need be
    specified.
endwith : {True, False}, optional
    Whether missing values (if any) should be treated as the largest values
    (True) or the smallest values (False)
    When the array contains unmasked values at the same extremes of the
    datatype, the ordering of these values and the masked values is
    undefined.
fill_value : {var}, optional
    Value used internally for the masked values.
    If ``fill_value`` is not None, it supersedes ``endwith``.

Returns
-------
index_array : ndarray, int
    Array of indices that sort `a` along the specified axis.
    In other words, ``a[index_array]`` yields a sorted `a`.

See Also
--------
MaskedArray.sort : Describes sorting algorithms used.
lexsort : Indirect stable sort with multiple keys.
numpy.ndarray.sort : Inplace sort.

Notes
-----
See `sort` for notes on the different sorting algorithms.

Examples
--------
>>> a = np.ma.array([3,2,1], mask=[False, False, True])
>>> a
masked_array(data=[3, 2, --],
             mask=[False, False,  True],
       fill_value=999999)
>>> a.argsort()
array([1, 0, 2])
*)

val astype : ?order:[`C | `F | `A | `K] -> ?casting:[`No | `Equiv | `Safe | `Same_kind | `Unsafe] -> ?subok:Py.Object.t -> ?copy:bool -> dtype:[`S of string | `Dtype of Np.Dtype.t] -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)

Copy of the array, cast to a specified type.

Parameters
----------
dtype : str or dtype
    Typecode or data-type to which the array is cast.
order : {'C', 'F', 'A', 'K'}, optional
    Controls the memory layout order of the result.
    'C' means C order, 'F' means Fortran order, 'A'
    means 'F' order if all the arrays are Fortran contiguous,
    'C' order otherwise, and 'K' means as close to the
    order the array elements appear in memory as possible.
    Default is 'K'.
casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
    Controls what kind of data casting may occur. Defaults to 'unsafe'
    for backwards compatibility.

      * 'no' means the data types should not be cast at all.
      * 'equiv' means only byte-order changes are allowed.
      * 'safe' means only casts which can preserve values are allowed.
      * 'same_kind' means only safe casts or casts within a kind,
        like float64 to float32, are allowed.
      * 'unsafe' means any data conversions may be done.
subok : bool, optional
    If True, then sub-classes will be passed-through (default), otherwise
    the returned array will be forced to be a base-class array.
copy : bool, optional
    By default, astype always returns a newly allocated array. If this
    is set to false, and the `dtype`, `order`, and `subok`
    requirements are satisfied, the input array is returned instead
    of a copy.

Returns
-------
arr_t : ndarray
    Unless `copy` is False and the other conditions for returning the input
    array are satisfied (see description for `copy` input parameter), `arr_t`
    is a new array of the same shape as the input array, with dtype, order
    given by `dtype`, `order`.

Notes
-----
.. versionchanged:: 1.17.0
   Casting between a simple data type and a structured one is possible only
   for 'unsafe' casting.  Casting to multiple fields is allowed, but
   casting from multiple fields is not.

.. versionchanged:: 1.9.0
   Casting from numeric to string types in 'safe' casting mode requires
   that the string dtype length is long enough to store the max
   integer/float value converted.

Raises
------
ComplexWarning
    When casting from complex to float or int. To avoid this,
    one should use ``a.real.astype(t)``.

Examples
--------
>>> x = np.array([1, 2, 2.5])
>>> x
array([1. ,  2. ,  2.5])

>>> x.astype(int)
array([1, 2, 2])
*)

val byteswap : ?inplace:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
a.byteswap(inplace=False)

Swap the bytes of the array elements

Toggle between low-endian and big-endian data representation by
returning a byteswapped array, optionally swapped in-place.
Arrays of byte-strings are not swapped. The real and imaginary
parts of a complex number are swapped individually.

Parameters
----------
inplace : bool, optional
    If ``True``, swap bytes in-place, default is ``False``.

Returns
-------
out : ndarray
    The byteswapped array. If `inplace` is ``True``, this is
    a view to self.

Examples
--------
>>> A = np.array([1, 256, 8755], dtype=np.int16)
>>> list(map(hex, A))
['0x1', '0x100', '0x2233']
>>> A.byteswap(inplace=True)
array([  256,     1, 13090], dtype=int16)
>>> list(map(hex, A))
['0x100', '0x1', '0x3322']

Arrays of byte-strings are not swapped

>>> A = np.array([b'ceg', b'fac'])
>>> A.byteswap()
array([b'ceg', b'fac'], dtype='|S3')

``A.newbyteorder().byteswap()`` produces an array with the same values
  but different representation in memory

>>> A = np.array([1, 2, 3])
>>> A.view(np.uint8)
array([1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,
       0, 0], dtype=uint8)
>>> A.newbyteorder().byteswap(inplace=True)
array([1, 2, 3])
>>> A.view(np.uint8)
array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,
       0, 3], dtype=uint8)
*)

val choose : ?out:Py.Object.t -> ?mode:Py.Object.t -> choices:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
a.choose(choices, out=None, mode='raise')

Use an index array to construct a new array from a set of choices.

Refer to `numpy.choose` for full documentation.

See Also
--------
numpy.choose : equivalent function
*)

val clip : ?min:Py.Object.t -> ?max:Py.Object.t -> ?out:Py.Object.t -> ?kwargs:(string * Py.Object.t) list -> [> tag] Obj.t -> Py.Object.t
(**
a.clip(min=None, max=None, out=None, **kwargs)

Return an array whose values are limited to ``[min, max]``.
One of max or min must be given.

Refer to `numpy.clip` for full documentation.

See Also
--------
numpy.clip : equivalent function
*)

val compress : ?axis:int -> ?out:Py.Object.t -> condition:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return `a` where condition is ``True``.

If condition is a `MaskedArray`, missing values are considered
as ``False``.

Parameters
----------
condition : var
    Boolean 1-d array selecting which entries to return. If len(condition)
    is less than the size of a along the axis, then output is truncated
    to length of condition array.
axis : {None, int}, optional
    Axis along which the operation must be performed.
out : {None, ndarray}, optional
    Alternative output array in which to place the result. It must have
    the same shape as the expected output but the type will be cast if
    necessary.

Returns
-------
result : MaskedArray
    A :class:`MaskedArray` object.

Notes
-----
Please note the difference with :meth:`compressed` !
The output of :meth:`compress` has a mask, the output of
:meth:`compressed` does not.

Examples
--------
>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.compress([1, 0, 1])
masked_array(data=[1, 3],
             mask=[False, False],
       fill_value=999999)

>>> x.compress([1, 0, 1], axis=1)
masked_array(
  data=[[1, 3],
        [--, --],
        [7, 9]],
  mask=[[False, False],
        [ True,  True],
        [False, False]],
  fill_value=999999)
*)

val compressed : [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return all the non-masked data as a 1-D array.

Returns
-------
data : ndarray
    A new `ndarray` holding the non-masked data is returned.

Notes
-----
The result is **not** a MaskedArray!

Examples
--------
>>> x = np.ma.array(np.arange(5), mask=[0]*2 + [1]*3)
>>> x.compressed()
array([0, 1])
>>> type(x.compressed())
<class 'numpy.ndarray'>
*)

val copy : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.copy(order='C')

Return a copy of the array.

Parameters
----------
order : {'C', 'F', 'A', 'K'}, optional
    Controls the memory layout of the copy. 'C' means C-order,
    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
    'C' otherwise. 'K' means match the layout of `a` as closely
    as possible. (Note that this function and :func:`numpy.copy` are very
    similar, but have different default values for their order=
    arguments.)

See also
--------
numpy.copy
numpy.copyto

Examples
--------
>>> x = np.array([[1,2,3],[4,5,6]], order='F')

>>> y = x.copy()

>>> x.fill(0)

>>> x
array([[0, 0, 0],
       [0, 0, 0]])

>>> y
array([[1, 2, 3],
       [4, 5, 6]])

>>> y.flags['C_CONTIGUOUS']
True
*)

val count : ?axis:int list -> ?keepdims:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Count the non-masked elements of the array along the given axis.

Parameters
----------
axis : None or int or tuple of ints, optional
    Axis or axes along which the count is performed.
    The default, None, performs the count over all
    the dimensions of the input array. `axis` may be negative, in
    which case it counts from the last to the first axis.

    .. versionadded:: 1.10.0

    If this is a tuple of ints, the count is performed on multiple
    axes, instead of a single axis or all the axes as before.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the array.

Returns
-------
result : ndarray or scalar
    An array with the same shape as the input array, with the specified
    axis removed. If the array is a 0-d array, or if `axis` is None, a
    scalar is returned.

See Also
--------
count_masked : Count masked elements in array or along a given axis.

Examples
--------
>>> import numpy.ma as ma
>>> a = ma.arange(6).reshape((2, 3))
>>> a[1, :] = ma.masked
>>> a
masked_array(
  data=[[0, 1, 2],
        [--, --, --]],
  mask=[[False, False, False],
        [ True,  True,  True]],
  fill_value=999999)
>>> a.count()
3

When the `axis` keyword is specified an array of appropriate size is
returned.

>>> a.count(axis=0)
array([1, 1, 1])
>>> a.count(axis=1)
array([3, 0])
*)

val cumprod : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return the cumulative product of the array elements over the given axis.

Masked values are set to 1 internally during the computation.
However, their position is saved, and the result will be masked at
the same locations.

Refer to `numpy.cumprod` for full documentation.

Notes
-----
The mask is lost if `out` is not a valid MaskedArray !

Arithmetic is modular when using integer types, and no error is
raised on overflow.

See Also
--------
numpy.ndarray.cumprod : corresponding function for ndarrays
numpy.cumprod : equivalent function
*)

val cumsum : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return the cumulative sum of the array elements over the given axis.

Masked values are set to 0 internally during the computation.
However, their position is saved, and the result will be masked at
the same locations.

Refer to `numpy.cumsum` for full documentation.

Notes
-----
The mask is lost if `out` is not a valid :class:`MaskedArray` !

Arithmetic is modular when using integer types, and no error is
raised on overflow.

See Also
--------
numpy.ndarray.cumsum : corresponding function for ndarrays
numpy.cumsum : equivalent function

Examples
--------
>>> marr = np.ma.array(np.arange(10), mask=[0,0,0,1,1,1,0,0,0,0])
>>> marr.cumsum()
masked_array(data=[0, 1, 3, --, --, --, 9, 16, 24, 33],
             mask=[False, False, False,  True,  True,  True, False, False,
                   False, False],
       fill_value=999999)
*)

val diagonal : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.diagonal(offset=0, axis1=0, axis2=1)

Return specified diagonals. In NumPy 1.9 the returned array is a
read-only view instead of a copy as in previous NumPy versions.  In
a future version the read-only restriction will be removed.

Refer to :func:`numpy.diagonal` for full documentation.

See Also
--------
numpy.diagonal : equivalent function
*)

val dot : ?out:Py.Object.t -> ?strict:bool -> b:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
a.dot(b, out=None)

Masked dot product of two arrays. Note that `out` and `strict` are
located in different positions than in `ma.dot`. In order to
maintain compatibility with the functional version, it is
recommended that the optional arguments be treated as keyword only.
At some point that may be mandatory.

.. versionadded:: 1.10.0

Parameters
----------
b : masked_array_like
    Inputs array.
out : masked_array, optional
    Output argument. This must have the exact kind that would be
    returned if it was not used. In particular, it must have the
    right type, must be C-contiguous, and its dtype must be the
    dtype that would be returned for `ma.dot(a,b)`. This is a
    performance feature. Therefore, if these conditions are not
    met, an exception is raised, instead of attempting to be
    flexible.
strict : bool, optional
    Whether masked data are propagated (True) or set to 0 (False)
    for the computation. Default is False.  Propagating the mask
    means that if a masked value appears in a row or column, the
    whole row or column is considered masked.

    .. versionadded:: 1.10.2

See Also
--------
numpy.ma.dot : equivalent function
*)

val dump : file:[`S of string | `Path of Py.Object.t] -> [> tag] Obj.t -> Py.Object.t
(**
a.dump(file)

Dump a pickle of the array to the specified file.
The array can be read back with pickle.load or numpy.load.

Parameters
----------
file : str or Path
    A string naming the dump file.

    .. versionchanged:: 1.17.0
        `pathlib.Path` objects are now accepted.
*)

val fill : value:[`F of float | `I of int | `Bool of bool | `S of string] -> [> tag] Obj.t -> Py.Object.t
(**
a.fill(value)

Fill the array with a scalar value.

Parameters
----------
value : scalar
    All elements of `a` will be assigned this value.

Examples
--------
>>> a = np.array([1, 2])
>>> a.fill(0)
>>> a
array([0, 0])
>>> a = np.empty(2)
>>> a.fill(1)
>>> a
array([1.,  1.])
*)

val filled : ?fill_value:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return a copy of self, with masked values filled with a given value.
**However**, if there are no masked values to fill, self will be
returned instead as an ndarray.

Parameters
----------
fill_value : array_like, optional
    The value to use for invalid entries. Can be scalar or non-scalar.
    If non-scalar, the resulting ndarray must be broadcastable over
    input array. Default is None, in which case, the `fill_value`
    attribute of the array is used instead.

Returns
-------
filled_array : ndarray
    A copy of ``self`` with invalid entries replaced by *fill_value*
    (be it the function argument or the attribute of ``self``), or
    ``self`` itself as an ndarray if there are no invalid entries to
    be replaced.

Notes
-----
The result is **not** a MaskedArray!

Examples
--------
>>> x = np.ma.array([1,2,3,4,5], mask=[0,0,1,0,1], fill_value=-999)
>>> x.filled()
array([   1,    2, -999,    4, -999])
>>> x.filled(fill_value=1000)
array([   1,    2, 1000,    4, 1000])
>>> type(x.filled())
<class 'numpy.ndarray'>

Subclassing is preserved. This means that if, e.g., the data part of
the masked array is a recarray, `filled` returns a recarray:

>>> x = np.array([(-1, 2), (-3, 4)], dtype='i8,i8').view(np.recarray)
>>> m = np.ma.array(x, mask=[(True, False), (False, True)])
>>> m.filled()
rec.array([(999999,      2), (    -3, 999999)],
          dtype=[('f0', '<i8'), ('f1', '<i8')])
*)

val flatten : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
a.flatten(order='C')

Return a copy of the array collapsed into one dimension.

Parameters
----------
order : {'C', 'F', 'A', 'K'}, optional
    'C' means to flatten in row-major (C-style) order.
    'F' means to flatten in column-major (Fortran-
    style) order. 'A' means to flatten in column-major
    order if `a` is Fortran *contiguous* in memory,
    row-major order otherwise. 'K' means to flatten
    `a` in the order the elements occur in memory.
    The default is 'C'.

Returns
-------
y : ndarray
    A copy of the input array, flattened to one dimension.

See Also
--------
ravel : Return a flattened array.
flat : A 1-D flat iterator over the array.

Examples
--------
>>> a = np.array([[1,2], [3,4]])
>>> a.flatten()
array([1, 2, 3, 4])
>>> a.flatten('F')
array([1, 3, 2, 4])
*)

val get_fill_value : [> tag] Obj.t -> Py.Object.t
(**
The filling value of the masked array is a scalar. When setting, None
will set to a default based on the data type.

Examples
--------
>>> for dt in [np.int32, np.int64, np.float64, np.complex128]:
...     np.ma.array([0, 1], dtype=dt).get_fill_value()
...
999999
999999
1e+20
(1e+20+0j)

>>> x = np.ma.array([0, 1.], fill_value=-np.inf)
>>> x.fill_value
-inf
>>> x.fill_value = np.pi
>>> x.fill_value
3.1415926535897931 # may vary

Reset to default:

>>> x.fill_value = None
>>> x.fill_value
1e+20
*)

val get_imag : [> tag] Obj.t -> Py.Object.t
(**
The imaginary part of the masked array.

This property is a view on the imaginary part of this `MaskedArray`.

See Also
--------
real

Examples
--------
>>> x = np.ma.array([1+1.j, -2j, 3.45+1.6j], mask=[False, True, False])
>>> x.imag
masked_array(data=[1.0, --, 1.6],
             mask=[False,  True, False],
       fill_value=1e+20)
*)

val get_real : [> tag] Obj.t -> Py.Object.t
(**
The real part of the masked array.

This property is a view on the real part of this `MaskedArray`.

See Also
--------
imag

Examples
--------
>>> x = np.ma.array([1+1.j, -2j, 3.45+1.6j], mask=[False, True, False])
>>> x.real
masked_array(data=[1.0, --, 3.45],
             mask=[False,  True, False],
       fill_value=1e+20)
*)

val getfield : ?offset:int -> dtype:[`S of string | `Dtype of Np.Dtype.t] -> [> tag] Obj.t -> Py.Object.t
(**
a.getfield(dtype, offset=0)

Returns a field of the given array as a certain type.

A field is a view of the array data with a given data-type. The values in
the view are determined by the given type and the offset into the current
array in bytes. The offset needs to be such that the view dtype fits in the
array dtype; for example an array of dtype complex128 has 16-byte elements.
If taking a view with a 32-bit integer (4 bytes), the offset needs to be
between 0 and 12 bytes.

Parameters
----------
dtype : str or dtype
    The data type of the view. The dtype size of the view can not be larger
    than that of the array itself.
offset : int
    Number of bytes to skip before beginning the element view.

Examples
--------
>>> x = np.diag([1.+1.j]*2)
>>> x[1, 1] = 2 + 4.j
>>> x
array([[1.+1.j,  0.+0.j],
       [0.+0.j,  2.+4.j]])
>>> x.getfield(np.float64)
array([[1.,  0.],
       [0.,  2.]])

By choosing an offset of 8 bytes we can select the complex part of the
array for our view:

>>> x.getfield(np.float64, offset=8)
array([[1.,  0.],
       [0.,  4.]])
*)

val harden_mask : [> tag] Obj.t -> Py.Object.t
(**
Force the mask to hard.

Whether the mask of a masked array is hard or soft is determined by
its `hardmask` property. `harden_mask` sets `hardmask` to True.

See Also
--------
hardmask
*)

val ids : [> tag] Obj.t -> Py.Object.t
(**
Return the addresses of the data and mask areas.

Parameters
----------
None

Examples
--------
>>> x = np.ma.array([1, 2, 3], mask=[0, 1, 1])
>>> x.ids()
(166670640, 166659832) # may vary

If the array has no mask, the address of `nomask` is returned. This address
is typically not close to the data in memory:

>>> x = np.ma.array([1, 2, 3])
>>> x.ids()
(166691080, 3083169284L) # may vary
*)

val iscontiguous : [> tag] Obj.t -> Py.Object.t
(**
Return a boolean indicating whether the data is contiguous.

Parameters
----------
None

Examples
--------
>>> x = np.ma.array([1, 2, 3])
>>> x.iscontiguous()
True

`iscontiguous` returns one of the flags of the masked array:

>>> x.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
*)

val item : Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.item( *args)

Copy an element of an array to a standard Python scalar and return it.

Parameters
----------
\*args : Arguments (variable number and type)

    * none: in this case, the method only works for arrays
      with one element (`a.size == 1`), which element is
      copied into a standard Python scalar object and returned.

    * int_type: this argument is interpreted as a flat index into
      the array, specifying which element to copy and return.

    * tuple of int_types: functions as does a single int_type argument,
      except that the argument is interpreted as an nd-index into the
      array.

Returns
-------
z : Standard Python scalar object
    A copy of the specified element of the array as a suitable
    Python scalar

Notes
-----
When the data type of `a` is longdouble or clongdouble, item() returns
a scalar array object because there is no available Python scalar that
would not lose information. Void arrays return a buffer object for item(),
unless fields are defined, in which case a tuple is returned.

`item` is very similar to a[args], except, instead of an array scalar,
a standard Python scalar is returned. This can be useful for speeding up
access to elements of the array and doing arithmetic on elements of the
array using Python's optimized math.

Examples
--------
>>> np.random.seed(123)
>>> x = np.random.randint(9, size=(3, 3))
>>> x
array([[2, 2, 6],
       [1, 3, 6],
       [1, 0, 1]])
>>> x.item(3)
1
>>> x.item(7)
0
>>> x.item((0, 1))
2
>>> x.item((2, 2))
1
*)

val itemset : Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.itemset( *args)

Insert scalar into an array (scalar is cast to array's dtype, if possible)

There must be at least 1 argument, and define the last argument
as *item*.  Then, ``a.itemset( *args)`` is equivalent to but faster
than ``a[args] = item``.  The item should be a scalar value and `args`
must select a single item in the array `a`.

Parameters
----------
\*args : Arguments
    If one argument: a scalar, only used in case `a` is of size 1.
    If two arguments: the last argument is the value to be set
    and must be a scalar, the first argument specifies a single array
    element location. It is either an int or a tuple.

Notes
-----
Compared to indexing syntax, `itemset` provides some speed increase
for placing a scalar into a particular location in an `ndarray`,
if you must do this.  However, generally this is discouraged:
among other problems, it complicates the appearance of the code.
Also, when using `itemset` (and `item`) inside a loop, be sure
to assign the methods to a local variable to avoid the attribute
look-up at each loop iteration.

Examples
--------
>>> np.random.seed(123)
>>> x = np.random.randint(9, size=(3, 3))
>>> x
array([[2, 2, 6],
       [1, 3, 6],
       [1, 0, 1]])
>>> x.itemset(4, 0)
>>> x.itemset((2, 2), 9)
>>> x
array([[2, 2, 6],
       [1, 0, 6],
       [1, 0, 9]])
*)

val max : ?axis:int -> ?out:[>`Ndarray] Np.Obj.t -> ?fill_value:Py.Object.t -> ?keepdims:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return the maximum along a given axis.

Parameters
----------
axis : {None, int}, optional
    Axis along which to operate.  By default, ``axis`` is None and the
    flattened input is used.
out : array_like, optional
    Alternative output array in which to place the result.  Must
    be of the same shape and buffer length as the expected output.
fill_value : {var}, optional
    Value used to fill in the masked values.
    If None, use the output of maximum_fill_value().
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the array.

Returns
-------
amax : array_like
    New array holding the result.
    If ``out`` was specified, ``out`` is returned.

See Also
--------
maximum_fill_value
    Returns the maximum filling value for a given datatype.
*)

val mean : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Returns the average of the array elements along given axis.

Masked entries are ignored, and result elements which are not
finite will be masked.

Refer to `numpy.mean` for full documentation.

See Also
--------
numpy.ndarray.mean : corresponding function for ndarrays
numpy.mean : Equivalent function
numpy.ma.average: Weighted average.

Examples
--------
>>> a = np.ma.array([1,2,3], mask=[False, False, True])
>>> a
masked_array(data=[1, 2, --],
             mask=[False, False,  True],
       fill_value=999999)
>>> a.mean()
1.5
*)

val min : ?axis:int -> ?out:[>`Ndarray] Np.Obj.t -> ?fill_value:Py.Object.t -> ?keepdims:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return the minimum along a given axis.

Parameters
----------
axis : {None, int}, optional
    Axis along which to operate.  By default, ``axis`` is None and the
    flattened input is used.
out : array_like, optional
    Alternative output array in which to place the result.  Must be of
    the same shape and buffer length as the expected output.
fill_value : {var}, optional
    Value used to fill in the masked values.
    If None, use the output of `minimum_fill_value`.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the array.

Returns
-------
amin : array_like
    New array holding the result.
    If ``out`` was specified, ``out`` is returned.

See Also
--------
minimum_fill_value
    Returns the minimum filling value for a given datatype.
*)

val mini : ?axis:int -> [> tag] Obj.t -> Py.Object.t
(**
Return the array minimum along the specified axis.

.. deprecated:: 1.13.0
   This function is identical to both:

    * ``self.min(keepdims=True, axis=axis).squeeze(axis=axis)``
    * ``np.ma.minimum.reduce(self, axis=axis)``

   Typically though, ``self.min(axis=axis)`` is sufficient.

Parameters
----------
axis : int, optional
    The axis along which to find the minima. Default is None, in which case
    the minimum value in the whole array is returned.

Returns
-------
min : scalar or MaskedArray
    If `axis` is None, the result is a scalar. Otherwise, if `axis` is
    given and the array is at least 2-D, the result is a masked array with
    dimension one smaller than the array on which `mini` is called.

Examples
--------
>>> x = np.ma.array(np.arange(6), mask=[0 ,1, 0, 0, 0 ,1]).reshape(3, 2)
>>> x
masked_array(
  data=[[0, --],
        [2, 3],
        [4, --]],
  mask=[[False,  True],
        [False, False],
        [False,  True]],
  fill_value=999999)
>>> x.mini()
masked_array(data=0,
             mask=False,
       fill_value=999999)
>>> x.mini(axis=0)
masked_array(data=[0, 3],
             mask=[False, False],
       fill_value=999999)
>>> x.mini(axis=1)
masked_array(data=[0, 2, 4],
             mask=[False, False, False],
       fill_value=999999)

There is a small difference between `mini` and `min`:

>>> x[:,1].mini(axis=0)
masked_array(data=3,
             mask=False,
       fill_value=999999)
>>> x[:,1].min(axis=0)
3
*)

val newbyteorder : ?new_order:string -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
arr.newbyteorder(new_order='S')

Return the array with the same data viewed with a different byte order.

Equivalent to::

    arr.view(arr.dtype.newbytorder(new_order))

Changes are also made in all fields and sub-arrays of the array data
type.



Parameters
----------
new_order : string, optional
    Byte order to force; a value from the byte order specifications
    below. `new_order` codes can be any of:

    * 'S' - swap dtype from current to opposite endian
    * {'<', 'L'} - little endian
    * {'>', 'B'} - big endian
    * {'=', 'N'} - native order
    * {'|', 'I'} - ignore (no change to byte order)

    The default value ('S') results in swapping the current
    byte order. The code does a case-insensitive check on the first
    letter of `new_order` for the alternatives above.  For example,
    any of 'B' or 'b' or 'biggish' are valid to specify big-endian.


Returns
-------
new_arr : array
    New array object with the dtype reflecting given change to the
    byte order.
*)

val nonzero : [> tag] Obj.t -> Py.Object.t
(**
Return the indices of unmasked elements that are not zero.

Returns a tuple of arrays, one for each dimension, containing the
indices of the non-zero elements in that dimension. The corresponding
non-zero values can be obtained with::

    a[a.nonzero()]

To group the indices by element, rather than dimension, use
instead::

    np.transpose(a.nonzero())

The result of this is always a 2d array, with a row for each non-zero
element.

Parameters
----------
None

Returns
-------
tuple_of_arrays : tuple
    Indices of elements that are non-zero.

See Also
--------
numpy.nonzero :
    Function operating on ndarrays.
flatnonzero :
    Return indices that are non-zero in the flattened version of the input
    array.
numpy.ndarray.nonzero :
    Equivalent ndarray method.
count_nonzero :
    Counts the number of non-zero elements in the input array.

Examples
--------
>>> import numpy.ma as ma
>>> x = ma.array(np.eye(3))
>>> x
masked_array(
  data=[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],
  mask=False,
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 1, 2]), array([0, 1, 2]))

Masked elements are ignored.

>>> x[1, 1] = ma.masked
>>> x
masked_array(
  data=[[1.0, 0.0, 0.0],
        [0.0, --, 0.0],
        [0.0, 0.0, 1.0]],
  mask=[[False, False, False],
        [False,  True, False],
        [False, False, False]],
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 2]), array([0, 2]))

Indices can also be grouped by element.

>>> np.transpose(x.nonzero())
array([[0, 0],
       [2, 2]])

A common use for ``nonzero`` is to find the indices of an array, where
a condition is True.  Given an array `a`, the condition `a` > 3 is a
boolean array and since False is interpreted as 0, ma.nonzero(a > 3)
yields the indices of the `a` where the condition is true.

>>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])
>>> a > 3
masked_array(
  data=[[False, False, False],
        [ True,  True,  True],
        [ True,  True,  True]],
  mask=False,
  fill_value=True)
>>> ma.nonzero(a > 3)
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))

The ``nonzero`` method of the condition array can also be called.

>>> (a > 3).nonzero()
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))
*)

val partition : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.partition(kth, axis=-1, kind='introselect', order=None)

Rearranges the elements in the array in such a way that the value of the
element in kth position is in the position it would be in a sorted array.
All elements smaller than the kth element are moved before this element and
all equal or greater are moved behind it. The ordering of the elements in
the two partitions is undefined.

.. versionadded:: 1.8.0

Parameters
----------
kth : int or sequence of ints
    Element index to partition by. The kth element value will be in its
    final sorted position and all smaller elements will be moved before it
    and all equal or greater elements behind it.
    The order of all elements in the partitions is undefined.
    If provided with a sequence of kth it will partition all elements
    indexed by kth of them into their sorted position at once.
axis : int, optional
    Axis along which to sort. Default is -1, which means sort along the
    last axis.
kind : {'introselect'}, optional
    Selection algorithm. Default is 'introselect'.
order : str or list of str, optional
    When `a` is an array with fields defined, this argument specifies
    which fields to compare first, second, etc. A single field can
    be specified as a string, and not all fields need to be specified,
    but unspecified fields will still be used, in the order in which
    they come up in the dtype, to break ties.

See Also
--------
numpy.partition : Return a parititioned copy of an array.
argpartition : Indirect partition.
sort : Full sort.

Notes
-----
See ``np.partition`` for notes on the different algorithms.

Examples
--------
>>> a = np.array([3, 4, 2, 1])
>>> a.partition(3)
>>> a
array([2, 1, 3, 4])

>>> a.partition((1, 3))
>>> a
array([1, 2, 3, 4])
*)

val prod : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return the product of the array elements over the given axis.

Masked elements are set to 1 internally for computation.

Refer to `numpy.prod` for full documentation.

Notes
-----
Arithmetic is modular when using integer types, and no error is raised
on overflow.

See Also
--------
numpy.ndarray.prod : corresponding function for ndarrays
numpy.prod : equivalent function
*)

val ptp : ?axis:int -> ?out:[>`Ndarray] Np.Obj.t -> ?fill_value:Py.Object.t -> ?keepdims:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return (maximum - minimum) along the given dimension
(i.e. peak-to-peak value).

Parameters
----------
axis : {None, int}, optional
    Axis along which to find the peaks.  If None (default) the
    flattened array is used.
out : {None, array_like}, optional
    Alternative output array in which to place the result. It must
    have the same shape and buffer length as the expected output
    but the type will be cast if necessary.
fill_value : {var}, optional
    Value used to fill in the masked values.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the array.

Returns
-------
ptp : ndarray.
    A new array holding the result, unless ``out`` was
    specified, in which case a reference to ``out`` is returned.
*)

val put : ?mode:[`Raise | `Wrap | `Clip] -> indices:Py.Object.t -> values:[>`Ndarray] Np.Obj.t -> [> tag] Obj.t -> Py.Object.t
(**
Set storage-indexed locations to corresponding values.

Sets self._data.flat[n] = values[n] for each n in indices.
If `values` is shorter than `indices` then it will repeat.
If `values` has some masked values, the initial mask is updated
in consequence, else the corresponding values are unmasked.

Parameters
----------
indices : 1-D array_like
    Target indices, interpreted as integers.
values : array_like
    Values to place in self._data copy at target indices.
mode : {'raise', 'wrap', 'clip'}, optional
    Specifies how out-of-bounds indices will behave.
    'raise' : raise an error.
    'wrap' : wrap around.
    'clip' : clip to the range.

Notes
-----
`values` can be a scalar or length 1 array.

Examples
--------
>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.put([0,4,8],[10,20,30])
>>> x
masked_array(
  data=[[10, --, 3],
        [--, 20, --],
        [7, --, 30]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)

>>> x.put(4,999)
>>> x
masked_array(
  data=[[10, --, 3],
        [--, 999, --],
        [7, --, 30]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
*)

val ravel : ?order:[`C | `F | `A | `K] -> [> tag] Obj.t -> Py.Object.t
(**
Returns a 1D version of self, as a view.

Parameters
----------
order : {'C', 'F', 'A', 'K'}, optional
    The elements of `a` are read using this index order. 'C' means to
    index the elements in C-like order, with the last axis index
    changing fastest, back to the first axis index changing slowest.
    'F' means to index the elements in Fortran-like index order, with
    the first index changing fastest, and the last index changing
    slowest. Note that the 'C' and 'F' options take no account of the
    memory layout of the underlying array, and only refer to the order
    of axis indexing.  'A' means to read the elements in Fortran-like
    index order if `m` is Fortran *contiguous* in memory, C-like order
    otherwise.  'K' means to read the elements in the order they occur
    in memory, except for reversing the data when strides are negative.
    By default, 'C' index order is used.

Returns
-------
MaskedArray
    Output view is of shape ``(self.size,)`` (or
    ``(np.ma.product(self.shape),)``).

Examples
--------
>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.ravel()
masked_array(data=[1, --, 3, --, 5, --, 7, --, 9],
             mask=[False,  True, False,  True, False,  True, False,  True,
                   False],
       fill_value=999999)
*)

val repeat : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.repeat(repeats, axis=None)

Repeat elements of an array.

Refer to `numpy.repeat` for full documentation.

See Also
--------
numpy.repeat : equivalent function
*)

val reshape : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Give a new shape to the array without changing its data.

Returns a masked array containing the same data, but with a new shape.
The result is a view on the original array; if this is not possible, a
ValueError is raised.

Parameters
----------
shape : int or tuple of ints
    The new shape should be compatible with the original shape. If an
    integer is supplied, then the result will be a 1-D array of that
    length.
order : {'C', 'F'}, optional
    Determines whether the array data should be viewed as in C
    (row-major) or FORTRAN (column-major) order.

Returns
-------
reshaped_array : array
    A new view on the array.

See Also
--------
reshape : Equivalent function in the masked array module.
numpy.ndarray.reshape : Equivalent method on ndarray object.
numpy.reshape : Equivalent function in the NumPy module.

Notes
-----
The reshaping operation cannot guarantee that a copy will not be made,
to modify the shape in place, use ``a.shape = s``

Examples
--------
>>> x = np.ma.array([[1,2],[3,4]], mask=[1,0,0,1])
>>> x
masked_array(
  data=[[--, 2],
        [3, --]],
  mask=[[ True, False],
        [False,  True]],
  fill_value=999999)
>>> x = x.reshape((4,1))
>>> x
masked_array(
  data=[[--],
        [2],
        [3],
        [--]],
  mask=[[ True],
        [False],
        [False],
        [ True]],
  fill_value=999999)
*)

val resize : ?refcheck:Py.Object.t -> ?order:Py.Object.t -> newshape:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
.. warning::

    This method does nothing, except raise a ValueError exception. A
    masked array does not own its data and therefore cannot safely be
    resized in place. Use the `numpy.ma.resize` function instead.

This method is difficult to implement safely and may be deprecated in
future releases of NumPy.
*)

val round : ?decimals:Py.Object.t -> ?out:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return each element rounded to the given number of decimals.

Refer to `numpy.around` for full documentation.

See Also
--------
numpy.ndarray.around : corresponding function for ndarrays
numpy.around : equivalent function
*)

val searchsorted : ?side:Py.Object.t -> ?sorter:Py.Object.t -> v:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
a.searchsorted(v, side='left', sorter=None)

Find indices where elements of v should be inserted in a to maintain order.

For full documentation, see `numpy.searchsorted`

See Also
--------
numpy.searchsorted : equivalent function
*)

val set_fill_value : ?value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
The filling value of the masked array is a scalar. When setting, None
will set to a default based on the data type.

Examples
--------
>>> for dt in [np.int32, np.int64, np.float64, np.complex128]:
...     np.ma.array([0, 1], dtype=dt).get_fill_value()
...
999999
999999
1e+20
(1e+20+0j)

>>> x = np.ma.array([0, 1.], fill_value=-np.inf)
>>> x.fill_value
-inf
>>> x.fill_value = np.pi
>>> x.fill_value
3.1415926535897931 # may vary

Reset to default:

>>> x.fill_value = None
>>> x.fill_value
1e+20
*)

val setfield : ?offset:int -> val_:Py.Object.t -> dtype:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
a.setfield(val, dtype, offset=0)

Put a value into a specified place in a field defined by a data-type.

Place `val` into `a`'s field defined by `dtype` and beginning `offset`
bytes into the field.

Parameters
----------
val : object
    Value to be placed in field.
dtype : dtype object
    Data-type of the field in which to place `val`.
offset : int, optional
    The number of bytes into the field at which to place `val`.

Returns
-------
None

See Also
--------
getfield

Examples
--------
>>> x = np.eye(3)
>>> x.getfield(np.float64)
array([[1.,  0.,  0.],
       [0.,  1.,  0.],
       [0.,  0.,  1.]])
>>> x.setfield(3, np.int32)
>>> x.getfield(np.int32)
array([[3, 3, 3],
       [3, 3, 3],
       [3, 3, 3]], dtype=int32)
>>> x
array([[1.0e+000, 1.5e-323, 1.5e-323],
       [1.5e-323, 1.0e+000, 1.5e-323],
       [1.5e-323, 1.5e-323, 1.0e+000]])
>>> x.setfield(np.eye(3), np.int32)
>>> x
array([[1.,  0.,  0.],
       [0.,  1.,  0.],
       [0.,  0.,  1.]])
*)

val setflags : ?write:bool -> ?align:bool -> ?uic:bool -> [> tag] Obj.t -> Py.Object.t
(**
a.setflags(write=None, align=None, uic=None)

Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY),
respectively.

These Boolean-valued flags affect how numpy interprets the memory
area used by `a` (see Notes below). The ALIGNED flag can only
be set to True if the data is actually aligned according to the type.
The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set
to True. The flag WRITEABLE can only be set to True if the array owns its
own memory, or the ultimate owner of the memory exposes a writeable buffer
interface, or is a string. (The exception for string is made so that
unpickling can be done without copying memory.)

Parameters
----------
write : bool, optional
    Describes whether or not `a` can be written to.
align : bool, optional
    Describes whether or not `a` is aligned properly for its type.
uic : bool, optional
    Describes whether or not `a` is a copy of another 'base' array.

Notes
-----
Array flags provide information about how the memory area used
for the array is to be interpreted. There are 7 Boolean flags
in use, only four of which can be changed by the user:
WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.

WRITEABLE (W) the data area can be written to;

ALIGNED (A) the data and strides are aligned appropriately for the hardware
(as determined by the compiler);

UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;

WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced
by .base). When the C-API function PyArray_ResolveWritebackIfCopy is
called, the base array will be updated with the contents of this array.

All flags can be accessed using the single (upper case) letter as well
as the full name.

Examples
--------
>>> y = np.array([[3, 1, 7],
...               [2, 0, 0],
...               [8, 5, 9]])
>>> y
array([[3, 1, 7],
       [2, 0, 0],
       [8, 5, 9]])
>>> y.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
>>> y.setflags(write=0, align=0)
>>> y.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : False
  ALIGNED : False
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
>>> y.setflags(uic=1)
Traceback (most recent call last):
  File '<stdin>', line 1, in <module>
ValueError: cannot set WRITEBACKIFCOPY flag to True
*)

val shrink_mask : [> tag] Obj.t -> Py.Object.t
(**
Reduce a mask to nomask when possible.

Parameters
----------
None

Returns
-------
None

Examples
--------
>>> x = np.ma.array([[1,2 ], [3, 4]], mask=[0]*4)
>>> x.mask
array([[False, False],
       [False, False]])
>>> x.shrink_mask()
masked_array(
  data=[[1, 2],
        [3, 4]],
  mask=False,
  fill_value=999999)
>>> x.mask
False
*)

val soften_mask : [> tag] Obj.t -> Py.Object.t
(**
Force the mask to soft.

Whether the mask of a masked array is hard or soft is determined by
its `hardmask` property. `soften_mask` sets `hardmask` to False.

See Also
--------
hardmask
*)

val sort : ?axis:int -> ?kind:[`Stable | `Quicksort | `Heapsort | `Mergesort] -> ?order:[>`Ndarray] Np.Obj.t -> ?endwith:bool -> ?fill_value:Py.Object.t -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Sort the array, in-place

Parameters
----------
a : array_like
    Array to be sorted.
axis : int, optional
    Axis along which to sort. If None, the array is flattened before
    sorting. The default is -1, which sorts along the last axis.
kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional
    The sorting algorithm used.
order : list, optional
    When `a` is a structured array, this argument specifies which fields
    to compare first, second, and so on.  This list does not need to
    include all of the fields.
endwith : {True, False}, optional
    Whether missing values (if any) should be treated as the largest values
    (True) or the smallest values (False)
    When the array contains unmasked values sorting at the same extremes of the
    datatype, the ordering of these values and the masked values is
    undefined.
fill_value : {var}, optional
    Value used internally for the masked values.
    If ``fill_value`` is not None, it supersedes ``endwith``.

Returns
-------
sorted_array : ndarray
    Array of the same type and shape as `a`.

See Also
--------
numpy.ndarray.sort : Method to sort an array in-place.
argsort : Indirect sort.
lexsort : Indirect stable sort on multiple keys.
searchsorted : Find elements in a sorted array.

Notes
-----
See ``sort`` for notes on the different sorting algorithms.

Examples
--------
>>> a = np.ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0])
>>> # Default
>>> a.sort()
>>> a
masked_array(data=[1, 3, 5, --, --],
             mask=[False, False, False,  True,  True],
       fill_value=999999)

>>> a = np.ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0])
>>> # Put missing values in the front
>>> a.sort(endwith=False)
>>> a
masked_array(data=[--, --, 1, 3, 5],
             mask=[ True,  True, False, False, False],
       fill_value=999999)

>>> a = np.ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0])
>>> # fill_value takes over endwith
>>> a.sort(endwith=False, fill_value=3)
>>> a
masked_array(data=[1, --, --, 3, 5],
             mask=[False,  True,  True, False, False],
       fill_value=999999)
*)

val squeeze : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.squeeze(axis=None)

Remove single-dimensional entries from the shape of `a`.

Refer to `numpy.squeeze` for full documentation.

See Also
--------
numpy.squeeze : equivalent function
*)

val std : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> ?ddof:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Returns the standard deviation of the array elements along given axis.

Masked entries are ignored.

Refer to `numpy.std` for full documentation.

See Also
--------
numpy.ndarray.std : corresponding function for ndarrays
numpy.std : Equivalent function
*)

val sum : ?axis:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> ?keepdims:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return the sum of the array elements over the given axis.

Masked elements are set to 0 internally.

Refer to `numpy.sum` for full documentation.

See Also
--------
numpy.ndarray.sum : corresponding function for ndarrays
numpy.sum : equivalent function

Examples
--------
>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.sum()
25
>>> x.sum(axis=1)
masked_array(data=[4, 5, 16],
             mask=[False, False, False],
       fill_value=999999)
>>> x.sum(axis=0)
masked_array(data=[8, 5, 12],
             mask=[False, False, False],
       fill_value=999999)
>>> print(type(x.sum(axis=0, dtype=np.int64)[0]))
<class 'numpy.int64'>
*)

val swapaxes : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> Py.Object.t
(**
a.swapaxes(axis1, axis2)

Return a view of the array with `axis1` and `axis2` interchanged.

Refer to `numpy.swapaxes` for full documentation.

See Also
--------
numpy.swapaxes : equivalent function
*)

val take : ?axis:Py.Object.t -> ?out:Py.Object.t -> ?mode:Py.Object.t -> indices:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
        
*)

val tobytes : ?fill_value:[`Bool of bool | `S of string | `I of int | `F of float] -> ?order:[`C | `F | `A] -> [> tag] Obj.t -> Py.Object.t
(**
Return the array data as a string containing the raw bytes in the array.

The array is filled with a fill value before the string conversion.

.. versionadded:: 1.9.0

Parameters
----------
fill_value : scalar, optional
    Value used to fill in the masked values. Default is None, in which
    case `MaskedArray.fill_value` is used.
order : {'C','F','A'}, optional
    Order of the data item in the copy. Default is 'C'.

    - 'C'   -- C order (row major).
    - 'F'   -- Fortran order (column major).
    - 'A'   -- Any, current order of array.
    - None  -- Same as 'A'.

See Also
--------
numpy.ndarray.tobytes
tolist, tofile

Notes
-----
As for `ndarray.tobytes`, information about the shape, dtype, etc.,
but also about `fill_value`, will be lost.

Examples
--------
>>> x = np.ma.array(np.array([[1, 2], [3, 4]]), mask=[[0, 1], [1, 0]])
>>> x.tobytes()
b'\x01\x00\x00\x00\x00\x00\x00\x00?B\x0f\x00\x00\x00\x00\x00?B\x0f\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00'
*)

val tofile : ?sep:Py.Object.t -> ?format:Py.Object.t -> fid:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Save a masked array to a file in binary format.

.. warning::
  This function is not implemented yet.

Raises
------
NotImplementedError
    When `tofile` is called.
*)

val toflex : [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Transforms a masked array into a flexible-type array.

The flexible type array that is returned will have two fields:

* the ``_data`` field stores the ``_data`` part of the array.
* the ``_mask`` field stores the ``_mask`` part of the array.

Parameters
----------
None

Returns
-------
record : ndarray
    A new flexible-type `ndarray` with two fields: the first element
    containing a value, the second element containing the corresponding
    mask boolean. The returned record shape matches self.shape.

Notes
-----
A side-effect of transforming a masked array into a flexible `ndarray` is
that meta information (``fill_value``, ...) will be lost.

Examples
--------
>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.toflex()
array([[(1, False), (2,  True), (3, False)],
       [(4,  True), (5, False), (6,  True)],
       [(7, False), (8,  True), (9, False)]],
      dtype=[('_data', '<i8'), ('_mask', '?')])
*)

val tolist : ?fill_value:[`Bool of bool | `S of string | `I of int | `F of float] -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return the data portion of the masked array as a hierarchical Python list.

Data items are converted to the nearest compatible Python type.
Masked values are converted to `fill_value`. If `fill_value` is None,
the corresponding entries in the output list will be ``None``.

Parameters
----------
fill_value : scalar, optional
    The value to use for invalid entries. Default is None.

Returns
-------
result : list
    The Python list representation of the masked array.

Examples
--------
>>> x = np.ma.array([[1,2,3], [4,5,6], [7,8,9]], mask=[0] + [1,0]*4)
>>> x.tolist()
[[1, None, 3], [None, 5, None], [7, None, 9]]
>>> x.tolist(-999)
[[1, -999, 3], [-999, 5, -999], [7, -999, 9]]
*)

val tostring : ?fill_value:Py.Object.t -> ?order:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
This function is a compatibility alias for tobytes. Despite its name it
returns bytes not strings.
*)

val trace : ?offset:Py.Object.t -> ?axis1:Py.Object.t -> ?axis2:Py.Object.t -> ?dtype:Py.Object.t -> ?out:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)

Return the sum along diagonals of the array.

Refer to `numpy.trace` for full documentation.

See Also
--------
numpy.trace : equivalent function
*)

val transpose : ?params:(string * Py.Object.t) list -> Py.Object.t list -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
a.transpose( *axes)

Returns a view of the array with axes transposed.

For a 1-D array this has no effect, as a transposed vector is simply the
same vector. To convert a 1-D array into a 2D column vector, an additional
dimension must be added. `np.atleast2d(a).T` achieves this, as does
`a[:, np.newaxis]`.
For a 2-D array, this is a standard matrix transpose.
For an n-D array, if axes are given, their order indicates how the
axes are permuted (see Examples). If axes are not provided and
``a.shape = (i[0], i[1], ... i[n-2], i[n-1])``, then
``a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.

Parameters
----------
axes : None, tuple of ints, or `n` ints

 * None or no argument: reverses the order of the axes.

 * tuple of ints: `i` in the `j`-th place in the tuple means `a`'s
   `i`-th axis becomes `a.transpose()`'s `j`-th axis.

 * `n` ints: same as an n-tuple of the same ints (this form is
   intended simply as a 'convenience' alternative to the tuple form)

Returns
-------
out : ndarray
    View of `a`, with axes suitably permuted.

See Also
--------
ndarray.T : Array property returning the array transposed.
ndarray.reshape : Give a new shape to an array without changing its data.

Examples
--------
>>> a = np.array([[1, 2], [3, 4]])
>>> a
array([[1, 2],
       [3, 4]])
>>> a.transpose()
array([[1, 3],
       [2, 4]])
>>> a.transpose((1, 0))
array([[1, 3],
       [2, 4]])
>>> a.transpose(1, 0)
array([[1, 3],
       [2, 4]])
*)

val unshare_mask : [> tag] Obj.t -> Py.Object.t
(**
Copy the mask and set the sharedmask flag to False.

Whether the mask is shared between masked arrays can be seen from
the `sharedmask` property. `unshare_mask` ensures the mask is not shared.
A copy of the mask is only made if it was shared.

See Also
--------
sharedmask
*)

val var : ?axis:int list -> ?dtype:Np.Dtype.t -> ?out:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?keepdims:bool -> [> tag] Obj.t -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the variance along the specified axis.

Returns the variance of the array elements, a measure of the spread of a
distribution.  The variance is computed for the flattened array by
default, otherwise over the specified axis.

Parameters
----------
a : array_like
    Array containing numbers whose variance is desired.  If `a` is not an
    array, a conversion is attempted.
axis : None or int or tuple of ints, optional
    Axis or axes along which the variance is computed.  The default is to
    compute the variance of the flattened array.

    .. versionadded:: 1.7.0

    If this is a tuple of ints, a variance is performed over multiple axes,
    instead of a single axis or all the axes as before.
dtype : data-type, optional
    Type to use in computing the variance.  For arrays of integer type
    the default is `float64`; for arrays of float types it is the same as
    the array type.
out : ndarray, optional
    Alternate output array in which to place the result.  It must have
    the same shape as the expected output, but the type is cast if
    necessary.
ddof : int, optional
    'Delta Degrees of Freedom': the divisor used in the calculation is
    ``N - ddof``, where ``N`` represents the number of elements. By
    default `ddof` is zero.
keepdims : bool, optional
    If this is set to True, the axes which are reduced are left
    in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

    If the default value is passed, then `keepdims` will not be
    passed through to the `var` method of sub-classes of
    `ndarray`, however any non-default value will be.  If the
    sub-class' method does not implement `keepdims` any
    exceptions will be raised.

Returns
-------
variance : ndarray, see dtype parameter above
    If ``out=None``, returns a new array containing the variance;
    otherwise, a reference to the output array is returned.

See Also
--------
std, mean, nanmean, nanstd, nanvar
ufuncs-output-type

Notes
-----
The variance is the average of the squared deviations from the mean,
i.e.,  ``var = mean(abs(x - x.mean())**2)``.

The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
If, however, `ddof` is specified, the divisor ``N - ddof`` is used
instead.  In standard statistical practice, ``ddof=1`` provides an
unbiased estimator of the variance of a hypothetical infinite population.
``ddof=0`` provides a maximum likelihood estimate of the variance for
normally distributed variables.

Note that for complex numbers, the absolute value is taken before
squaring, so that the result is always real and nonnegative.

For floating-point input, the variance is computed using the same
precision the input has.  Depending on the input data, this can cause
the results to be inaccurate, especially for `float32` (see example
below).  Specifying a higher-accuracy accumulator using the ``dtype``
keyword can alleviate this issue.

Examples
--------
>>> a = np.array([[1, 2], [3, 4]])
>>> np.var(a)
1.25
>>> np.var(a, axis=0)
array([1.,  1.])
>>> np.var(a, axis=1)
array([0.25,  0.25])

In single precision, var() can be inaccurate:

>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.var(a)
0.20250003

Computing the variance in float64 is more accurate:

>>> np.var(a, dtype=np.float64)
0.20249999932944759 # may vary
>>> ((1-0.55)**2 + (0.1-0.55)**2)/2
0.2025
*)

val view : ?dtype:[`Ndarray_sub_class of Py.Object.t | `Dtype of Np.Dtype.t] -> ?type_:Py.Object.t -> ?fill_value:[`Bool of bool | `S of string | `I of int | `F of float] -> [> tag] Obj.t -> Py.Object.t
(**
Return a view of the MaskedArray data.

Parameters
----------
dtype : data-type or ndarray sub-class, optional
    Data-type descriptor of the returned view, e.g., float32 or int16.
    The default, None, results in the view having the same data-type
    as `a`. As with ``ndarray.view``, dtype can also be specified as
    an ndarray sub-class, which then specifies the type of the
    returned object (this is equivalent to setting the ``type``
    parameter).
type : Python type, optional
    Type of the returned view, either ndarray or a subclass.  The
    default None results in type preservation.
fill_value : scalar, optional
    The value to use for invalid entries (None by default).
    If None, then this argument is inferred from the passed `dtype`, or
    in its absence the original array, as discussed in the notes below.

See Also
--------
numpy.ndarray.view : Equivalent method on ndarray object.

Notes
-----

``a.view()`` is used two different ways:

``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view
of the array's memory with a different data-type.  This can cause a
reinterpretation of the bytes of memory.

``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just
returns an instance of `ndarray_subclass` that looks at the same array
(same shape, dtype, etc.)  This does not cause a reinterpretation of the
memory.

If `fill_value` is not specified, but `dtype` is specified (and is not
an ndarray sub-class), the `fill_value` of the MaskedArray will be
reset. If neither `fill_value` nor `dtype` are specified (or if
`dtype` is an ndarray sub-class), then the fill value is preserved.
Finally, if `fill_value` is specified, but `dtype` is not, the fill
value is set to the specified value.

For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of
bytes per entry than the previous dtype (for example, converting a
regular array to a structured array), then the behavior of the view
cannot be predicted just from the superficial appearance of ``a`` (shown
by ``print(a)``). It also depends on exactly how ``a`` is stored in
memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus
defined as a slice or transpose, etc., the view may give different
results.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

val beta : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Beta_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A beta continuous random variable.

As an instance of the `rv_continuous` class, `beta` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `beta` is:

.. math::

    f(x, a, b) = \frac{\Gamma(a+b) x^{a-1} (1-x)^{b-1}}
                      {\Gamma(a) \Gamma(b)}

for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`beta` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``beta.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import beta
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 2.31, 0.627
>>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(beta.ppf(0.01, a, b),
...                 beta.ppf(0.99, a, b), 100)
>>> ax.plot(x, beta.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='beta pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = beta(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))
True

Generate random numbers:

>>> r = beta.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val binom : ?loc:float -> n:Py.Object.t -> p:Py.Object.t -> unit -> [`Binom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A binomial discrete random variable.

As an instance of the `rv_discrete` class, `binom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, p, loc=0)
    Probability mass function.
logpmf(k, n, p, loc=0)
    Log of the probability mass function.
cdf(k, n, p, loc=0)
    Cumulative distribution function.
logcdf(k, n, p, loc=0)
    Log of the cumulative distribution function.
sf(k, n, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, p, loc=0)
    Log of the survival function.
ppf(q, n, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, p, loc=0)
    Median of the distribution.
mean(n, p, loc=0)
    Mean of the distribution.
var(n, p, loc=0)
    Variance of the distribution.
std(n, p, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `binom` is:

.. math::

   f(k) = \binom{n}{k} p^k (1-p)^{n-k}

for ``k`` in ``{0, 1,..., n}``.

`binom` takes ``n`` and ``p`` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``binom.pmf(k, n, p, loc)`` is identically
equivalent to ``binom.pmf(k - loc, n, p)``.

Examples
--------
>>> from scipy.stats import binom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, p = 5, 0.4
>>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(binom.ppf(0.01, n, p),
...               binom.ppf(0.99, n, p))
>>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
>>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = binom(n, p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = binom.cdf(x, n, p)
>>> np.allclose(x, binom.ppf(prob, n, p))
True

Generate random numbers:

>>> r = binom.rvs(n, p, size=1000)
*)

val compare_medians_ms : ?axis:int -> group_1:[>`Ndarray] Np.Obj.t -> group_2:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compares the medians from two independent groups along the given axis.

The comparison is performed using the McKean-Schrader estimate of the
standard error of the medians.

Parameters
----------
group_1 : array_like
    First dataset.  Has to be of size >=7.
group_2 : array_like
    Second dataset.  Has to be of size >=7.
axis : int, optional
    Axis along which the medians are estimated. If None, the arrays are
    flattened.  If `axis` is not None, then `group_1` and `group_2`
    should have the same shape.

Returns
-------
compare_medians_ms : {float, ndarray}
    If `axis` is None, then returns a float, otherwise returns a 1-D
    ndarray of floats with a length equal to the length of `group_1`
    along `axis`.
*)

val hdmedian : ?axis:int -> ?var:bool -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns the Harrell-Davis estimate of the median along the given axis.

Parameters
----------
data : ndarray
    Data array.
axis : int, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
var : bool, optional
    Whether to return the variance of the estimate.

Returns
-------
hdmedian : MaskedArray
    The median values.  If ``var=True``, the variance is returned inside
    the masked array.  E.g. for a 1-D array the shape change from (1,) to
    (2,).
*)

val hdquantiles : ?prob:Py.Object.t -> ?axis:int -> ?var:bool -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes quantile estimates with the Harrell-Davis method.

The quantile estimates are calculated as a weighted linear combination
of order statistics.

Parameters
----------
data : array_like
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
var : bool, optional
    Whether to return the variance of the estimate.

Returns
-------
hdquantiles : MaskedArray
    A (p,) array of quantiles (if `var` is False), or a (2,p) array of
    quantiles and variances (if `var` is True), where ``p`` is the
    number of quantiles.

See Also
--------
hdquantiles_sd
*)

val hdquantiles_sd : ?prob:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
The standard error of the Harrell-Davis quantile estimates by jackknife.

Parameters
----------
data : array_like
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.

Returns
-------
hdquantiles_sd : MaskedArray
    Standard error of the Harrell-Davis quantile estimates.

See Also
--------
hdquantiles
*)

val idealfourths : ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns an estimate of the lower and upper quartiles.

Uses the ideal fourths algorithm.

Parameters
----------
data : array_like
    Input array.
axis : int, optional
    Axis along which the quartiles are estimated. If None, the arrays are
    flattened.

Returns
-------
idealfourths : {list of floats, masked array}
    Returns the two internal values that divide `data` into four parts
    using the ideal fourths algorithm either along the flattened array
    (if `axis` is None) or along `axis` of `data`.
*)

val median_cihs : ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Computes the alpha-level confidence interval for the median of the data.

Uses the Hettmasperger-Sheather method.

Parameters
----------
data : array_like
    Input data. Masked values are discarded. The input should be 1D only,
    or `axis` should be set to None.
alpha : float, optional
    Confidence level of the intervals.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.

Returns
-------
median_cihs
    Alpha level confidence interval.
*)

val mjci : ?prob:Py.Object.t -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Returns the Maritz-Jarrett estimators of the standard error of selected
experimental quantiles of the data.

Parameters
----------
data : ndarray
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
axis : int or None, optional
    Axis along which to compute the quantiles. If None, use a flattened
    array.
*)

val mquantiles_cimj : ?prob:Py.Object.t -> ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Computes the alpha confidence interval for the selected quantiles of the
data, with Maritz-Jarrett estimators.

Parameters
----------
data : ndarray
    Data array.
prob : sequence, optional
    Sequence of quantiles to compute.
alpha : float, optional
    Confidence level of the intervals.
axis : int or None, optional
    Axis along which to compute the quantiles.
    If None, use a flattened array.

Returns
-------
ci_lower : ndarray
    The lower boundaries of the confidence interval.  Of the same length as
    `prob`.
ci_upper : ndarray
    The upper boundaries of the confidence interval.  Of the same length as
    `prob`.
*)

val norm : ?loc:float -> ?scale:float -> unit -> [`Norm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A normal continuous random variable.

The location (``loc``) keyword specifies the mean.
The scale (``scale``) keyword specifies the standard deviation.

As an instance of the `rv_continuous` class, `norm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `norm` is:

.. math::

    f(x) = \frac{\exp(-x^2/2)}{\sqrt{2\pi}}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``norm.pdf(x, loc, scale)`` is identically
equivalent to ``norm.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import norm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = norm.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(norm.ppf(0.01),
...                 norm.ppf(0.99), 100)
>>> ax.plot(x, norm.pdf(x),
...        'r-', lw=5, alpha=0.6, label='norm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = norm()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = norm.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))
True

Generate random numbers:

>>> r = norm.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val rsh : ?points:Py.Object.t -> data:Py.Object.t -> unit -> Py.Object.t
(**
Evaluates Rosenblatt's shifted histogram estimators for each data point.

Rosenblatt's estimator is a centered finite-difference approximation to the
derivative of the empirical cumulative distribution function.

Parameters
----------
data : sequence
    Input data, should be 1-D. Masked values are ignored.
points : sequence or None, optional
    Sequence of points where to evaluate Rosenblatt shifted histogram.
    If None, use the data.
*)

val t : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`T_gen] Np.Obj.t
(**
A Student's t continuous random variable.

As an instance of the `rv_continuous` class, `t` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `t` is:

.. math::

    f(x, \nu) = \frac{\Gamma((\nu+1)/2)}
                    {\sqrt{\pi \nu} \Gamma(\nu/2)}
                (1+x^2/\nu)^{-(\nu+1)/2}

where :math:`x` is a real number and the degrees of freedom parameter
:math:`\nu` (denoted ``df`` in the implementation) satisfies
:math:`\nu > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``t.pdf(x, df, loc, scale)`` is identically
equivalent to ``t.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import t
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 2.74
>>> mean, var, skew, kurt = t.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(t.ppf(0.01, df),
...                 t.ppf(0.99, df), 100)
>>> ax.plot(x, t.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='t pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = t(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = t.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))
True

Generate random numbers:

>>> r = t.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val trimmed_mean_ci : ?limits:[`Tuple of Py.Object.t | `None] -> ?inclusive:Py.Object.t -> ?alpha:float -> ?axis:int -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Selected confidence interval of the trimmed mean along the given axis.

Parameters
----------
data : array_like
    Input data.
limits : {None, tuple}, optional
    None or a two item tuple.
    Tuple of the percentages to cut on each side of the array, with respect
    to the number of unmasked data, as floats between 0. and 1. If ``n``
    is the number of unmasked data before trimming, then
    (``n * limits[0]``)th smallest data and (``n * limits[1]``)th
    largest data are masked.  The total number of unmasked data after
    trimming is ``n * (1. - sum(limits))``.
    The value of one limit can be set to None to indicate an open interval.

    Defaults to (0.2, 0.2).
inclusive : (2,) tuple of boolean, optional
    If relative==False, tuple indicating whether values exactly equal to
    the absolute limits are allowed.
    If relative==True, tuple indicating whether the number of data being
    masked on each side should be rounded (True) or truncated (False).

    Defaults to (True, True).
alpha : float, optional
    Confidence level of the intervals.

    Defaults to 0.05.
axis : int, optional
    Axis along which to cut. If None, uses a flattened version of `data`.

    Defaults to None.

Returns
-------
trimmed_mean_ci : (2,) ndarray
    The lower and upper confidence intervals of the trimmed data.
*)


end

module Mvn : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t


end

module Statlib : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t


end

module Stats : sig
(** Get an attribute of this module as a Py.Object.t.
                   This is useful to pass a Python function to another function. *)
val get_py : string -> Py.Object.t

module BrunnerMunzelResult : sig
type tag = [`BrunnerMunzelResult]
type t = [`BrunnerMunzelResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
BrunnerMunzelResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module CumfreqResult : sig
type tag = [`CumfreqResult]
type t = [`CumfreqResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : cumcount:Py.Object.t -> lowerlimit:Py.Object.t -> binsize:Py.Object.t -> extrapoints:Py.Object.t -> unit -> t
(**
CumfreqResult(cumcount, lowerlimit, binsize, extrapoints)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module DescribeResult : sig
type tag = [`DescribeResult]
type t = [`DescribeResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : nobs:Py.Object.t -> minmax:Py.Object.t -> mean:Py.Object.t -> variance:Py.Object.t -> skewness:Py.Object.t -> kurtosis:Py.Object.t -> unit -> t
(**
DescribeResult(nobs, minmax, mean, variance, skewness, kurtosis)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module F_onewayResult : sig
type tag = [`F_onewayResult]
type t = [`F_onewayResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
F_onewayResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module FriedmanchisquareResult : sig
type tag = [`FriedmanchisquareResult]
type t = [`FriedmanchisquareResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
FriedmanchisquareResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module HistogramResult : sig
type tag = [`HistogramResult]
type t = [`HistogramResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : count:Py.Object.t -> lowerlimit:Py.Object.t -> binsize:Py.Object.t -> extrapoints:Py.Object.t -> unit -> t
(**
HistogramResult(count, lowerlimit, binsize, extrapoints)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KendalltauResult : sig
type tag = [`KendalltauResult]
type t = [`KendalltauResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KendalltauResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KruskalResult : sig
type tag = [`KruskalResult]
type t = [`KruskalResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KruskalResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ks_2sampResult : sig
type tag = [`Ks_2sampResult]
type t = [`Ks_2sampResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ks_2sampResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KstestResult : sig
type tag = [`KstestResult]
type t = [`KstestResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KstestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module KurtosistestResult : sig
type tag = [`KurtosistestResult]
type t = [`KurtosistestResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
KurtosistestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module MGCResult : sig
type tag = [`MGCResult]
type t = [`MGCResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : stat:Py.Object.t -> pvalue:Py.Object.t -> mgc_dict:Py.Object.t -> unit -> t
(**
MGCResult(stat, pvalue, mgc_dict)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module MannwhitneyuResult : sig
type tag = [`MannwhitneyuResult]
type t = [`MannwhitneyuResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
MannwhitneyuResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module MapWrapper : sig
type tag = [`MapWrapper]
type t = [`MapWrapper | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : ?pool:[`Map_like_callable of Py.Object.t | `I of int] -> unit -> t
(**
Parallelisation wrapper for working with map-like callables, such as
`multiprocessing.Pool.map`.

Parameters
----------
pool : int or map-like callable
    If `pool` is an integer, then it specifies the number of threads to
    use for parallelization. If ``int(pool) == 1``, then no parallel
    processing is used and the map builtin is used.
    If ``pool == -1``, then the pool will utilise all available CPUs.
    If `pool` is a map-like callable that follows the same
    calling sequence as the built-in map function, then this callable is
    used for parallelisation.
*)

val close : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val join : [> tag] Obj.t -> Py.Object.t
(**
None
*)

val terminate : [> tag] Obj.t -> Py.Object.t
(**
None
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module ModeResult : sig
type tag = [`ModeResult]
type t = [`ModeResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : mode:Py.Object.t -> count:Py.Object.t -> unit -> t
(**
ModeResult(mode, count)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module NormaltestResult : sig
type tag = [`NormaltestResult]
type t = [`NormaltestResult | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
NormaltestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module NumpyVersion : sig
type tag = [`NumpyVersion]
type t = [`NumpyVersion | `Object] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : string -> t
(**
Parse and compare numpy version strings.

Numpy has the following versioning scheme (numbers given are examples; they
can be >9) in principle):

- Released version: '1.8.0', '1.8.1', etc.
- Alpha: '1.8.0a1', '1.8.0a2', etc.
- Beta: '1.8.0b1', '1.8.0b2', etc.
- Release candidates: '1.8.0rc1', '1.8.0rc2', etc.
- Development versions: '1.8.0.dev-f1234afa' (git commit hash appended)
- Development versions after a1: '1.8.0a1.dev-f1234afa',
                                 '1.8.0b2.dev-f1234afa',
                                 '1.8.1rc1.dev-f1234afa', etc.
- Development versions (no git hash available): '1.8.0.dev-Unknown'

Comparing needs to be done against a valid version string or other
`NumpyVersion` instance.

Parameters
----------
vstring : str
    Numpy version string (``np.__version__``).

Notes
-----
All dev versions of the same (pre-)release compare equal.

Examples
--------
>>> from scipy._lib._version import NumpyVersion
>>> if NumpyVersion(np.__version__) < '1.7.0':
...     print('skip')
skip

>>> NumpyVersion('1.7')  # raises ValueError, add '.0'
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module PointbiserialrResult : sig
type tag = [`PointbiserialrResult]
type t = [`Object | `PointbiserialrResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
PointbiserialrResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Power_divergenceResult : sig
type tag = [`Power_divergenceResult]
type t = [`Object | `Power_divergenceResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Power_divergenceResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module RanksumsResult : sig
type tag = [`RanksumsResult]
type t = [`Object | `RanksumsResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
RanksumsResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module RelfreqResult : sig
type tag = [`RelfreqResult]
type t = [`Object | `RelfreqResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : frequency:Py.Object.t -> lowerlimit:Py.Object.t -> binsize:Py.Object.t -> extrapoints:Py.Object.t -> unit -> t
(**
RelfreqResult(frequency, lowerlimit, binsize, extrapoints)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module RepeatedResults : sig
type tag = [`RepeatedResults]
type t = [`Object | `RepeatedResults] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : values:Py.Object.t -> counts:Py.Object.t -> unit -> t
(**
RepeatedResults(values, counts)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module SigmaclipResult : sig
type tag = [`SigmaclipResult]
type t = [`Object | `SigmaclipResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : clipped:Py.Object.t -> lower:Py.Object.t -> upper:Py.Object.t -> unit -> t
(**
SigmaclipResult(clipped, lower, upper)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module SkewtestResult : sig
type tag = [`SkewtestResult]
type t = [`Object | `SkewtestResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
SkewtestResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module SpearmanrResult : sig
type tag = [`SpearmanrResult]
type t = [`Object | `SpearmanrResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
SpearmanrResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_1sampResult : sig
type tag = [`Ttest_1sampResult]
type t = [`Object | `Ttest_1sampResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_1sampResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_indResult : sig
type tag = [`Ttest_indResult]
type t = [`Object | `Ttest_indResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_indResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module Ttest_relResult : sig
type tag = [`Ttest_relResult]
type t = [`Object | `Ttest_relResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : statistic:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
Ttest_relResult(statistic, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

module WeightedTauResult : sig
type tag = [`WeightedTauResult]
type t = [`Object | `WeightedTauResult] Obj.t
val of_pyobject : Py.Object.t -> t
val to_pyobject : [> tag] Obj.t -> Py.Object.t

val create : correlation:Py.Object.t -> pvalue:Py.Object.t -> unit -> t
(**
WeightedTauResult(correlation, pvalue)
*)

val __getitem__ : key:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return self[key].
*)

val __iter__ : [> tag] Obj.t -> Py.Object.t
(**
Implement iter(self).
*)

val count : value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return number of occurrences of value.
*)

val index : ?start:Py.Object.t -> ?stop:Py.Object.t -> value:Py.Object.t -> [> tag] Obj.t -> Py.Object.t
(**
Return first index of value.

Raises ValueError if the value is not present.
*)


(** Print the object to a human-readable representation. *)
val to_string : t -> string


(** Print the object to a human-readable representation. *)
val show : t -> string

(** Pretty-print the object to a formatter. *)
val pp : Format.formatter -> t -> unit [@@ocaml.toplevel_printer]


end

val array : ?dtype:Np.Dtype.t -> ?copy:bool -> ?order:[`K | `A | `C | `F] -> ?subok:bool -> ?ndmin:int -> object_:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0)

Create an array.

Parameters
----------
object : array_like
    An array, any object exposing the array interface, an object whose
    __array__ method returns an array, or any (nested) sequence.
dtype : data-type, optional
    The desired data-type for the array.  If not given, then the type will
    be determined as the minimum type required to hold the objects in the
    sequence.
copy : bool, optional
    If true (default), then the object is copied.  Otherwise, a copy will
    only be made if __array__ returns a copy, if obj is a nested sequence,
    or if a copy is needed to satisfy any of the other requirements
    (`dtype`, `order`, etc.).
order : {'K', 'A', 'C', 'F'}, optional
    Specify the memory layout of the array. If object is not an array, the
    newly created array will be in C order (row major) unless 'F' is
    specified, in which case it will be in Fortran order (column major).
    If object is an array the following holds.

    ===== ========= ===================================================
    order  no copy                     copy=True
    ===== ========= ===================================================
    'K'   unchanged F & C order preserved, otherwise most similar order
    'A'   unchanged F order if input is F and not C, otherwise C order
    'C'   C order   C order
    'F'   F order   F order
    ===== ========= ===================================================

    When ``copy=False`` and a copy is made for other reasons, the result is
    the same as if ``copy=True``, with some exceptions for `A`, see the
    Notes section. The default order is 'K'.
subok : bool, optional
    If True, then sub-classes will be passed-through, otherwise
    the returned array will be forced to be a base-class array (default).
ndmin : int, optional
    Specifies the minimum number of dimensions that the resulting
    array should have.  Ones will be pre-pended to the shape as
    needed to meet this requirement.

Returns
-------
out : ndarray
    An array object satisfying the specified requirements.

See Also
--------
empty_like : Return an empty array with shape and type of input.
ones_like : Return an array of ones with shape and type of input.
zeros_like : Return an array of zeros with shape and type of input.
full_like : Return a new array with shape of input filled with value.
empty : Return a new uninitialized array.
ones : Return a new array setting values to one.
zeros : Return a new array setting values to zero.
full : Return a new array of given shape filled with value.


Notes
-----
When order is 'A' and `object` is an array in neither 'C' nor 'F' order,
and a copy is forced by a change in dtype, then the order of the result is
not necessarily 'C' as expected. This is likely a bug.

Examples
--------
>>> np.array([1, 2, 3])
array([1, 2, 3])

Upcasting:

>>> np.array([1, 2, 3.0])
array([ 1.,  2.,  3.])

More than one dimension:

>>> np.array([[1, 2], [3, 4]])
array([[1, 2],
       [3, 4]])

Minimum dimensions 2:

>>> np.array([1, 2, 3], ndmin=2)
array([[1, 2, 3]])

Type provided:

>>> np.array([1, 2, 3], dtype=complex)
array([ 1.+0.j,  2.+0.j,  3.+0.j])

Data-type consisting of more than one element:

>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])
>>> x['a']
array([1, 3])

Creating an array from sub-classes:

>>> np.array(np.mat('1 2; 3 4'))
array([[1, 2],
       [3, 4]])

>>> np.array(np.mat('1 2; 3 4'), subok=True)
matrix([[1, 2],
        [3, 4]])
*)

val asarray : ?dtype:Np.Dtype.t -> ?order:[`F | `C] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Convert the input to an array.

Parameters
----------
a : array_like
    Input data, in any form that can be converted to an array.  This
    includes lists, lists of tuples, tuples, tuples of tuples, tuples
    of lists and ndarrays.
dtype : data-type, optional
    By default, the data-type is inferred from the input data.
order : {'C', 'F'}, optional
    Whether to use row-major (C-style) or
    column-major (Fortran-style) memory representation.
    Defaults to 'C'.

Returns
-------
out : ndarray
    Array interpretation of `a`.  No copy is performed if the input
    is already an ndarray with matching dtype and order.  If `a` is a
    subclass of ndarray, a base class ndarray is returned.

See Also
--------
asanyarray : Similar function which passes through subclasses.
ascontiguousarray : Convert input to a contiguous array.
asfarray : Convert input to a floating point ndarray.
asfortranarray : Convert input to an ndarray with column-major
                 memory order.
asarray_chkfinite : Similar function which checks input for NaNs and Infs.
fromiter : Create an array from an iterator.
fromfunction : Construct an array by executing a function on grid
               positions.

Examples
--------
Convert a list into an array:

>>> a = [1, 2]
>>> np.asarray(a)
array([1, 2])

Existing arrays are not copied:

>>> a = np.array([1, 2])
>>> np.asarray(a) is a
True

If `dtype` is set, array is copied only if dtype does not match:

>>> a = np.array([1, 2], dtype=np.float32)
>>> np.asarray(a, dtype=np.float32) is a
True
>>> np.asarray(a, dtype=np.float64) is a
False

Contrary to `asanyarray`, ndarray subclasses are not passed through:

>>> issubclass(np.recarray, np.ndarray)
True
>>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)
>>> np.asarray(a) is a
False
>>> np.asanyarray(a) is a
True
*)

val brunnermunzel : ?alternative:[`Two_sided | `Less | `Greater] -> ?distribution:[`T | `Normal] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Brunner-Munzel test on samples x and y.

The Brunner-Munzel test is a nonparametric test of the null hypothesis that
when values are taken one by one from each group, the probabilities of
getting large values in both groups are equal.
Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the
assumption of equivariance of two groups. Note that this does not assume
the distributions are same. This test works on two independent samples,
which may have different sizes.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided
distribution : {'t', 'normal'}, optional
    Defines how to get the p-value.
    The following options are available (default is 't'):

      * 't': get the p-value by t-distribution
      * 'normal': get the p-value by standard normal distribution.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The Brunner-Munzer W statistic.
pvalue : float
    p-value assuming an t distribution. One-sided or
    two-sided, depending on the choice of `alternative` and `distribution`.

See Also
--------
mannwhitneyu : Mann-Whitney rank test on two samples.

Notes
-----
Brunner and Munzel recommended to estimate the p-value by t-distribution
when the size of data is 50 or less. If the size is lower than 10, it would
be better to use permuted Brunner Munzel test (see [2]_).

References
----------
.. [1] Brunner, E. and Munzel, U. 'The nonparametric Benhrens-Fisher
       problem: Asymptotic theory and a small-sample approximation'.
       Biometrical Journal. Vol. 42(2000): 17-25.
.. [2] Neubert, K. and Brunner, E. 'A studentized permutation test for the
       non-parametric Behrens-Fisher problem'. Computational Statistics and
       Data Analysis. Vol. 51(2007): 5192-5204.

Examples
--------
>>> from scipy import stats
>>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]
>>> x2 = [3,3,4,3,1,2,3,1,1,5,4]
>>> w, p_value = stats.brunnermunzel(x1, x2)
>>> w
3.1374674823029505
>>> p_value
0.0057862086661515377
*)

val callable : Py.Object.t -> Py.Object.t
(**
None
*)

val cdist : ?metric:[`Callable of Py.Object.t | `S of string] -> ?kwargs:(string * Py.Object.t) list -> xa:[>`Ndarray] Np.Obj.t -> xb:[>`Ndarray] Np.Obj.t -> Py.Object.t list -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute distance between each pair of the two collections of inputs.

See Notes for common calling conventions.

Parameters
----------
XA : ndarray
    An :math:`m_A` by :math:`n` array of :math:`m_A`
    original observations in an :math:`n`-dimensional space.
    Inputs are converted to float type.
XB : ndarray
    An :math:`m_B` by :math:`n` array of :math:`m_B`
    original observations in an :math:`n`-dimensional space.
    Inputs are converted to float type.
metric : str or callable, optional
    The distance metric to use.  If a string, the distance function can be
    'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation',
    'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'jensenshannon',
    'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
    'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',
    'wminkowski', 'yule'.
*args : tuple. Deprecated.
    Additional arguments should be passed as keyword arguments
**kwargs : dict, optional
    Extra arguments to `metric`: refer to each metric documentation for a
    list of all possible arguments.

    Some possible arguments:

    p : scalar
    The p-norm to apply for Minkowski, weighted and unweighted.
    Default: 2.

    w : ndarray
    The weight vector for metrics that support weights (e.g., Minkowski).

    V : ndarray
    The variance vector for standardized Euclidean.
    Default: var(vstack([XA, XB]), axis=0, ddof=1)

    VI : ndarray
    The inverse of the covariance matrix for Mahalanobis.
    Default: inv(cov(vstack([XA, XB].T))).T

    out : ndarray
    The output array
    If not None, the distance matrix Y is stored in this array.
    Note: metric independent, it will become a regular keyword arg in a
    future scipy version

Returns
-------
Y : ndarray
    A :math:`m_A` by :math:`m_B` distance matrix is returned.
    For each :math:`i` and :math:`j`, the metric
    ``dist(u=XA[i], v=XB[j])`` is computed and stored in the
    :math:`ij` th entry.

Raises
------
ValueError
    An exception is thrown if `XA` and `XB` do not have
    the same number of columns.

Notes
-----
The following are common calling conventions:

1. ``Y = cdist(XA, XB, 'euclidean')``

   Computes the distance between :math:`m` points using
   Euclidean distance (2-norm) as the distance metric between the
   points. The points are arranged as :math:`m`
   :math:`n`-dimensional row vectors in the matrix X.

2. ``Y = cdist(XA, XB, 'minkowski', p=2.)``

   Computes the distances using the Minkowski distance
   :math:`||u-v||_p` (:math:`p`-norm) where :math:`p \geq 1`.

3. ``Y = cdist(XA, XB, 'cityblock')``

   Computes the city block or Manhattan distance between the
   points.

4. ``Y = cdist(XA, XB, 'seuclidean', V=None)``

   Computes the standardized Euclidean distance. The standardized
   Euclidean distance between two n-vectors ``u`` and ``v`` is

   .. math::

      \sqrt{\sum {(u_i-v_i)^2 / V[x_i]}}.

   V is the variance vector; V[i] is the variance computed over all
   the i'th components of the points. If not passed, it is
   automatically computed.

5. ``Y = cdist(XA, XB, 'sqeuclidean')``

   Computes the squared Euclidean distance :math:`||u-v||_2^2` between
   the vectors.

6. ``Y = cdist(XA, XB, 'cosine')``

   Computes the cosine distance between vectors u and v,

   .. math::

      1 - \frac{u \cdot v}
               {{ ||u|| }_2 { ||v|| }_2}

   where :math:`||*||_2` is the 2-norm of its argument ``*``, and
   :math:`u \cdot v` is the dot product of :math:`u` and :math:`v`.

7. ``Y = cdist(XA, XB, 'correlation')``

   Computes the correlation distance between vectors u and v. This is

   .. math::

      1 - \frac{(u - \bar{u}) \cdot (v - \bar{v})}
               {{ ||(u - \bar{u})|| }_2 { ||(v - \bar{v})|| }_2}

   where :math:`\bar{v}` is the mean of the elements of vector v,
   and :math:`x \cdot y` is the dot product of :math:`x` and :math:`y`.


8. ``Y = cdist(XA, XB, 'hamming')``

   Computes the normalized Hamming distance, or the proportion of
   those vector elements between two n-vectors ``u`` and ``v``
   which disagree. To save memory, the matrix ``X`` can be of type
   boolean.

9. ``Y = cdist(XA, XB, 'jaccard')``

   Computes the Jaccard distance between the points. Given two
   vectors, ``u`` and ``v``, the Jaccard distance is the
   proportion of those elements ``u[i]`` and ``v[i]`` that
   disagree where at least one of them is non-zero.

10. ``Y = cdist(XA, XB, 'chebyshev')``

   Computes the Chebyshev distance between the points. The
   Chebyshev distance between two n-vectors ``u`` and ``v`` is the
   maximum norm-1 distance between their respective elements. More
   precisely, the distance is given by

   .. math::

      d(u,v) = \max_i { |u_i-v_i| }.

11. ``Y = cdist(XA, XB, 'canberra')``

   Computes the Canberra distance between the points. The
   Canberra distance between two points ``u`` and ``v`` is

   .. math::

     d(u,v) = \sum_i \frac{ |u_i-v_i| }
                          { |u_i|+|v_i| }.

12. ``Y = cdist(XA, XB, 'braycurtis')``

   Computes the Bray-Curtis distance between the points. The
   Bray-Curtis distance between two points ``u`` and ``v`` is


   .. math::

        d(u,v) = \frac{\sum_i (|u_i-v_i|)}
                      {\sum_i (|u_i+v_i|)}

13. ``Y = cdist(XA, XB, 'mahalanobis', VI=None)``

   Computes the Mahalanobis distance between the points. The
   Mahalanobis distance between two points ``u`` and ``v`` is
   :math:`\sqrt{(u-v)(1/V)(u-v)^T}` where :math:`(1/V)` (the ``VI``
   variable) is the inverse covariance. If ``VI`` is not None,
   ``VI`` will be used as the inverse covariance matrix.

14. ``Y = cdist(XA, XB, 'yule')``

   Computes the Yule distance between the boolean
   vectors. (see `yule` function documentation)

15. ``Y = cdist(XA, XB, 'matching')``

   Synonym for 'hamming'.

16. ``Y = cdist(XA, XB, 'dice')``

   Computes the Dice distance between the boolean vectors. (see
   `dice` function documentation)

17. ``Y = cdist(XA, XB, 'kulsinski')``

   Computes the Kulsinski distance between the boolean
   vectors. (see `kulsinski` function documentation)

18. ``Y = cdist(XA, XB, 'rogerstanimoto')``

   Computes the Rogers-Tanimoto distance between the boolean
   vectors. (see `rogerstanimoto` function documentation)

19. ``Y = cdist(XA, XB, 'russellrao')``

   Computes the Russell-Rao distance between the boolean
   vectors. (see `russellrao` function documentation)

20. ``Y = cdist(XA, XB, 'sokalmichener')``

   Computes the Sokal-Michener distance between the boolean
   vectors. (see `sokalmichener` function documentation)

21. ``Y = cdist(XA, XB, 'sokalsneath')``

   Computes the Sokal-Sneath distance between the vectors. (see
   `sokalsneath` function documentation)


22. ``Y = cdist(XA, XB, 'wminkowski', p=2., w=w)``

   Computes the weighted Minkowski distance between the
   vectors. (see `wminkowski` function documentation)

23. ``Y = cdist(XA, XB, f)``

   Computes the distance between all pairs of vectors in X
   using the user supplied 2-arity function f. For example,
   Euclidean distance between the vectors could be computed
   as follows::

     dm = cdist(XA, XB, lambda u, v: np.sqrt(((u-v)**2).sum()))

   Note that you should avoid passing a reference to one of
   the distance functions defined in this library. For example,::

     dm = cdist(XA, XB, sokalsneath)

   would calculate the pair-wise distances between the vectors in
   X using the Python function `sokalsneath`. This would result in
   sokalsneath being called :math:`{n \choose 2}` times, which
   is inefficient. Instead, the optimized C version is more
   efficient, and we call it using the following syntax::

     dm = cdist(XA, XB, 'sokalsneath')

Examples
--------
Find the Euclidean distances between four 2-D coordinates:

>>> from scipy.spatial import distance
>>> coords = [(35.0456, -85.2672),
...           (35.1174, -89.9711),
...           (35.9728, -83.9422),
...           (36.1667, -86.7833)]
>>> distance.cdist(coords, coords, 'euclidean')
array([[ 0.    ,  4.7044,  1.6172,  1.8856],
       [ 4.7044,  0.    ,  6.0893,  3.3561],
       [ 1.6172,  6.0893,  0.    ,  2.8477],
       [ 1.8856,  3.3561,  2.8477,  0.    ]])


Find the Manhattan distance from a 3-D point to the corners of the unit
cube:

>>> a = np.array([[0, 0, 0],
...               [0, 0, 1],
...               [0, 1, 0],
...               [0, 1, 1],
...               [1, 0, 0],
...               [1, 0, 1],
...               [1, 1, 0],
...               [1, 1, 1]])
>>> b = np.array([[ 0.1,  0.2,  0.4]])
>>> distance.cdist(a, b, 'cityblock')
array([[ 0.7],
       [ 0.9],
       [ 1.3],
       [ 1.5],
       [ 1.5],
       [ 1.7],
       [ 2.1],
       [ 2.3]])
*)

val check_random_state : Py.Object.t -> Py.Object.t
(**
Turn seed into a np.random.RandomState instance

If seed is None (or np.random), return the RandomState singleton used
by np.random.
If seed is an int, return a new RandomState instance seeded with seed.
If seed is already a RandomState instance, return it.
If seed is a new-style np.random.Generator, return it.
Otherwise raise ValueError.
*)

val chisquare : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate a one-way chi-square test.

The chi-square test tests the null hypothesis that the categorical data
has the given frequencies.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.

Returns
-------
chisq : float or ndarray
    The chi-squared test statistic.  The value is a float if `axis` is
    None or `f_obs` and `f_exp` are 1-D.
p : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `chisq` are scalars.

See Also
--------
scipy.stats.power_divergence

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not chi-square, in which case this test
is not appropriate.

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test

Examples
--------
When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.

>>> from scipy.stats import chisquare
>>> chisquare([16, 18, 16, 14, 12, 12])
(2.0, 0.84914503608460956)

With `f_exp` the expected frequencies can be given.

>>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])
(3.5, 0.62338762774958223)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> chisquare(obs)
(array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> chisquare(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> chisquare(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
chi-squared statistic with `ddof`.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we use ``axis=1``:

>>> chisquare([16, 18, 16, 14, 12, 12],
...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],
...           axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val combine_pvalues : ?method_:[`Fisher | `Pearson | `Tippett | `Stouffer | `Mudholkar_george] -> ?weights:[`Ndarray of [>`Ndarray] Np.Obj.t | `T1_D of Py.Object.t] -> pvalues:[`Ndarray of [>`Ndarray] Np.Obj.t | `T1_D of Py.Object.t] -> unit -> (float * float)
(**
Combine p-values from independent tests bearing upon the same hypothesis.

Parameters
----------
pvalues : array_like, 1-D
    Array of p-values assumed to come from independent tests.
method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}, optional
    Name of method to use to combine p-values.
    The following methods are available (default is 'fisher'):

      * 'fisher': Fisher's method (Fisher's combined probability test), the
        sum of the logarithm of the p-values
      * 'pearson': Pearson's method (similar to Fisher's but uses sum of the
        complement of the p-values inside the logarithms)
      * 'tippett': Tippett's method (minimum of p-values)
      * 'stouffer': Stouffer's Z-score method
      * 'mudholkar_george': the difference of Fisher's and Pearson's methods
        divided by 2
weights : array_like, 1-D, optional
    Optional array of weights used only for Stouffer's Z-score method.

Returns
-------
statistic: float
    The statistic calculated by the specified method.
pval: float
    The combined p-value.

Notes
-----
Fisher's method (also known as Fisher's combined probability test) [1]_ uses
a chi-squared statistic to compute a combined p-value. The closely related
Stouffer's Z-score method [2]_ uses Z-scores rather than p-values. The
advantage of Stouffer's method is that it is straightforward to introduce
weights, which can make Stouffer's method more powerful than Fisher's
method when the p-values are from studies of different size [6]_ [7]_.
The Pearson's method uses :math:`log(1-p_i)` inside the sum whereas Fisher's
method uses :math:`log(p_i)` [4]_. For Fisher's and Pearson's method, the
sum of the logarithms is multiplied by -2 in the implementation. This
quantity has a chi-square distribution that determines the p-value. The
`mudholkar_george` method is the difference of the Fisher's and Pearson's
test statistics, each of which include the -2 factor [4]_. However, the
`mudholkar_george` method does not include these -2 factors. The test
statistic of `mudholkar_george` is the sum of logisitic random variables and
equation 3.6 in [3]_ is used to approximate the p-value based on Student's
t-distribution.

Fisher's method may be extended to combine p-values from dependent tests
[5]_. Extensions such as Brown's method and Kost's method are not currently
implemented.

.. versionadded:: 0.15.0

References
----------
.. [1] https://en.wikipedia.org/wiki/Fisher%27s_method
.. [2] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method
.. [3] George, E. O., and G. S. Mudholkar. 'On the convolution of logistic
       random variables.' Metrika 30.1 (1983): 1-13.
.. [4] Heard, N. and Rubin-Delanchey, P. 'Choosing between methods of
       combining p-values.'  Biometrika 105.1 (2018): 239-246.
.. [5] Whitlock, M. C. 'Combining probability from independent tests: the
       weighted Z-method is superior to Fisher's approach.' Journal of
       Evolutionary Biology 18, no. 5 (2005): 1368-1373.
.. [6] Zaykin, Dmitri V. 'Optimally weighted Z-test is a powerful method
       for combining probabilities in meta-analysis.' Journal of
       Evolutionary Biology 24, no. 8 (2011): 1836-1841.
.. [7] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method
*)

val cumfreq : ?numbins:int -> ?defaultreallimits:Py.Object.t -> ?weights:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float * int)
(**
Return a cumulative frequency histogram, using the histogram function.

A cumulative histogram is a mapping that counts the cumulative number of
observations in all of the bins up to the specified bin.

Parameters
----------
a : array_like
    Input array.
numbins : int, optional
    The number of bins to use for the histogram. Default is 10.
defaultreallimits : tuple (lower, upper), optional
    The lower and upper values for the range of the histogram.
    If no value is given, a range slightly larger than the range of the
    values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,
    where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.
weights : array_like, optional
    The weights for each value in `a`. Default is None, which gives each
    value a weight of 1.0

Returns
-------
cumcount : ndarray
    Binned values of cumulative frequency.
lowerlimit : float
    Lower real limit
binsize : float
    Width of each bin.
extrapoints : int
    Extra points.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats
>>> x = [1, 4, 2, 1, 3, 1]
>>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
>>> res.cumcount
array([ 1.,  2.,  3.,  3.])
>>> res.extrapoints
3

Create a normal distribution with 1000 random values

>>> rng = np.random.RandomState(seed=12345)
>>> samples = stats.norm.rvs(size=1000, random_state=rng)

Calculate cumulative frequencies

>>> res = stats.cumfreq(samples, numbins=25)

Calculate space of values for x

>>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,
...                                  res.cumcount.size)

Plot histogram and cumulative histogram

>>> fig = plt.figure(figsize=(10, 4))
>>> ax1 = fig.add_subplot(1, 2, 1)
>>> ax2 = fig.add_subplot(1, 2, 2)
>>> ax1.hist(samples, bins=25)
>>> ax1.set_title('Histogram')
>>> ax2.bar(x, res.cumcount, width=res.binsize)
>>> ax2.set_title('Cumulative histogram')
>>> ax2.set_xlim([x.min(), x.max()])

>>> plt.show()
*)

val describe : ?axis:[`I of int | `None] -> ?ddof:int -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t)
(**
Compute several descriptive statistics of the passed array.

Parameters
----------
a : array_like
   Input data.
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.
ddof : int, optional
    Delta degrees of freedom (only for variance).  Default is 1.
bias : bool, optional
    If False, then the skewness and kurtosis calculations are corrected for
    statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
nobs : int or ndarray of ints
   Number of observations (length of data along `axis`).
   When 'omit' is chosen as nan_policy, each column is counted separately.
minmax: tuple of ndarrays or floats
   Minimum and maximum value of data array.
mean : ndarray or float
   Arithmetic mean of data along axis.
variance : ndarray or float
   Unbiased variance of the data along axis, denominator is number of
   observations minus one.
skewness : ndarray or float
   Skewness, based on moment calculations with denominator equal to
   the number of observations, i.e. no degrees of freedom correction.
kurtosis : ndarray or float
   Kurtosis (Fisher).  The kurtosis is normalized so that it is
   zero for the normal distribution.  No degrees of freedom are used.

See Also
--------
skew, kurtosis

Examples
--------
>>> from scipy import stats
>>> a = np.arange(10)
>>> stats.describe(a)
DescribeResult(nobs=10, minmax=(0, 9), mean=4.5, variance=9.166666666666666,
               skewness=0.0, kurtosis=-1.2242424242424244)
>>> b = [[1, 2], [3, 4]]
>>> stats.describe(b)
DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),
               mean=array([2., 3.]), variance=array([2., 2.]),
               skewness=array([0., 0.]), kurtosis=array([-2., -2.]))
*)

val energy_distance : ?u_weights:Py.Object.t -> ?v_weights:Py.Object.t -> u_values:Py.Object.t -> v_values:Py.Object.t -> unit -> float
(**
Compute the energy distance between two 1D distributions.

.. versionadded:: 1.0.0

Parameters
----------
u_values, v_values : array_like
    Values observed in the (empirical) distribution.
u_weights, v_weights : array_like, optional
    Weight for each value. If unspecified, each value is assigned the same
    weight.
    `u_weights` (resp. `v_weights`) must have the same length as
    `u_values` (resp. `v_values`). If the weight sum differs from 1, it
    must still be positive and finite so that the weights can be normalized
    to sum to 1.

Returns
-------
distance : float
    The computed distance between the distributions.

Notes
-----
The energy distance between two distributions :math:`u` and :math:`v`, whose
respective CDFs are :math:`U` and :math:`V`, equals to:

.. math::

    D(u, v) = \left( 2\mathbb E|X - Y| - \mathbb E|X - X'| -
    \mathbb E|Y - Y'| \right)^{1/2}

where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are
independent random variables whose probability distribution is :math:`u`
(resp. :math:`v`).

As shown in [2]_, for one-dimensional real-valued variables, the energy
distance is linked to the non-distribution-free version of the Cramer-von
Mises distance:

.. math::

    D(u, v) = \sqrt{2} l_2(u, v) = \left( 2 \int_{-\infty}^{+\infty} (U-V)^2
    \right)^{1/2}

Note that the common Cramer-von Mises criterion uses the distribution-free
version of the distance. See [2]_ (section 2), for more details about both
versions of the distance.

The input distributions can be empirical, therefore coming from samples
whose values are effectively inputs of the function, or they can be seen as
generalized functions, in which case they are weighted sums of Dirac delta
functions located at the specified values.

References
----------
.. [1] 'Energy distance', https://en.wikipedia.org/wiki/Energy_distance
.. [2] Szekely 'E-statistics: The energy of statistical samples.' Bowling
       Green State University, Department of Mathematics and Statistics,
       Technical Report 02-16 (2002).
.. [3] Rizzo, Szekely 'Energy distance.' Wiley Interdisciplinary Reviews:
       Computational Statistics, 8(1):27-38 (2015).
.. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,
       Munos 'The Cramer Distance as a Solution to Biased Wasserstein
       Gradients' (2017). :arXiv:`1705.10743`.

Examples
--------
>>> from scipy.stats import energy_distance
>>> energy_distance([0], [2])
2.0000000000000004
>>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])
1.0000000000000002
>>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],
...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])
0.88003340976158217
*)

val epps_singleton_2samp : ?t:[>`Ndarray] Np.Obj.t -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Epps-Singleton (ES) test statistic.

Test the null hypothesis that two samples have the same underlying
probability distribution.

Parameters
----------
x, y : array-like
    The two samples of observations to be tested. Input must not have more
    than one dimension. Samples can have different lengths.
t : array-like, optional
    The points (t1, ..., tn) where the empirical characteristic function is
    to be evaluated. It should be positive distinct numbers. The default
    value (0.4, 0.8) is proposed in [1]_. Input must not have more than
    one dimension.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The associated p-value based on the asymptotic chi2-distribution.

See Also
--------
ks_2samp, anderson_ksamp

Notes
-----
Testing whether two samples are generated by the same underlying
distribution is a classical question in statistics. A widely used test is
the Kolmogorov-Smirnov (KS) test which relies on the empirical
distribution function. Epps and Singleton introduce a test based on the
empirical characteristic function in [1]_.

One advantage of the ES test compared to the KS test is that is does
not assume a continuous distribution. In [1]_, the authors conclude
that the test also has a higher power than the KS test in many
examples. They recommend the use of the ES test for discrete samples as
well as continuous samples with at least 25 observations each, whereas
`anderson_ksamp` is recommended for smaller sample sizes in the
continuous case.

The p-value is computed from the asymptotic distribution of the test
statistic which follows a `chi2` distribution. If the sample size of both
`x` and `y` is below 25, the small sample correction proposed in [1]_ is
applied to the test statistic.

The default values of `t` are determined in [1]_ by considering
various distributions and finding good values that lead to a high power
of the test in general. Table III in [1]_ gives the optimal values for
the distributions tested in that study. The values of `t` are scaled by
the semi-interquartile range in the implementation, see [1]_.

References
----------
.. [1] T. W. Epps and K. J. Singleton, 'An omnibus test for the two-sample
   problem using the empirical characteristic function', Journal of
   Statistical Computation and Simulation 26, p. 177--203, 1986.

.. [2] S. J. Goerg and J. Kaiser, 'Nonparametric testing of distributions
   - the Epps-Singleton two-sample test using the empirical characteristic
   function', The Stata Journal 9(3), p. 454--465, 2009.
*)

val f_oneway : Py.Object.t list -> (float * float)
(**
Perform one-way ANOVA.

The one-way ANOVA tests the null hypothesis that two or more groups have
the same population mean.  The test is applied to samples from two or
more groups, possibly with differing sizes.

Parameters
----------
sample1, sample2, ... : array_like
    The sample measurements for each group.

Returns
-------
statistic : float
    The computed F-value of the test.
pvalue : float
    The associated p-value from the F-distribution.

Notes
-----
The ANOVA test has important assumptions that must be satisfied in order
for the associated p-value to be valid.

1. The samples are independent.
2. Each sample is from a normally distributed population.
3. The population standard deviations of the groups are all equal.  This
   property is known as homoscedasticity.

If these assumptions are not true for a given set of data, it may still be
possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) although
with some loss of power.

The algorithm is from Heiman[2], pp.394-7.

References
----------
.. [1] R. Lowry, 'Concepts and Applications of Inferential Statistics',
       Chapter 14, 2014, http://vassarstats.net/textbook/

.. [2] G.W. Heiman, 'Understanding research methods and statistics: An
       integrated introduction for psychology', Houghton, Mifflin and
       Company, 2001.

.. [3] G.H. McDonald, 'Handbook of Biological Statistics', One-way ANOVA.
       http://www.biostathandbook.com/onewayanova.html

Examples
--------
>>> import scipy.stats as stats

[3]_ Here are some data on a shell measurement (the length of the anterior
adductor muscle scar, standardized by dividing by length) in the mussel
Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;
Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a
much larger data set used in McDonald et al. (1991).

>>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,
...              0.0659, 0.0923, 0.0836]
>>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,
...            0.0725]
>>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]
>>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,
...            0.0689]
>>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]
>>> stats.f_oneway(tillamook, newport, petersburg, magadan, tvarminne)
(7.1210194716424473, 0.00028122423145345439)
*)

val find_repeats : [>`Ndarray] Np.Obj.t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find repeats and repeat counts.

Parameters
----------
arr : array_like
    Input array. This is cast to float64.

Returns
-------
values : ndarray
    The unique values from the (flattened) input that are repeated.

counts : ndarray
    Number of times the corresponding 'value' is repeated.

Notes
-----
In numpy >= 1.9 `numpy.unique` provides similar functionality. The main
difference is that `find_repeats` only returns repeated values.

Examples
--------
>>> from scipy import stats
>>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])
RepeatedResults(values=array([2.]), counts=array([4]))

>>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])
RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))
*)

val fisher_exact : ?alternative:[`Two_sided | `Less | `Greater] -> table:Py.Object.t -> unit -> (float * float)
(**
Perform a Fisher exact test on a 2x2 contingency table.

Parameters
----------
table : array_like of ints
    A 2x2 contingency table.  Elements should be non-negative integers.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided

Returns
-------
oddsratio : float
    This is prior odds ratio and not a posterior estimate.
p_value : float
    P-value, the probability of obtaining a distribution at least as
    extreme as the one that was actually observed, assuming that the
    null hypothesis is true.

See Also
--------
chi2_contingency : Chi-square test of independence of variables in a
    contingency table.

Notes
-----
The calculated odds ratio is different from the one R uses. This scipy
implementation returns the (more common) 'unconditional Maximum
Likelihood Estimate', while R uses the 'conditional Maximum Likelihood
Estimate'.

For tables with large numbers, the (inexact) chi-square test implemented
in the function `chi2_contingency` can also be used.

Examples
--------
Say we spend a few days counting whales and sharks in the Atlantic and
Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the
Indian ocean 2 whales and 5 sharks. Then our contingency table is::

            Atlantic  Indian
    whales     8        2
    sharks     1        5

We use this table to find the p-value:

>>> import scipy.stats as stats
>>> oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])
>>> pvalue
0.0349...

The probability that we would observe this or an even more imbalanced ratio
by chance is about 3.5%.  A commonly used significance level is 5%--if we
adopt that, we can therefore conclude that our observed imbalance is
statistically significant; whales prefer the Atlantic while sharks prefer
the Indian ocean.
*)

val friedmanchisquare : Py.Object.t list -> (float * float)
(**
Compute the Friedman test for repeated measurements.

The Friedman test tests the null hypothesis that repeated measurements of
the same individuals have the same distribution.  It is often used
to test for consistency among measurements obtained in different ways.
For example, if two measurement techniques are used on the same set of
individuals, the Friedman test can be used to determine if the two
measurement techniques are consistent.

Parameters
----------
measurements1, measurements2, measurements3... : array_like
    Arrays of measurements.  All of the arrays must have the same number
    of elements.  At least 3 sets of measurements must be given.

Returns
-------
statistic : float
    The test statistic, correcting for ties.
pvalue : float
    The associated p-value assuming that the test statistic has a chi
    squared distribution.

Notes
-----
Due to the assumption that the test statistic has a chi squared
distribution, the p-value is only reliable for n > 10 and more than
6 repeated measurements.

References
----------
.. [1] https://en.wikipedia.org/wiki/Friedman_test
*)

val gcd : x:Py.Object.t -> y:Py.Object.t -> unit -> Py.Object.t
(**
greatest common divisor of x and y
*)

val gmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the geometric mean along the specified axis.

Return the geometric average of the array elements.
That is:  n-th root of (x1 * x2 * ... * xn)

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the geometric mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If dtype is not specified, it defaults to the
    dtype of a, unless a has an integer dtype with a precision less than
    that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
gmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
hmean : Harmonic mean

Notes
-----
The geometric average is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity because masked
arrays automatically mask any non-finite values.

Examples
--------
>>> from scipy.stats import gmean
>>> gmean([1, 4])
2.0
>>> gmean([1, 2, 3, 4, 5, 6, 7])
3.3800151591412964
*)

val gstd : ?axis:[`Tuple of Py.Object.t | `I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the geometric standard deviation of an array.

The geometric standard deviation describes the spread of a set of numbers
where the geometric mean is preferred. It is a multiplicative factor, and
so a dimensionless quantity.

It is defined as the exponent of the standard deviation of ``log(a)``.
Mathematically the population geometric standard deviation can be
evaluated as::

    gstd = exp(std(log(a)))

.. versionadded:: 1.3.0

Parameters
----------
a : array_like
    An array like object containing the sample data.
axis : int, tuple or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Degree of freedom correction in the calculation of the
    geometric standard deviation. Default is 1.

Returns
-------
ndarray or float
    An array of the geometric standard deviation. If `axis` is None or `a`
    is a 1d array a float is returned.

Notes
-----
As the calculation requires the use of logarithms the geometric standard
deviation only supports strictly positive values. Any non-positive or
infinite values will raise a `ValueError`.
The geometric standard deviation is sometimes confused with the exponent of
the standard deviation, ``exp(std(a))``. Instead the geometric standard
deviation is ``exp(std(log(a)))``.
The default value for `ddof` is different to the default value (0) used
by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.

Examples
--------
Find the geometric standard deviation of a log-normally distributed sample.
Note that the standard deviation of the distribution is one, on a
log scale this evaluates to approximately ``exp(1)``.

>>> from scipy.stats import gstd
>>> np.random.seed(123)
>>> sample = np.random.lognormal(mean=0, sigma=1, size=1000)
>>> gstd(sample)
2.7217860664589946

Compute the geometric standard deviation of a multidimensional array and
of a given axis.

>>> a = np.arange(1, 25).reshape(2, 3, 4)
>>> gstd(a, axis=None)
2.2944076136018947
>>> gstd(a, axis=2)
array([[1.82424757, 1.22436866, 1.13183117],
       [1.09348306, 1.07244798, 1.05914985]])
>>> gstd(a, axis=(1,2))
array([2.12939215, 1.22120169])

The geometric standard deviation further handles masked arrays.

>>> a = np.arange(1, 25).reshape(2, 3, 4)
>>> ma = np.ma.masked_where(a > 16, a)
>>> ma
masked_array(
  data=[[[1, 2, 3, 4],
         [5, 6, 7, 8],
         [9, 10, 11, 12]],
        [[13, 14, 15, 16],
         [--, --, --, --],
         [--, --, --, --]]],
  mask=[[[False, False, False, False],
         [False, False, False, False],
         [False, False, False, False]],
        [[False, False, False, False],
         [ True,  True,  True,  True],
         [ True,  True,  True,  True]]],
  fill_value=999999)
>>> gstd(ma, axis=2)
masked_array(
  data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],
        [1.0934830582350938, --, --]],
  mask=[[False, False, False],
        [False,  True,  True]],
  fill_value=999999)
*)

val hmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the harmonic mean along the specified axis.

That is:  n / (1/x1 + 1/x2 + ... + 1/xn)

Parameters
----------
a : array_like
    Input array, masked array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the harmonic mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If `dtype` is not specified, it defaults to the
    dtype of `a`, unless `a` has an integer `dtype` with a precision less
    than that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
hmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
gmean : Geometric mean

Notes
-----
The harmonic mean is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity.

Examples
--------
>>> from scipy.stats import hmean
>>> hmean([1, 4])
1.6000000000000001
>>> hmean([1, 2, 3, 4, 5, 6, 7])
2.6997245179063363
*)

val iqr : ?axis:[`Sequence_of_int of Py.Object.t | `I of int] -> ?rng:Py.Object.t -> ?scale:float -> ?nan_policy:[`Propagate | `Raise | `Omit] -> ?interpolation:[`Linear | `Lower | `Higher | `Midpoint | `Nearest] -> ?keepdims:bool -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the interquartile range of the data along the specified axis.

The interquartile range (IQR) is the difference between the 75th and
25th percentile of the data. It is a measure of the dispersion
similar to standard deviation or variance, but is much more robust
against outliers [2]_.

The ``rng`` parameter allows this function to compute other
percentile ranges than the actual IQR. For example, setting
``rng=(0, 100)`` is equivalent to `numpy.ptp`.

The IQR of an empty array is `np.nan`.

.. versionadded:: 0.18.0

Parameters
----------
x : array_like
    Input array or object that can be converted to an array.
axis : int or sequence of int, optional
    Axis along which the range is computed. The default is to
    compute the IQR for the entire array.
rng : Two-element sequence containing floats in range of [0,100] optional
    Percentiles over which to compute the range. Each must be
    between 0 and 100, inclusive. The default is the true IQR:
    `(25, 75)`. The order of the elements is not important.
scale : scalar or str, optional
    The numerical value of scale will be divided out of the final
    result. The following string values are recognized:

      'raw' : No scaling, just return the raw IQR.
      'normal' : Scale by :math:`2 \sqrt{2} erf^{-1}(\frac{1}{2}) \approx 1.349`.

    The default is 'raw'. Array-like scale is also allowed, as long
    as it broadcasts correctly to the output such that
    ``out / scale`` is a valid operation. The output dimensions
    depend on the input array, `x`, the `axis` argument, and the
    `keepdims` flag.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values
interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}, optional
    Specifies the interpolation method to use when the percentile
    boundaries lie between two data points `i` and `j`.
    The following options are available (default is 'linear'):

      * 'linear': `i + (j - i) * fraction`, where `fraction` is the
        fractional part of the index surrounded by `i` and `j`.
      * 'lower': `i`.
      * 'higher': `j`.
      * 'nearest': `i` or `j` whichever is nearest.
      * 'midpoint': `(i + j) / 2`.

keepdims : bool, optional
    If this is set to `True`, the reduced axes are left in the
    result as dimensions with size one. With this option, the result
    will broadcast correctly against the original array `x`.

Returns
-------
iqr : scalar or ndarray
    If ``axis=None``, a scalar is returned. If the input contains
    integers or floats of smaller precision than ``np.float64``, then the
    output data-type is ``np.float64``. Otherwise, the output data-type is
    the same as that of the input.

See Also
--------
numpy.std, numpy.var

Notes
-----
This function is heavily dependent on the version of `numpy` that is
installed. Versions greater than 1.11.0b3 are highly recommended, as they
include a number of enhancements and fixes to `numpy.percentile` and
`numpy.nanpercentile` that affect the operation of this function. The
following modifications apply:

Below 1.10.0 : `nan_policy` is poorly defined.
    The default behavior of `numpy.percentile` is used for 'propagate'. This
    is a hybrid of 'omit' and 'propagate' that mostly yields a skewed
    version of 'omit' since NaNs are sorted to the end of the data. A
    warning is raised if there are NaNs in the data.
Below 1.9.0: `numpy.nanpercentile` does not exist.
    This means that `numpy.percentile` is used regardless of `nan_policy`
    and a warning is issued. See previous item for a description of the
    behavior.
Below 1.9.0: `keepdims` and `interpolation` are not supported.
    The keywords get ignored with a warning if supplied with non-default
    values. However, multiple axes are still supported.

References
----------
.. [1] 'Interquartile range' https://en.wikipedia.org/wiki/Interquartile_range
.. [2] 'Robust measures of scale' https://en.wikipedia.org/wiki/Robust_measures_of_scale
.. [3] 'Quantile' https://en.wikipedia.org/wiki/Quantile

Examples
--------
>>> from scipy.stats import iqr
>>> x = np.array([[10, 7, 4], [3, 2, 1]])
>>> x
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> iqr(x)
4.0
>>> iqr(x, axis=0)
array([ 3.5,  2.5,  1.5])
>>> iqr(x, axis=1)
array([ 3.,  1.])
>>> iqr(x, axis=1, keepdims=True)
array([[ 3.],
       [ 1.]])
*)

val itemfreq : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> Py.Object.t
(**
`itemfreq` is deprecated!
`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`

    Return a 2-D array of item frequencies.

    Parameters
    ----------
    a : (N,) array_like
        Input array.

    Returns
    -------
    itemfreq : (K, 2) ndarray
        A 2-D frequency table.  Column 1 contains sorted, unique values from
        `a`, column 2 contains their respective counts.

    Examples
    --------
    >>> from scipy import stats
    >>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])
    >>> stats.itemfreq(a)
    array([[ 0.,  2.],
           [ 1.,  4.],
           [ 2.,  2.],
           [ 4.,  1.],
           [ 5.,  1.]])
    >>> np.bincount(a)
    array([2, 4, 2, 0, 1, 1])

    >>> stats.itemfreq(a/10.)
    array([[ 0. ,  2. ],
           [ 0.1,  4. ],
           [ 0.2,  2. ],
           [ 0.4,  1. ],
           [ 0.5,  1. ]])

    
*)

val jarque_bera : [>`Ndarray] Np.Obj.t -> (float * float)
(**
Perform the Jarque-Bera goodness of fit test on sample data.

The Jarque-Bera test tests whether the sample data has the skewness and
kurtosis matching a normal distribution.

Note that this test only works for a large enough number of data samples
(>2000) as the test statistic asymptotically has a Chi-squared distribution
with 2 degrees of freedom.

Parameters
----------
x : array_like
    Observations of a random variable.

Returns
-------
jb_value : float
    The test statistic.
p : float
    The p-value for the hypothesis test.

References
----------
.. [1] Jarque, C. and Bera, A. (1980) 'Efficient tests for normality,
       homoscedasticity and serial independence of regression residuals',
       6 Econometric Letters 255-259.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(987654321)
>>> x = np.random.normal(0, 1, 100000)
>>> y = np.random.rayleigh(1, 100000)
>>> stats.jarque_bera(x)
(4.7165707989581342, 0.09458225503041906)
>>> stats.jarque_bera(y)
(6713.7098548143422, 0.0)
*)

val kendalltau : ?initial_lexsort:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> ?method_:[`Auto | `Asymptotic | `Exact] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Calculate Kendall's tau, a correlation measure for ordinal data.

Kendall's tau is a measure of the correspondence between two rankings.
Values close to 1 indicate strong agreement, values close to -1 indicate
strong disagreement.  This is the 1945 'tau-b' version of Kendall's
tau [2]_, which can account for ties and which reduces to the 1938 'tau-a'
version [1]_ in absence of ties.

Parameters
----------
x, y : array_like
    Arrays of rankings, of the same shape. If arrays are not 1-D, they will
    be flattened to 1-D.
initial_lexsort : bool, optional
    Unused (deprecated).
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values
method : {'auto', 'asymptotic', 'exact'}, optional
    Defines which method is used to calculate the p-value [5]_.
    The following options are available (default is 'auto'):

      * 'auto': selects the appropriate method based on a trade-off between
        speed and accuracy
      * 'asymptotic': uses a normal approximation valid for large samples
      * 'exact': computes the exact p-value, but can only be used if no ties
        are present

Returns
-------
correlation : float
   The tau statistic.
pvalue : float
   The two-sided p-value for a hypothesis test whose null hypothesis is
   an absence of association, tau = 0.

See Also
--------
spearmanr : Calculates a Spearman rank-order correlation coefficient.
theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).
weightedtau : Computes a weighted version of Kendall's tau.

Notes
-----
The definition of Kendall's tau that is used is [2]_::

  tau = (P - Q) / sqrt((P + Q + T) * (P + Q + U))

where P is the number of concordant pairs, Q the number of discordant
pairs, T the number of ties only in `x`, and U the number of ties only in
`y`.  If a tie occurs for the same pair in both `x` and `y`, it is not
added to either T or U.

References
----------
.. [1] Maurice G. Kendall, 'A New Measure of Rank Correlation', Biometrika
       Vol. 30, No. 1/2, pp. 81-93, 1938.
.. [2] Maurice G. Kendall, 'The treatment of ties in ranking problems',
       Biometrika Vol. 33, No. 3, pp. 239-251. 1945.
.. [3] Gottfried E. Noether, 'Elements of Nonparametric Statistics', John
       Wiley & Sons, 1967.
.. [4] Peter M. Fenwick, 'A new data structure for cumulative frequency
       tables', Software: Practice and Experience, Vol. 24, No. 3,
       pp. 327-336, 1994.
.. [5] Maurice G. Kendall, 'Rank Correlation Methods' (4th Edition),
       Charles Griffin & Co., 1970.

Examples
--------
>>> from scipy import stats
>>> x1 = [12, 2, 1, 12, 2]
>>> x2 = [1, 4, 7, 1, 0]
>>> tau, p_value = stats.kendalltau(x1, x2)
>>> tau
-0.47140452079103173
>>> p_value
0.2827454599327748
*)

val kruskal : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples.

The Kruskal-Wallis H-test tests the null hypothesis that the population
median of all of the groups are equal.  It is a non-parametric version of
ANOVA.  The test works on 2 or more independent samples, which may have
different sizes.  Note that rejecting the null hypothesis does not
indicate which of the groups differs.  Post hoc comparisons between
groups are required to determine which groups are different.

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties.
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution.

See Also
--------
f_oneway : 1-way ANOVA.
mannwhitneyu : Mann-Whitney rank test on two samples.
friedmanchisquare : Friedman test for repeated measurements.

Notes
-----
Due to the assumption that H has a chi square distribution, the number
of samples in each group must not be too small.  A typical rule is
that each sample must have at least 5 measurements.

References
----------
.. [1] W. H. Kruskal & W. W. Wallis, 'Use of Ranks in
   One-Criterion Variance Analysis', Journal of the American Statistical
   Association, Vol. 47, Issue 260, pp. 583-621, 1952.
.. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance

Examples
--------
>>> from scipy import stats
>>> x = [1, 3, 5, 7, 9]
>>> y = [2, 4, 6, 8, 10]
>>> stats.kruskal(x, y)
KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)

>>> x = [1, 1, 1]
>>> y = [2, 2, 2]
>>> z = [2, 2]
>>> stats.kruskal(x, y, z)
KruskalResult(statistic=7.0, pvalue=0.0301973834223185)
*)

val ks_2samp : ?alternative:[`Two_sided | `Less | `Greater] -> ?mode:[`Auto | `Exact | `Asymp] -> data1:Py.Object.t -> data2:Py.Object.t -> unit -> (float * float)
(**
Compute the Kolmogorov-Smirnov statistic on 2 samples.

This is a two-sided test for the null hypothesis that 2 independent samples
are drawn from the same continuous distribution.  The alternative hypothesis
can be either 'two-sided' (default), 'less' or 'greater'.

Parameters
----------
data1, data2 : sequence of 1-D ndarrays
    Two arrays of sample observations assumed to be drawn from a continuous
    distribution, sample sizes can be different.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided, see explanation in Notes
      * 'greater': one-sided, see explanation in Notes
mode : {'auto', 'exact', 'asymp'}, optional
    Defines the method used for calculating the p-value.
    The following options are available (default is 'auto'):

      * 'auto' : use 'exact' for small size arrays, 'asymp' for large
      * 'exact' : use approximation to exact distribution of test statistic
      * 'asymp' : use asymptotic distribution of test statistic

Returns
-------
statistic : float
    KS statistic.
pvalue : float
    Two-tailed p-value.

See Also
--------
kstest

Notes
-----
This tests whether 2 samples are drawn from the same distribution. Note
that, like in the case of the one-sample KS test, the distribution is
assumed to be continuous.

In the one-sided test, the alternative is that the empirical
cumulative distribution function F(x) of the data1 variable is 'less'
or 'greater' than the empirical cumulative distribution function G(x)
of the data2 variable, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.

If the KS statistic is small or the p-value is high, then we cannot
reject the hypothesis that the distributions of the two samples
are the same.

If the mode is 'auto', the computation is exact if the sample sizes are
less than 10000.  For larger sizes, the computation uses the
Kolmogorov-Smirnov distributions to compute an approximate value.

We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.

References
----------
.. [1] Hodges, J.L. Jr.,  'The Significance Probability of the Smirnov
       Two-Sample Test,' Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.


Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)  #fix random seed to get the same result
>>> n1 = 200  # size of first sample
>>> n2 = 300  # size of second sample

For a different distribution, we can reject the null hypothesis since the
pvalue is below 1%:

>>> rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)
>>> rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)
>>> stats.ks_2samp(rvs1, rvs2)
(0.20833333333333334, 5.129279597781977e-05)

For a slightly different distribution, we cannot reject the null hypothesis
at a 10% or lower alpha since the p-value at 0.144 is higher than 10%

>>> rvs3 = stats.norm.rvs(size=n2, loc=0.01, scale=1.0)
>>> stats.ks_2samp(rvs1, rvs3)
(0.10333333333333333, 0.14691437867433876)

For an identical distribution, we cannot reject the null hypothesis since
the p-value is high, 41%:

>>> rvs4 = stats.norm.rvs(size=n2, loc=0.0, scale=1.0)
>>> stats.ks_2samp(rvs1, rvs4)
(0.07999999999999996, 0.41126949729859719)
*)

val kstest : ?args:Py.Object.t -> ?n:int -> ?alternative:[`Two_sided | `Less | `Greater] -> ?mode:[`Approx | `Asymp] -> rvs:[`Ndarray of [>`Ndarray] Np.Obj.t | `Callable of Py.Object.t | `S of string] -> cdf:[`Callable of Py.Object.t | `S of string] -> unit -> (float * float)
(**
Perform the Kolmogorov-Smirnov test for goodness of fit.

This performs a test of the distribution F(x) of an observed
random variable against a given distribution G(x). Under the null
hypothesis, the two distributions are identical, F(x)=G(x). The
alternative hypothesis can be either 'two-sided' (default), 'less'
or 'greater'. The KS test is only valid for continuous distributions.

Parameters
----------
rvs : str, array_like, or callable
    If a string, it should be the name of a distribution in `scipy.stats`.
    If an array, it should be a 1-D array of observations of random
    variables.
    If a callable, it should be a function to generate random variables;
    it is required to have a keyword argument `size`.
cdf : str or callable
    If a string, it should be the name of a distribution in `scipy.stats`.
    If `rvs` is a string then `cdf` can be False or the same as `rvs`.
    If a callable, that callable is used to calculate the cdf.
args : tuple, sequence, optional
    Distribution parameters, used if `rvs` or `cdf` are strings.
N : int, optional
    Sample size if `rvs` is string or callable.  Default is 20.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided, see explanation in Notes
      * 'greater': one-sided, see explanation in Notes
mode : {'approx', 'asymp'}, optional
    Defines the distribution used for calculating the p-value.
    The following options are available (default is 'approx'):

      * 'approx': use approximation to exact distribution of test statistic
      * 'asymp': use asymptotic distribution of test statistic

Returns
-------
statistic : float
    KS test statistic, either D, D+ or D-.
pvalue :  float
    One-tailed or two-tailed p-value.

See Also
--------
ks_2samp

Notes
-----
In the one-sided test, the alternative is that the empirical
cumulative distribution function of the random variable is 'less'
or 'greater' than the cumulative distribution function G(x) of the
hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.

Examples
--------
>>> from scipy import stats

>>> x = np.linspace(-15, 15, 9)
>>> stats.kstest(x, 'norm')
(0.44435602715924361, 0.038850142705171065)

>>> np.random.seed(987654321) # set random seed to get the same result
>>> stats.kstest('norm', False, N=100)
(0.058352892479417884, 0.88531190944151261)

The above lines are equivalent to:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.norm.rvs(size=100), 'norm')
(0.058352892479417884, 0.88531190944151261)

*Test against one-sided alternative hypothesis*

Shift distribution to larger values, so that ``cdf_dgp(x) < norm.cdf(x)``:

>>> np.random.seed(987654321)
>>> x = stats.norm.rvs(loc=0.2, size=100)
>>> stats.kstest(x,'norm', alternative = 'less')
(0.12464329735846891, 0.040989164077641749)

Reject equal distribution against alternative hypothesis: less

>>> stats.kstest(x,'norm', alternative = 'greater')
(0.0072115233216311081, 0.98531158590396395)

Don't reject equal distribution against alternative hypothesis: greater

>>> stats.kstest(x,'norm', mode='asymp')
(0.12464329735846891, 0.08944488871182088)

*Testing t distributed random variables against normal distribution*

With 100 degrees of freedom the t distribution looks close to the normal
distribution, and the K-S test does not reject the hypothesis that the
sample came from the normal distribution:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.t.rvs(100,size=100),'norm')
(0.072018929165471257, 0.67630062862479168)

With 3 degrees of freedom the t distribution looks sufficiently different
from the normal distribution, that we can reject the hypothesis that the
sample came from the normal distribution at the 10% level:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.t.rvs(3,size=100),'norm')
(0.131016895759829, 0.058826222555312224)
*)

val kurtosis : ?axis:[`I of int | `None] -> ?fisher:bool -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the kurtosis (Fisher or Pearson) of a dataset.

Kurtosis is the fourth central moment divided by the square of the
variance. If Fisher's definition is used, then 3.0 is subtracted from
the result to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics to
eliminate bias coming from biased moment estimators

Use `kurtosistest` to see if result is close enough to normal.

Parameters
----------
a : array
    Data for which the kurtosis is calculated.
axis : int or None, optional
    Axis along which the kurtosis is calculated. Default is 0.
    If None, compute over the whole array `a`.
fisher : bool, optional
    If True, Fisher's definition is used (normal ==> 0.0). If False,
    Pearson's definition is used (normal ==> 3.0).
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
kurtosis : array
    The kurtosis of values along an axis. If all values are equal,
    return -3 for Fisher's definition and 0 for Pearson's definition.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.

Examples
--------
In Fisher's definiton, the kurtosis of the normal distribution is zero.
In the following example, the kurtosis is close to zero, because it was
calculated from the dataset, not from the continuous distribution.

>>> from scipy.stats import norm, kurtosis
>>> data = norm.rvs(size=1000, random_state=3)
>>> kurtosis(data)
-0.06928694200380558

The distribution with a higher kurtosis has a heavier tail.
The zero valued kurtosis of the normal distribution in Fisher's definition
can serve as a reference point.

>>> import matplotlib.pyplot as plt
>>> import scipy.stats as stats
>>> from scipy.stats import kurtosis

>>> x = np.linspace(-5, 5, 100)
>>> ax = plt.subplot()
>>> distnames = ['laplace', 'norm', 'uniform']

>>> for distname in distnames:
...     if distname == 'uniform':
...         dist = getattr(stats, distname)(loc=-2, scale=4)
...     else:
...         dist = getattr(stats, distname)
...     data = dist.rvs(size=1000)
...     kur = kurtosis(data, fisher=True)
...     y = dist.pdf(x)
...     ax.plot(x, y, label='{}, {}'.format(distname, round(kur, 3)))
...     ax.legend()

The Laplace distribution has a heavier tail than the normal distribution.
The uniform distribution (which has negative kurtosis) has the thinnest
tail.
*)

val kurtosistest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Test whether a dataset has normal kurtosis.

This function tests the null hypothesis that the kurtosis
of the population from which the sample was drawn is that
of the normal distribution: ``kurtosis = 3(n-1)/(n+1)``.

Parameters
----------
a : array
    Array of the sample data.
axis : int or None, optional
   Axis along which to compute test. Default is 0. If None,
   compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    The two-sided p-value for the hypothesis test.

Notes
-----
Valid only for n>20. This function uses the method described in [1]_.

References
----------
.. [1] see e.g. F. J. Anscombe, W. J. Glynn, 'Distribution of the kurtosis
   statistic b2 for normal samples', Biometrika, vol. 70, pp. 227-234, 1983.

Examples
--------
>>> from scipy.stats import kurtosistest
>>> kurtosistest(list(range(20)))
KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.08804338332528348)

>>> np.random.seed(28041990)
>>> s = np.random.normal(0, 1, 1000)
>>> kurtosistest(s)
KurtosistestResult(statistic=1.2317590987707365, pvalue=0.21803908613450895)
*)

val linregress : ?y:Py.Object.t -> x:Py.Object.t -> unit -> (float * float * float * float * float)
(**
Calculate a linear least-squares regression for two sets of measurements.

Parameters
----------
x, y : array_like
    Two sets of measurements.  Both arrays should have the same length.  If
    only `x` is given (and ``y=None``), then it must be a two-dimensional
    array where one dimension has length 2.  The two sets of measurements
    are then found by splitting the array along the length-2 dimension.  In
    the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is
    equivalent to ``linregress(x[0], x[1])``.

Returns
-------
slope : float
    Slope of the regression line.
intercept : float
    Intercept of the regression line.
rvalue : float
    Correlation coefficient.
pvalue : float
    Two-sided p-value for a hypothesis test whose null hypothesis is
    that the slope is zero, using Wald Test with t-distribution of
    the test statistic.
stderr : float
    Standard error of the estimated gradient.

See also
--------
:func:`scipy.optimize.curve_fit` : Use non-linear
 least squares to fit a function to data.
:func:`scipy.optimize.leastsq` : Minimize the sum of
 squares of a set of equations.

Notes
-----
Missing values are considered pair-wise: if a value is missing in `x`,
the corresponding value in `y` is masked.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats

Generate some data:

>>> np.random.seed(12345678)
>>> x = np.random.random(10)
>>> y = 1.6*x + np.random.random(10)

Perform the linear regression:

>>> slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
>>> print('slope: %f    intercept: %f' % (slope, intercept))
slope: 1.944864    intercept: 0.268578

To get coefficient of determination (R-squared):

>>> print('R-squared: %f' % r_value**2)
R-squared: 0.735498

Plot the data along with the fitted line:

>>> plt.plot(x, y, 'o', label='original data')
>>> plt.plot(x, intercept + slope*x, 'r', label='fitted line')
>>> plt.legend()
>>> plt.show()

Example for the case where only x is provided as a 2x2 array:

>>> x = np.array([[0, 1], [0, 2]])
>>> r = stats.linregress(x)
>>> r.slope, r.intercept
(2.0, 0.0)
*)

val mannwhitneyu : ?use_continuity:bool -> ?alternative:[`Two_sided | `Less | `Greater] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Mann-Whitney rank test on samples x and y.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
use_continuity : bool, optional
        Whether a continuity correction (1/2.) should be taken into
        account. Default is True.
alternative : {None, 'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is None):

      * None: computes p-value half the size of the 'two-sided' p-value and
        a different U statistic. The default behavior is not the same as
        using 'less' or 'greater'; it only exists for backward compatibility
        and is deprecated.
      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided

    Use of the None option is deprecated.

Returns
-------
statistic : float
    The Mann-Whitney U statistic, equal to min(U for x, U for y) if
    `alternative` is equal to None (deprecated; exists for backward
    compatibility), and U for y otherwise.
pvalue : float
    p-value assuming an asymptotic normal distribution. One-sided or
    two-sided, depending on the choice of `alternative`.

Notes
-----
Use only when the number of observation in each sample is > 20 and
you have 2 independent samples of ranks. Mann-Whitney U is
significant if the u-obtained is LESS THAN or equal to the critical
value of U.

This test corrects for ties and by default uses a continuity correction.

References
----------
.. [1] https://en.wikipedia.org/wiki/Mann-Whitney_U_test

.. [2] H.B. Mann and D.R. Whitney, 'On a Test of Whether one of Two Random
       Variables is Stochastically Larger than the Other,' The Annals of
       Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 1947.
*)

val median_absolute_deviation : ?axis:[`I of int | `None] -> ?center:Py.Object.t -> ?scale:float -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the median absolute deviation of the data along the given axis.

The median absolute deviation (MAD, [1]_) computes the median over the
absolute deviations from the median. It is a measure of dispersion
similar to the standard deviation but more robust to outliers [2]_.

The MAD of an empty array is ``np.nan``.

.. versionadded:: 1.3.0

Parameters
----------
x : array_like
    Input array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the range is computed. Default is 0. If None, compute
    the MAD over the entire array.
center : callable, optional
    A function that will return the central value. The default is to use
    np.median. Any user defined function used will need to have the function
    signature ``func(arr, axis)``.
scale : int, optional
    The scaling factor applied to the MAD. The default scale (1.4826)
    ensures consistency with the standard deviation for normally distributed
    data.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
mad : scalar or ndarray
    If ``axis=None``, a scalar is returned. If the input contains
    integers or floats of smaller precision than ``np.float64``, then the
    output data-type is ``np.float64``. Otherwise, the output data-type is
    the same as that of the input.

See Also
--------
numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,
scipy.stats.tstd, scipy.stats.tvar

Notes
-----
The `center` argument only affects the calculation of the central value
around which the MAD is calculated. That is, passing in ``center=np.mean``
will calculate the MAD around the mean - it will not calculate the *mean*
absolute deviation.

References
----------
.. [1] 'Median absolute deviation' https://en.wikipedia.org/wiki/Median_absolute_deviation
.. [2] 'Robust measures of scale' https://en.wikipedia.org/wiki/Robust_measures_of_scale

Examples
--------
When comparing the behavior of `median_absolute_deviation` with ``np.std``,
the latter is affected when we change a single value of an array to have an
outlier value while the MAD hardly changes:

>>> from scipy import stats
>>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)
>>> x.std()
0.9973906394005013
>>> stats.median_absolute_deviation(x)
1.2280762773108278
>>> x[0] = 345.6
>>> x.std()
34.42304872314415
>>> stats.median_absolute_deviation(x)
1.2340335571164334

Axis handling example:

>>> x = np.array([[10, 7, 4], [3, 2, 1]])
>>> x
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> stats.median_absolute_deviation(x)
array([5.1891, 3.7065, 2.2239])
>>> stats.median_absolute_deviation(x, axis=None)
2.9652
*)

val mode : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Return an array of the modal (most common) value in the passed array.

If there is more than one such value, only the smallest is returned.
The bin-count for the modal bins is also returned.

Parameters
----------
a : array_like
    n-dimensional array of which to find mode(s).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
mode : ndarray
    Array of modal values.
count : ndarray
    Array of counts for each mode.

Examples
--------
>>> a = np.array([[6, 8, 3, 0],
...               [3, 2, 1, 7],
...               [8, 1, 8, 4],
...               [5, 3, 0, 5],
...               [4, 7, 5, 9]])
>>> from scipy import stats
>>> stats.mode(a)
(array([[3, 1, 0, 0]]), array([[1, 1, 1, 1]]))

To get mode of whole array, specify ``axis=None``:

>>> stats.mode(a, axis=None)
(array([3]), array([3]))
*)

val moment : ?moment:[`I of int | `Array_like_of_ints of Py.Object.t] -> ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the nth moment about the mean for a sample.

A moment is a specific quantitative measure of the shape of a set of
points. It is often used to calculate coefficients of skewness and kurtosis
due to its close relationship with them.

Parameters
----------
a : array_like
   Input array.
moment : int or array_like of ints, optional
   Order of central moment that is returned. Default is 1.
axis : int or None, optional
   Axis along which the central moment is computed. Default is 0.
   If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
n-th central moment : ndarray or float
   The appropriate moment along the given axis or over all values if axis
   is None. The denominator for the moment calculation is the number of
   observations, no degrees of freedom correction is done.

See Also
--------
kurtosis, skew, describe

Notes
-----
The k-th central moment of a data sample is:

.. math::

    m_k = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})^k

Where n is the number of samples and x-bar is the mean. This function uses
exponentiation by squares [1]_ for efficiency.

References
----------
.. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms

Examples
--------
>>> from scipy.stats import moment
>>> moment([1, 2, 3, 4, 5], moment=1)
0.0
>>> moment([1, 2, 3, 4, 5], moment=2)
2.0
*)

val multiscale_graphcorr : ?compute_distance:Py.Object.t -> ?reps:int -> ?workers:[`Map_like_callable of Py.Object.t | `I of int] -> ?is_twosamp:bool -> ?random_state:[`I of int | `Np_random_RandomState_instance of Py.Object.t] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float * Py.Object.t)
(**
Computes the Multiscale Graph Correlation (MGC) test statistic.

Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for
one property (e.g. cloud density), and the :math:`l`-nearest neighbors for
the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is
called the 'scale'. A priori, however, it is not know which scales will be
most informative. So, MGC computes all distance pairs, and then efficiently
computes the distance correlations for all scales. The local correlations
illustrate which scales are relatively informative about the relationship.
The key, therefore, to successfully discover and decipher relationships
between disparate data modalities is to adaptively determine which scales
are the most informative, and the geometric implication for the most
informative scales. Doing so not only provides an estimate of whether the
modalities are related, but also provides insight into how the
determination was made. This is especially important in high-dimensional
data, where simple visualizations do not reveal relationships to the
unaided human eye. Characterizations of this implementation in particular
have been derived from and benchmarked within in [2]_.

Parameters
----------
x, y : ndarray
    If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is
    the number of samples and `p` and `q` are the number of dimensions,
    then the MGC independence test will be run.  Alternatively, ``x`` and
    ``y`` can have shapes ``(n, n)`` if they are distance or similarity
    matrices, and ``compute_distance`` must be sent to ``None``. If ``x``
    and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired
    two-sample MGC test will be run.
compute_distance : callable, optional
    A function that computes the distance or similarity among the samples
    within each data matrix. Set to ``None`` if ``x`` and ``y`` are
    already distance matrices. The default uses the euclidean norm metric.
    If you are calling a custom function, either create the distance
    matrix before-hand or create a function of the form
    ``compute_distance(x)`` where `x` is the data matrix for which
    pairwise distances are calculated.
reps : int, optional
    The number of replications used to estimate the null when using the
    permutation test. The default is ``1000``.
workers : int or map-like callable, optional
    If ``workers`` is an int the population is subdivided into ``workers``
    sections and evaluated in parallel (uses ``multiprocessing.Pool
    <multiprocessing>``). Supply ``-1`` to use all cores available to the
    Process. Alternatively supply a map-like callable, such as
    ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.
    This evaluation is carried out as ``workers(func, iterable)``.
    Requires that `func` be pickleable. The default is ``1``.
is_twosamp : bool, optional
    If `True`, a two sample test will be run. If ``x`` and ``y`` have
    shapes ``(n, p)`` and ``(m, p)``, this optional will be overriden and
    set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes
    ``(n, p)`` and a two sample test is desired. The default is ``False``.
random_state : int or np.random.RandomState instance, optional
    If already a RandomState instance, use it.
    If seed is an int, return a new RandomState instance seeded with seed.
    If None, use np.random.RandomState. Default is None.

Returns
-------
stat : float
    The sample MGC test statistic within `[-1, 1]`.
pvalue : float
    The p-value obtained via permutation.
mgc_dict : dict
    Contains additional useful additional returns containing the following
    keys:

        - mgc_map : ndarray
            A 2D representation of the latent geometry of the relationship.
            of the relationship.
        - opt_scale : (int, int)
            The estimated optimal scale as a `(x, y)` pair.
        - null_dist : list
            The null distribution derived from the permuted matrices

See Also
--------
pearsonr : Pearson correlation coefficient and p-value for testing
           non-correlation.
kendalltau : Calculates Kendall's tau.
spearmanr : Calculates a Spearman rank-order correlation coefficient.

Notes
-----
A description of the process of MGC and applications on neuroscience data
can be found in [1]_. It is performed using the following steps:

#. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and
   modified to be mean zero columnwise. This results in two
   :math:`n \times n` distance matrices :math:`A` and :math:`B` (the
   centering and unbiased modification) [3]_.

#. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,

   * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs
     are calculated for each property. Here, :math:`G_k (i, j)` indicates
     the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`
     and :math:`H_l (i, j)` indicates the :math:`l` smallested values of
     the :math:`i`-th row of :math:`B`

   * Let :math:`\circ` denotes the entry-wise matrix product, then local
     correlations are summed and normalized using the following statistic:

.. math::

    c^{kl} = \frac{\sum_{ij} A G_k B H_l}
                  {\sqrt{\sum_{ij} A^2 G_k \times \sum_{ij} B^2 H_l}}

#. The MGC test statistic is the smoothed optimal local correlation of
   :math:`\{ c^{kl} \}`. Denote the smoothing operation as :math:`R(\cdot)`
   (which essentially set all isolated large correlations) as 0 and
   connected large correlations the same as before, see [3]_.) MGC is,

.. math::

    MGC_n (x, y) = \max_{(k, l)} R \left(c^{kl} \left( x_n, y_n \right)
                                                \right)

The test statistic returns a value between :math:`(-1, 1)` since it is
normalized.

The p-value returned is calculated using a permutation test. This process
is completed by first randomly permuting :math:`y` to estimate the null
distribution and then calculating the probability of observing a test
statistic, under the null, at least as extreme as the observed test
statistic.

MGC requires at least 5 samples to run with reliable results. It can also
handle high-dimensional data sets.

In addition, by manipulating the input data matrices, the two-sample
testing problem can be reduced to the independence testing problem [4]_.
Given sample data :math:`U` and :math:`V` of sizes :math:`p \times n`
:math:`p \times m`, data matrix :math:`X` and :math:`Y` can be created as
follows:

.. math::

    X = [U | V] \in \mathcal{R}^{p \times (n + m)}

    Y = [0_{1 \times n} | 1_{1 \times m}] \in \mathcal{R}^{(n + m)}

Then, the MGC statistic can be calculated as normal. This methodology can
be extended to similar tests such as distance correlation [4]_.

.. versionadded:: 1.4.0

References
----------
.. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,
       Maggioni, M., & Shen, C. (2019). Discovering and deciphering
       relationships across disparate data modalities. ELife.
.. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,
       Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).
       mgcpy: A Comprehensive High Dimensional Independence Testing Python
       Package. ArXiv:1907.02088 [Cs, Stat].
.. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance
       correlation to multiscale graph correlation. Journal of the American
       Statistical Association.
.. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of
       Distance and Kernel Methods for Hypothesis Testing. ArXiv:1806.05514
       [Cs, Stat].

Examples
--------
>>> from scipy.stats import multiscale_graphcorr
>>> x = np.arange(100)
>>> y = x
>>> stat, pvalue, _ = multiscale_graphcorr(x, y, workers=-1)
>>> '%.1f, %.3f' % (stat, pvalue)
'1.0, 0.001'

Alternatively,

>>> x = np.arange(100)
>>> y = x
>>> mgc = multiscale_graphcorr(x, y)
>>> '%.1f, %.3f' % (mgc.stat, mgc.pvalue)
'1.0, 0.001'

To run an unpaired two-sample test,

>>> x = np.arange(100)
>>> y = np.arange(79)
>>> mgc = multiscale_graphcorr(x, y, random_state=1)
>>> '%.3f, %.2f' % (mgc.stat, mgc.pvalue)
'0.033, 0.02'

or, if shape of the inputs are the same,

>>> x = np.arange(100)
>>> y = x
>>> mgc = multiscale_graphcorr(x, y, is_twosamp=True)
>>> '%.3f, %.1f' % (mgc.stat, mgc.pvalue)
'-0.008, 1.0'
*)

val namedtuple : ?rename:Py.Object.t -> ?defaults:Py.Object.t -> ?module_:Py.Object.t -> typename:Py.Object.t -> field_names:Py.Object.t -> unit -> Py.Object.t
(**
Returns a new subclass of tuple with named fields.

>>> Point = namedtuple('Point', ['x', 'y'])
>>> Point.__doc__                   # docstring for the new class
'Point(x, y)'
>>> p = Point(11, y=22)             # instantiate with positional args or keywords
>>> p[0] + p[1]                     # indexable like a plain tuple
33
>>> x, y = p                        # unpack like a regular tuple
>>> x, y
(11, 22)
>>> p.x + p.y                       # fields also accessible by name
33
>>> d = p._asdict()                 # convert to a dictionary
>>> d['x']
11
>>> Point( **d)                      # convert from a dictionary
Point(x=11, y=22)
>>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields
Point(x=100, y=22)
*)

val normaltest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Test whether a sample differs from a normal distribution.

This function tests the null hypothesis that a sample comes
from a normal distribution.  It is based on D'Agostino and
Pearson's [1]_, [2]_ test that combines skew and kurtosis to
produce an omnibus test of normality.

Parameters
----------
a : array_like
    The array containing the sample to be tested.
axis : int or None, optional
    Axis along which to compute test. Default is 0. If None,
    compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and
    ``k`` is the z-score returned by `kurtosistest`.
pvalue : float or array
   A 2-sided chi squared probability for the hypothesis test.

References
----------
.. [1] D'Agostino, R. B. (1971), 'An omnibus test of normality for
       moderate and large sample size', Biometrika, 58, 341-348

.. [2] D'Agostino, R. and Pearson, E. S. (1973), 'Tests for departure from
       normality', Biometrika, 60, 613-622

Examples
--------
>>> from scipy import stats
>>> pts = 1000
>>> np.random.seed(28041990)
>>> a = np.random.normal(0, 1, size=pts)
>>> b = np.random.normal(2, 1, size=pts)
>>> x = np.concatenate((a, b))
>>> k2, p = stats.normaltest(x)
>>> alpha = 1e-3
>>> print('p = {:g}'.format(p))
p = 3.27207e-11
>>> if p < alpha:  # null hypothesis: x comes from a normal distribution
...     print('The null hypothesis can be rejected')
... else:
...     print('The null hypothesis cannot be rejected')
The null hypothesis can be rejected
*)

val obrientransform : Py.Object.t list -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the O'Brien transform on input data (any number of arrays).

Used to test for homogeneity of variance prior to running one-way stats.
Each array in ``*args`` is one level of a factor.
If `f_oneway` is run on the transformed data and found significant,
the variances are unequal.  From Maxwell and Delaney [1]_, p.112.

Parameters
----------
args : tuple of array_like
    Any number of arrays.

Returns
-------
obrientransform : ndarray
    Transformed data for use in an ANOVA.  The first dimension
    of the result corresponds to the sequence of transformed
    arrays.  If the arrays given are all 1-D of the same length,
    the return value is a 2-D array; otherwise it is a 1-D array
    of type object, with each element being an ndarray.

References
----------
.. [1] S. E. Maxwell and H. D. Delaney, 'Designing Experiments and
       Analyzing Data: A Model Comparison Perspective', Wadsworth, 1990.

Examples
--------
We'll test the following data sets for differences in their variance.

>>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]
>>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]

Apply the O'Brien transform to the data.

>>> from scipy.stats import obrientransform
>>> tx, ty = obrientransform(x, y)

Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the
transformed data.

>>> from scipy.stats import f_oneway
>>> F, p = f_oneway(tx, ty)
>>> p
0.1314139477040335

If we require that ``p < 0.05`` for significance, we cannot conclude
that the variances are different.
*)

val pearsonr : x:[>`Ndarray] Np.Obj.t -> y:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Pearson correlation coefficient and p-value for testing non-correlation.

The Pearson correlation coefficient [1]_ measures the linear relationship
between two datasets.  The calculation of the p-value relies on the
assumption that each dataset is normally distributed.  (See Kowalski [3]_
for a discussion of the effects of non-normality of the input on the
distribution of the correlation coefficient.)  Like other correlation
coefficients, this one varies between -1 and +1 with 0 implying no
correlation. Correlations of -1 or +1 imply an exact linear relationship.
Positive correlations imply that as x increases, so does y. Negative
correlations imply that as x increases, y decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Pearson correlation at least as extreme
as the one computed from these datasets.

Parameters
----------
x : (N,) array_like
    Input array.
y : (N,) array_like
    Input array.

Returns
-------
r : float
    Pearson's correlation coefficient.
p-value : float
    Two-tailed p-value.

Warns
-----
PearsonRConstantInputWarning
    Raised if an input is a constant array.  The correlation coefficient
    is not defined in this case, so ``np.nan`` is returned.

PearsonRNearConstantInputWarning
    Raised if an input is 'nearly' constant.  The array ``x`` is considered
    nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.
    Numerical errors in the calculation ``x - mean(x)`` in this case might
    result in an inaccurate calculation of r.

See Also
--------
spearmanr : Spearman rank-order correlation coefficient.
kendalltau : Kendall's tau, a correlation measure for ordinal data.

Notes
-----
The correlation coefficient is calculated as follows:

.. math::

    r = \frac{\sum (x - m_x) (y - m_y)}
             {\sqrt{\sum (x - m_x)^2 \sum (y - m_y)^2}}

where :math:`m_x` is the mean of the vector :math:`x` and :math:`m_y` is
the mean of the vector :math:`y`.

Under the assumption that x and y are drawn from independent normal
distributions (so the population correlation coefficient is 0), the
probability density function of the sample correlation coefficient r
is ([1]_, [2]_)::

           (1 - r**2)**(n/2 - 2)
    f(r) = ---------------------
              B(1/2, n/2 - 1)

where n is the number of samples, and B is the beta function.  This
is sometimes referred to as the exact distribution of r.  This is
the distribution that is used in `pearsonr` to compute the p-value.
The distribution is a beta distribution on the interval [-1, 1],
with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's
implementation of the beta distribution, the distribution of r is::

    dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)

The p-value returned by `pearsonr` is a two-sided p-value.  For a
given sample with correlation coefficient r, the p-value is
the probability that abs(r') of a random sample x' and y' drawn from
the population with zero correlation would be greater than or equal
to abs(r).  In terms of the object ``dist`` shown above, the p-value
for a given r and length n can be computed as::

    p = 2*dist.cdf(-abs(r))

When n is 2, the above continuous distribution is not well-defined.
One can interpret the limit of the beta distribution as the shape
parameters a and b approach a = b = 0 as a discrete distribution with
equal probability masses at r = 1 and r = -1.  More directly, one
can observe that, given the data x = [x1, x2] and y = [y1, y2], and
assuming x1 != x2 and y1 != y2, the only possible values for r are 1
and -1.  Because abs(r') for any sample x' and y' with length 2 will
be 1, the two-sided p-value for a sample of length 2 is always 1.

References
----------
.. [1] 'Pearson correlation coefficient', Wikipedia,
       https://en.wikipedia.org/wiki/Pearson_correlation_coefficient
.. [2] Student, 'Probable error of a correlation coefficient',
       Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.
.. [3] C. J. Kowalski, 'On the Effects of Non-Normality on the Distribution
       of the Sample Product-Moment Correlation Coefficient'
       Journal of the Royal Statistical Society. Series C (Applied
       Statistics), Vol. 21, No. 1 (1972), pp. 1-12.

Examples
--------
>>> from scipy import stats
>>> a = np.array([0, 0, 0, 1, 1, 1, 1])
>>> b = np.arange(7)
>>> stats.pearsonr(a, b)
(0.8660254037844386, 0.011724811003954649)

>>> stats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])
(-0.7426106572325057, 0.1505558088534455)
*)

val percentileofscore : ?kind:[`Rank | `Weak | `Strict | `Mean] -> a:[>`Ndarray] Np.Obj.t -> score:[`I of int | `F of float] -> unit -> float
(**
Compute the percentile rank of a score relative to a list of scores.

A `percentileofscore` of, for example, 80% means that 80% of the
scores in `a` are below the given score. In the case of gaps or
ties, the exact definition depends on the optional keyword, `kind`.

Parameters
----------
a : array_like
    Array of scores to which `score` is compared.
score : int or float
    Score that is compared to the elements in `a`.
kind : {'rank', 'weak', 'strict', 'mean'}, optional
    Specifies the interpretation of the resulting score.
    The following options are available (default is 'rank'):

      * 'rank': Average percentage ranking of score.  In case of multiple
        matches, average the percentage rankings of all matching scores.
      * 'weak': This kind corresponds to the definition of a cumulative
        distribution function.  A percentileofscore of 80% means that 80%
        of values are less than or equal to the provided score.
      * 'strict': Similar to 'weak', except that only values that are
        strictly less than the given score are counted.
      * 'mean': The average of the 'weak' and 'strict' scores, often used
        in testing.  See https://en.wikipedia.org/wiki/Percentile_rank

Returns
-------
pcos : float
    Percentile-position of score (0-100) relative to `a`.

See Also
--------
numpy.percentile

Examples
--------
Three-quarters of the given values lie below a given score:

>>> from scipy import stats
>>> stats.percentileofscore([1, 2, 3, 4], 3)
75.0

With multiple matches, note how the scores of the two matches, 0.6
and 0.8 respectively, are averaged:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3)
70.0

Only 2/5 values are strictly less than 3:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')
40.0

But 4/5 values are less than or equal to 3:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')
80.0

The average between the weak and the strict scores is:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')
60.0
*)

val pointbiserialr : x:Py.Object.t -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculate a point biserial correlation coefficient and its p-value.

The point biserial correlation is used to measure the relationship
between a binary variable, x, and a continuous variable, y. Like other
correlation coefficients, this one varies between -1 and +1 with 0
implying no correlation. Correlations of -1 or +1 imply a determinative
relationship.

This function uses a shortcut formula but produces the same result as
`pearsonr`.

Parameters
----------
x : array_like of bools
    Input array.
y : array_like
    Input array.

Returns
-------
correlation : float
    R value.
pvalue : float
    Two-sided p-value.

Notes
-----
`pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.
It is equivalent to `pearsonr.`

The value of the point-biserial correlation can be calculated from:

.. math::

    r_{pb} = \frac{\overline{Y_{1}} -
             \overline{Y_{0}}}{s_{y}}\sqrt{\frac{N_{1} N_{2}}{N (N - 1))}}

Where :math:`Y_{0}` and :math:`Y_{1}` are means of the metric
observations coded 0 and 1 respectively; :math:`N_{0}` and :math:`N_{1}`
are number of observations coded 0 and 1 respectively; :math:`N` is the
total number of observations and :math:`s_{y}` is the standard
deviation of all the metric observations.

A value of :math:`r_{pb}` that is significantly different from zero is
completely equivalent to a significant difference in means between the two
groups. Thus, an independent groups t Test with :math:`N-2` degrees of
freedom may be used to test whether :math:`r_{pb}` is nonzero. The
relation between the t-statistic for comparing two independent groups and
:math:`r_{pb}` is given by:

.. math::

    t = \sqrt{N - 2}\frac{r_{pb}}{\sqrt{1 - r^{2}_{pb}}}

References
----------
.. [1] J. Lev, 'The Point Biserial Coefficient of Correlation', Ann. Math.
       Statist., Vol. 20, no.1, pp. 125-126, 1949.

.. [2] R.F. Tate, 'Correlation Between a Discrete and a Continuous
       Variable. Point-Biserial Correlation.', Ann. Math. Statist., Vol. 25,
       np. 3, pp. 603-607, 1954.

.. [3] D. Kornbrot 'Point Biserial Correlation', In Wiley StatsRef:
       Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.
       https://doi.org/10.1002/9781118445112.stat06227

Examples
--------
>>> from scipy import stats
>>> a = np.array([0, 0, 0, 1, 1, 1, 1])
>>> b = np.arange(7)
>>> stats.pointbiserialr(a, b)
(0.8660254037844386, 0.011724811003954652)
>>> stats.pearsonr(a, b)
(0.86602540378443871, 0.011724811003954626)
>>> np.corrcoef(a, b)
array([[ 1.       ,  0.8660254],
       [ 0.8660254,  1.       ]])
*)

val power_divergence : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> ?lambda_:[`S of string | `F of float] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Cressie-Read power divergence statistic and goodness of fit test.

This function tests the null hypothesis that the categorical data
has the given frequencies, using the Cressie-Read power divergence
statistic.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.
lambda_ : float or str, optional
    The power in the Cressie-Read power divergence statistic.  The default
    is 1.  For convenience, `lambda_` may be assigned one of the following
    strings, in which case the corresponding numerical value is used::

        String              Value   Description
        'pearson'             1     Pearson's chi-squared statistic.
                                    In this case, the function is
                                    equivalent to `stats.chisquare`.
        'log-likelihood'      0     Log-likelihood ratio. Also known as
                                    the G-test [3]_.
        'freeman-tukey'      -1/2   Freeman-Tukey statistic.
        'mod-log-likelihood' -1     Modified log-likelihood ratio.
        'neyman'             -2     Neyman's statistic.
        'cressie-read'        2/3   The power recommended in [5]_.

Returns
-------
statistic : float or ndarray
    The Cressie-Read power divergence test statistic.  The value is
    a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.
pvalue : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `stat` are scalars.

See Also
--------
chisquare

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

When `lambda_` is less than zero, the formula for the statistic involves
dividing by `f_obs`, so a warning or error may be generated if any value
in `f_obs` is 0.

Similarly, a warning or error may be generated if any value in `f_exp` is
zero when `lambda_` >= 0.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not a chisquare, in which case this
test is not appropriate.

This function handles masked arrays.  If an element of `f_obs` or `f_exp`
is masked, then data at that position is ignored, and does not count
towards the size of the data set.

.. versionadded:: 0.13.0

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test
.. [3] 'G-test', https://en.wikipedia.org/wiki/G-test
.. [4] Sokal, R. R. and Rohlf, F. J. 'Biometry: the principles and
       practice of statistics in biological research', New York: Freeman
       (1981)
.. [5] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
(See `chisquare` for more examples.)

When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.  Here we
perform a G-test (i.e. use the log-likelihood ratio statistic):

>>> from scipy.stats import power_divergence
>>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')
(2.006573162632538, 0.84823476779463769)

The expected frequencies can be given with the `f_exp` argument:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[16, 16, 16, 16, 16, 8],
...                  lambda_='log-likelihood')
(3.3281031458963746, 0.6495419288047497)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> power_divergence(obs, lambda_='log-likelihood')
(array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> power_divergence(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> power_divergence(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
test statistic with `ddof`.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we must use ``axis=1``:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[[16, 16, 16, 16, 16, 8],
...                         [8, 20, 20, 16, 12, 12]],
...                  axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val rankdata : ?method_:[`Average | `Min | `Max | `Dense | `Ordinal] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Assign ranks to data, dealing with ties appropriately.

Ranks begin at 1.  The `method` argument controls how ranks are assigned
to equal values.  See [1]_ for further discussion of ranking methods.

Parameters
----------
a : array_like
    The array of values to be ranked.  The array is first flattened.
method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional
    The method used to assign ranks to tied elements.
    The following methods are available (default is 'average'):

      * 'average': The average of the ranks that would have been assigned to
        all the tied values is assigned to each value.
      * 'min': The minimum of the ranks that would have been assigned to all
        the tied values is assigned to each value.  (This is also
        referred to as 'competition' ranking.)
      * 'max': The maximum of the ranks that would have been assigned to all
        the tied values is assigned to each value.
      * 'dense': Like 'min', but the rank of the next highest element is
        assigned the rank immediately after those assigned to the tied
        elements.
      * 'ordinal': All values are given a distinct rank, corresponding to
        the order that the values occur in `a`.

Returns
-------
ranks : ndarray
     An array of length equal to the size of `a`, containing rank
     scores.

References
----------
.. [1] 'Ranking', https://en.wikipedia.org/wiki/Ranking

Examples
--------
>>> from scipy.stats import rankdata
>>> rankdata([0, 2, 3, 2])
array([ 1. ,  2.5,  4. ,  2.5])
>>> rankdata([0, 2, 3, 2], method='min')
array([ 1,  2,  4,  2])
>>> rankdata([0, 2, 3, 2], method='max')
array([ 1,  3,  4,  3])
>>> rankdata([0, 2, 3, 2], method='dense')
array([ 1,  2,  3,  2])
>>> rankdata([0, 2, 3, 2], method='ordinal')
array([ 1,  2,  4,  3])
*)

val ranksums : x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Wilcoxon rank-sum statistic for two samples.

The Wilcoxon rank-sum test tests the null hypothesis that two sets
of measurements are drawn from the same distribution.  The alternative
hypothesis is that values in one sample are more likely to be
larger than the values in the other sample.

This test should be used to compare two samples from continuous
distributions.  It does not handle ties between measurements
in x and y.  For tie-handling and an optional continuity correction
see `scipy.stats.mannwhitneyu`.

Parameters
----------
x,y : array_like
    The data from the two samples.

Returns
-------
statistic : float
    The test statistic under the large-sample approximation that the
    rank sum statistic is normally distributed.
pvalue : float
    The two-sided p-value of the test.

References
----------
.. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test
*)

val relfreq : ?numbins:int -> ?defaultreallimits:Py.Object.t -> ?weights:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float * int)
(**
Return a relative frequency histogram, using the histogram function.

A relative frequency  histogram is a mapping of the number of
observations in each of the bins relative to the total of observations.

Parameters
----------
a : array_like
    Input array.
numbins : int, optional
    The number of bins to use for the histogram. Default is 10.
defaultreallimits : tuple (lower, upper), optional
    The lower and upper values for the range of the histogram.
    If no value is given, a range slightly larger than the range of the
    values in a is used. Specifically ``(a.min() - s, a.max() + s)``,
    where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.
weights : array_like, optional
    The weights for each value in `a`. Default is None, which gives each
    value a weight of 1.0

Returns
-------
frequency : ndarray
    Binned values of relative frequency.
lowerlimit : float
    Lower real limit.
binsize : float
    Width of each bin.
extrapoints : int
    Extra points.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats
>>> a = np.array([2, 4, 1, 2, 3, 2])
>>> res = stats.relfreq(a, numbins=4)
>>> res.frequency
array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])
>>> np.sum(res.frequency)  # relative frequencies should add up to 1
1.0

Create a normal distribution with 1000 random values

>>> rng = np.random.RandomState(seed=12345)
>>> samples = stats.norm.rvs(size=1000, random_state=rng)

Calculate relative frequencies

>>> res = stats.relfreq(samples, numbins=25)

Calculate space of values for x

>>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,
...                                  res.frequency.size)

Plot relative frequency histogram

>>> fig = plt.figure(figsize=(5, 4))
>>> ax = fig.add_subplot(1, 1, 1)
>>> ax.bar(x, res.frequency, width=res.binsize)
>>> ax.set_title('Relative frequency histogram')
>>> ax.set_xlim([x.min(), x.max()])

>>> plt.show()
*)

val rvs_ratio_uniforms : ?size:int list -> ?c:float -> ?random_state:[`I of int | `Np_random_RandomState_instance of Py.Object.t] -> pdf:Py.Object.t -> umax:float -> vmin:float -> vmax:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Generate random samples from a probability density function using the
ratio-of-uniforms method.

Parameters
----------
pdf : callable
    A function with signature `pdf(x)` that is the probability
    density function of the distribution.
umax : float
    The upper bound of the bounding rectangle in the u-direction.
vmin : float
    The lower bound of the bounding rectangle in the v-direction.
vmax : float
    The upper bound of the bounding rectangle in the v-direction.
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
c : float, optional.
    Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.
random_state : int or np.random.RandomState instance, optional
    If already a RandomState instance, use it.
    If seed is an int, return a new RandomState instance seeded with seed.
    If None, use np.random.RandomState. Default is None.

Returns
-------
rvs : ndarray
    The random variates distributed according to the probability
    distribution defined by the pdf.

Notes
-----
Given a univariate probability density function `pdf` and a constant `c`,
define the set ``A = {(u, v) : 0 < u <= sqrt(pdf(v/u + c))}``.
If `(U, V)` is a random vector uniformly distributed over `A`,
then `V/U + c` follows a distribution according to `pdf`.

The above result (see [1]_, [2]_) can be used to sample random variables
using only the pdf, i.e. no inversion of the cdf is required. Typical
choices of `c` are zero or the mode of `pdf`. The set `A` is a subset of
the rectangle ``R = [0, umax] x [vmin, vmax]`` where

- ``umax = sup sqrt(pdf(x))``
- ``vmin = inf (x - c) sqrt(pdf(x))``
- ``vmax = sup (x - c) sqrt(pdf(x))``

In particular, these values are finite if `pdf` is bounded and
``x**2 * pdf(x)`` is bounded (i.e. subquadratic tails).
One can generate `(U, V)` uniformly on `R` and return
`V/U + c` if `(U, V)` are also in `A` which can be directly
verified.

Intuitively, the method works well if `A` fills up most of the
enclosing rectangle such that the probability is high that `(U, V)`
lies in `A` whenever it lies in `R` as the number of required
iterations becomes too large otherwise. To be more precise, note that
the expected number of iterations to draw `(U, V)` uniformly
distributed on `R` such that `(U, V)` is also in `A` is given by
the ratio ``area(R) / area(A) = 2 * umax * (vmax - vmin)``, using the fact
that the area of `A` is equal to 1/2 (Theorem 7.1 in [1]_). A warning
is displayed if this ratio is larger than 20. Moreover, if the sampling
fails to generate a single random variate after 50000 iterations (i.e.
not a single draw is in `A`), an exception is raised.

If the bounding rectangle is not correctly specified (i.e. if it does not
contain `A`), the algorithm samples from a distribution different from
the one given by `pdf`. It is therefore recommended to perform a
test such as `~scipy.stats.kstest` as a check.

References
----------
.. [1] L. Devroye, 'Non-Uniform Random Variate Generation',
   Springer-Verlag, 1986.

.. [2] W. Hoermann and J. Leydold, 'Generating generalized inverse Gaussian
   random variates', Statistics and Computing, 24(4), p. 547--557, 2014.

.. [3] A.J. Kinderman and J.F. Monahan, 'Computer Generation of Random
   Variables Using the Ratio of Uniform Deviates',
   ACM Transactions on Mathematical Software, 3(3), p. 257--260, 1977.

Examples
--------
>>> from scipy import stats

Simulate normally distributed random variables. It is easy to compute the
bounding rectangle explicitly in that case.

>>> f = stats.norm.pdf
>>> v_bound = np.sqrt(f(np.sqrt(2))) * np.sqrt(2)
>>> umax, vmin, vmax = np.sqrt(f(0)), -v_bound, v_bound
>>> np.random.seed(12345)
>>> rvs = stats.rvs_ratio_uniforms(f, umax, vmin, vmax, size=2500)

The K-S test confirms that the random variates are indeed normally
distributed (normality is not rejected at 5% significance level):

>>> stats.kstest(rvs, 'norm')[1]
0.3420173467307603

The exponential distribution provides another example where the bounding
rectangle can be determined explicitly.

>>> np.random.seed(12345)
>>> rvs = stats.rvs_ratio_uniforms(lambda x: np.exp(-x), umax=1,
...                                vmin=0, vmax=2*np.exp(-1), size=1000)
>>> stats.kstest(rvs, 'expon')[1]
0.928454552559516

Sometimes it can be helpful to use a non-zero shift parameter `c`, see e.g.
[2]_ above in the case of the generalized inverse Gaussian distribution.
*)

val scoreatpercentile : ?limit:Py.Object.t -> ?interpolation_method:[`Fraction | `Lower | `Higher] -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> per:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the score at a given percentile of the input sequence.

For example, the score at `per=50` is the median. If the desired quantile
lies between two data points, we interpolate between them, according to
the value of `interpolation`. If the parameter `limit` is provided, it
should be a tuple (lower, upper) of two values.

Parameters
----------
a : array_like
    A 1-D array of values from which to extract score.
per : array_like
    Percentile(s) at which to extract score.  Values should be in range
    [0,100].
limit : tuple, optional
    Tuple of two scalars, the lower and upper limits within which to
    compute the percentile. Values of `a` outside
    this (closed) interval will be ignored.
interpolation_method : {'fraction', 'lower', 'higher'}, optional
    Specifies the interpolation method to use,
    when the desired quantile lies between two data points `i` and `j`
    The following options are available (default is 'fraction'):

      * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the
        fractional part of the index surrounded by ``i`` and ``j``
      * 'lower': ``i``
      * 'higher': ``j``

axis : int, optional
    Axis along which the percentiles are computed. Default is None. If
    None, compute over the whole array `a`.

Returns
-------
score : float or ndarray
    Score at percentile(s).

See Also
--------
percentileofscore, numpy.percentile

Notes
-----
This function will become obsolete in the future.
For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality
that `scoreatpercentile` provides.  And it's significantly faster.
Therefore it's recommended to use `numpy.percentile` for users that have
numpy >= 1.9.

Examples
--------
>>> from scipy import stats
>>> a = np.arange(100)
>>> stats.scoreatpercentile(a, 50)
49.5
*)

val sem : ?axis:[`I of int | `None] -> ?ddof:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute standard error of the mean.

Calculate the standard error of the mean (or standard error of
measurement) of the values in the input array.

Parameters
----------
a : array_like
    An array containing the values for which the standard error is
    returned.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Delta degrees-of-freedom. How many degrees of freedom to adjust
    for bias in limited samples relative to the population estimate
    of variance. Defaults to 1.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
s : ndarray or float
    The standard error of the mean in the sample(s), along the input axis.

Notes
-----
The default value for `ddof` is different to the default (0) used by other
ddof containing routines, such as np.std and np.nanstd.

Examples
--------
Find standard error along the first axis:

>>> from scipy import stats
>>> a = np.arange(20).reshape(5,4)
>>> stats.sem(a)
array([ 2.8284,  2.8284,  2.8284,  2.8284])

Find standard error across the whole array, using n degrees of freedom:

>>> stats.sem(a, axis=None, ddof=0)
1.2893796958227628
*)

val siegelslopes : ?x:[>`Ndarray] Np.Obj.t -> ?method_:[`Hierarchical | `Separate] -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Siegel estimator for a set of points (x, y).

`siegelslopes` implements a method for robust linear regression
using repeated medians (see [1]_) to fit a line to the points (x, y).
The method is robust to outliers with an asymptotic breakdown point
of 50%.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
method : {'hierarchical', 'separate'}
    If 'hierarchical', estimate the intercept using the estimated
    slope ``medslope`` (default option).
    If 'separate', estimate the intercept independent of the estimated
    slope. See Notes for details.

Returns
-------
medslope : float
    Estimate of the slope of the regression line.
medintercept : float
    Estimate of the intercept of the regression line.

See also
--------
theilslopes : a similar technique without repeated medians

Notes
-----
With ``n = len(y)``, compute ``m_j`` as the median of
the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.
``medslope`` is then the median of all slopes ``m_j``.
Two ways are given to estimate the intercept in [1]_ which can be chosen
via the parameter ``method``.
The hierarchical approach uses the estimated slope ``medslope``
and computes ``medintercept`` as the median of ``y - medslope*x``.
The other approach estimates the intercept separately as follows: for
each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`
lines through the remaining points and take the median ``i_j``.
``medintercept`` is the median of the ``i_j``.

The implementation computes `n` times the median of a vector of size `n`
which can be slow for large vectors. There are more efficient algorithms
(see [2]_) which are not implemented here.

References
----------
.. [1] A. Siegel, 'Robust Regression Using Repeated Medians',
       Biometrika, Vol. 69, pp. 242-244, 1982.

.. [2] A. Stein and M. Werman, 'Finding the repeated median regression
       line', Proceedings of the Third Annual ACM-SIAM Symposium on
       Discrete Algorithms, pp. 409-413, 1992.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope and intercept.  For comparison, also compute the
least-squares fit with `linregress`:

>>> res = stats.siegelslopes(y, x)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Siegel regression line is shown in red. The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val sigmaclip : ?low:float -> ?high:float -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float)
(**
Perform iterative sigma-clipping of array elements.

Starting from the full sample, all elements outside the critical range are
removed, i.e. all elements of the input array `c` that satisfy either of
the following conditions::

    c < mean(c) - std(c)*low
    c > mean(c) + std(c)*high

The iteration continues with the updated sample until no
elements are outside the (updated) range.

Parameters
----------
a : array_like
    Data array, will be raveled if not 1-D.
low : float, optional
    Lower bound factor of sigma clipping. Default is 4.
high : float, optional
    Upper bound factor of sigma clipping. Default is 4.

Returns
-------
clipped : ndarray
    Input array with clipped elements removed.
lower : float
    Lower threshold value use for clipping.
upper : float
    Upper threshold value use for clipping.

Examples
--------
>>> from scipy.stats import sigmaclip
>>> a = np.concatenate((np.linspace(9.5, 10.5, 31),
...                     np.linspace(0, 20, 5)))
>>> fact = 1.5
>>> c, low, upp = sigmaclip(a, fact, fact)
>>> c
array([  9.96666667,  10.        ,  10.03333333,  10.        ])
>>> c.var(), c.std()
(0.00055555555555555165, 0.023570226039551501)
>>> low, c.mean() - fact*c.std(), c.min()
(9.9646446609406727, 9.9646446609406727, 9.9666666666666668)
>>> upp, c.mean() + fact*c.std(), c.max()
(10.035355339059327, 10.035355339059327, 10.033333333333333)

>>> a = np.concatenate((np.linspace(9.5, 10.5, 11),
...                     np.linspace(-100, -50, 3)))
>>> c, low, upp = sigmaclip(a, 1.8, 1.8)
>>> (c == np.linspace(9.5, 10.5, 11)).all()
True
*)

val skew : ?axis:[`I of int | `None] -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the sample skewness of a data set.

For normally distributed data, the skewness should be about zero. For
unimodal continuous distributions, a skewness value greater than zero means
that there is more weight in the right tail of the distribution. The
function `skewtest` can be used to determine if the skewness value
is close enough to zero, statistically speaking.

Parameters
----------
a : ndarray
    Input array.
axis : int or None, optional
    Axis along which skewness is calculated. Default is 0.
    If None, compute over the whole array `a`.
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
skewness : ndarray
    The skewness of values along an axis, returning 0 where all values are
    equal.

Notes
-----
The sample skewness is computed as the Fisher-Pearson coefficient
of skewness, i.e.

.. math:: 

    g_1=\frac{m_3}{m_2^{3/2}}

where

.. math:: 

    m_i=\frac{1}{N}\sum_{n=1}^N(x[n]-\bar{x})^i

is the biased sample :math:`i\texttt{th}` central moment, and :math:`\bar{x}` is
the sample mean.  If ``bias`` is False, the calculations are
corrected for bias and the value computed is the adjusted
Fisher-Pearson standardized moment coefficient, i.e.

.. math:: 

    G_1=\frac{k_3}{k_2^{3/2}}=
        \frac{\sqrt{N(N-1)}}{N-2}\frac{m_3}{m_2^{3/2}}.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.
   Section 2.2.24.1

Examples
--------
>>> from scipy.stats import skew
>>> skew([1, 2, 3, 4, 5])
0.0
>>> skew([2, 8, 0, 4, 1, 9, 9, 0])
0.2650554122698573
*)

val skewtest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Test whether the skew is different from the normal distribution.

This function tests the null hypothesis that the skewness of
the population that the sample was drawn from is the same
as that of a corresponding normal distribution.

Parameters
----------
a : array
    The data to be tested.
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    Two-sided p-value for the hypothesis test.

Notes
-----
The sample size must be at least 8.

References
----------
.. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,
        'A suggestion for using powerful and informative tests of
        normality', American Statistician 44, pp. 316-321, 1990.

Examples
--------
>>> from scipy.stats import skewtest
>>> skewtest([1, 2, 3, 4, 5, 6, 7, 8])
SkewtestResult(statistic=1.0108048609177787, pvalue=0.3121098361421897)
>>> skewtest([2, 8, 0, 4, 1, 9, 9, 0])
SkewtestResult(statistic=0.44626385374196975, pvalue=0.6554066631275459)
>>> skewtest([1, 2, 3, 4, 5, 6, 7, 8000])
SkewtestResult(statistic=3.571773510360407, pvalue=0.0003545719905823133)
>>> skewtest([100, 100, 100, 100, 100, 100, 100, 101])
SkewtestResult(statistic=3.5717766638478072, pvalue=0.000354567720281634)
*)

val spearmanr : ?b:Py.Object.t -> ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> unit -> (Py.Object.t * float)
(**
Calculate a Spearman correlation coefficient with associated p-value.

The Spearman rank-order correlation coefficient is a nonparametric measure
of the monotonicity of the relationship between two datasets. Unlike the
Pearson correlation, the Spearman correlation does not assume that both
datasets are normally distributed. Like other correlation coefficients,
this one varies between -1 and +1 with 0 implying no correlation.
Correlations of -1 or +1 imply an exact monotonic relationship. Positive
correlations imply that as x increases, so does y. Negative correlations
imply that as x increases, y decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Spearman correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
a, b : 1D or 2D array_like, b is optional
    One or two 1-D or 2-D arrays containing multiple variables and
    observations. When these are 1-D, each represents a vector of
    observations of a single variable. For the behavior in the 2-D case,
    see under ``axis``, below.
    Both arrays need to have the same length in the ``axis`` dimension.
axis : int or None, optional
    If axis=0 (default), then each column represents a variable, with
    observations in the rows. If axis=1, the relationship is transposed:
    each row represents a variable, while the columns contain observations.
    If axis=None, then both arrays will be raveled.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
correlation : float or ndarray (2-D square)
    Spearman correlation matrix or correlation coefficient (if only 2
    variables are given as parameters. Correlation matrix is square with
    length equal to total number of variables (columns or rows) in ``a``
    and ``b`` combined.
pvalue : float
    The two-sided p-value for a hypothesis test whose null hypothesis is
    that two sets of data are uncorrelated, has same dimension as rho.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.
   Section  14.7

Examples
--------
>>> from scipy import stats
>>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])
(0.82078268166812329, 0.088587005313543798)
>>> np.random.seed(1234321)
>>> x2n = np.random.randn(100, 2)
>>> y2n = np.random.randn(100, 2)
>>> stats.spearmanr(x2n)
(0.059969996999699973, 0.55338590803773591)
>>> stats.spearmanr(x2n[:,0], x2n[:,1])
(0.059969996999699973, 0.55338590803773591)
>>> rho, pval = stats.spearmanr(x2n, y2n)
>>> rho
array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],
       [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],
       [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],
       [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])
>>> pval
array([[ 0.        ,  0.55338591,  0.06435364,  0.53617935],
       [ 0.55338591,  0.        ,  0.27592895,  0.80234077],
       [ 0.06435364,  0.27592895,  0.        ,  0.73039992],
       [ 0.53617935,  0.80234077,  0.73039992,  0.        ]])
>>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1)
>>> rho
array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],
       [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],
       [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],
       [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])
>>> stats.spearmanr(x2n, y2n, axis=None)
(0.10816770419260482, 0.1273562188027364)
>>> stats.spearmanr(x2n.ravel(), y2n.ravel())
(0.10816770419260482, 0.1273562188027364)

>>> xint = np.random.randint(10, size=(100, 2))
>>> stats.spearmanr(xint)
(0.052760927029710199, 0.60213045837062351)
*)

val theilslopes : ?x:[>`Ndarray] Np.Obj.t -> ?alpha:float -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float * float * float)
(**
Computes the Theil-Sen estimator for a set of points (x, y).

`theilslopes` implements a method for robust linear regression.  It
computes the slope as the median of all slopes between paired values.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
alpha : float, optional
    Confidence degree between 0 and 1. Default is 95% confidence.
    Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are
    interpreted as 'find the 90% confidence interval'.

Returns
-------
medslope : float
    Theil slope.
medintercept : float
    Intercept of the Theil line, as ``median(y) - medslope*median(x)``.
lo_slope : float
    Lower bound of the confidence interval on `medslope`.
up_slope : float
    Upper bound of the confidence interval on `medslope`.

See also
--------
siegelslopes : a similar technique using repeated medians

Notes
-----
The implementation of `theilslopes` follows [1]_. The intercept is
not defined in [1]_, and here it is defined as ``median(y) -
medslope*median(x)``, which is given in [3]_. Other definitions of
the intercept exist in the literature. A confidence interval for
the intercept is not given as this question is not addressed in
[1]_.

References
----------
.. [1] P.K. Sen, 'Estimates of the regression coefficient based on Kendall's tau',
       J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.
.. [2] H. Theil, 'A rank-invariant method of linear and polynomial
       regression analysis I, II and III',  Nederl. Akad. Wetensch., Proc.
       53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.
.. [3] W.L. Conover, 'Practical nonparametric statistics', 2nd ed.,
       John Wiley and Sons, New York, pp. 493.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope, intercept and 90% confidence interval.  For comparison,
also compute the least-squares fit with `linregress`:

>>> res = stats.theilslopes(y, x, 0.90)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Theil-Sen regression line is shown in red, with the
dashed red lines illustrating the confidence interval of the slope (note
that the dashed red lines are not the confidence interval of the regression
as the confidence interval of the intercept is not included). The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, res[1] + res[2] * x, 'r--')
>>> ax.plot(x, res[1] + res[3] * x, 'r--')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val tiecorrect : [>`Ndarray] Np.Obj.t -> float
(**
Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.

Parameters
----------
rankvals : array_like
    A 1-D sequence of ranks.  Typically this will be the array
    returned by `~scipy.stats.rankdata`.

Returns
-------
factor : float
    Correction factor for U or H.

See Also
--------
rankdata : Assign ranks to the data
mannwhitneyu : Mann-Whitney rank test
kruskal : Kruskal-Wallis H test

References
----------
.. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral
       Sciences.  New York: McGraw-Hill.

Examples
--------
>>> from scipy.stats import tiecorrect, rankdata
>>> tiecorrect([1, 2.5, 2.5, 4])
0.9
>>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])
>>> ranks
array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])
>>> tiecorrect(ranks)
0.9833333333333333
*)

val tmax : ?upperlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed maximum.

This function computes the maximum value of an array along a given axis,
while ignoring values larger than a specified upper limit.

Parameters
----------
a : array_like
    Array of values.
upperlimit : None or float, optional
    Values in the input array greater than the given limit will be ignored.
    When upperlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the upper limit
    are included.  The default value is True.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
tmax : float, int or ndarray
    Trimmed maximum.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmax(x)
19

>>> stats.tmax(x, 13)
13

>>> stats.tmax(x, 13, inclusive=False)
12
*)

val tmean : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed mean.

This function finds the arithmetic mean of given values, ignoring values
outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored.  When limits is None (default), then all
    values are used.  Either of the limit values in the tuple can also be
    None representing a half-open interval.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to compute test. Default is None.

Returns
-------
tmean : float
    Trimmed mean.

See Also
--------
trim_mean : Returns mean after trimming a proportion from both tails.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmean(x)
9.5
>>> stats.tmean(x, (3,17))
10.0
*)

val tmin : ?lowerlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed minimum.

This function finds the miminum value of an array `a` along the
specified axis, but only considering values greater than a specified
lower limit.

Parameters
----------
a : array_like
    Array of values.
lowerlimit : None or float, optional
    Values in the input array less than the given limit will be ignored.
    When lowerlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the lower limit
    are included.  The default value is True.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
tmin : float, int or ndarray
    Trimmed minimum.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmin(x)
0

>>> stats.tmin(x, 13)
13

>>> stats.tmin(x, 13, inclusive=False)
14
*)

val trim1 : ?tail:[`Left | `Right] -> ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Slice off a proportion from ONE end of the passed array distribution.

If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'
10% of scores. The lowest or highest values are trimmed (depending on
the tail).
Slice off less if proportion results in a non-integer slice index
(i.e. conservatively slices off `proportiontocut` ).

Parameters
----------
a : array_like
    Input array.
proportiontocut : float
    Fraction to cut off of 'left' or 'right' of distribution.
tail : {'left', 'right'}, optional
    Defaults to 'right'.
axis : int or None, optional
    Axis along which to trim data. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
trim1 : ndarray
    Trimmed version of array `a`. The order of the trimmed content is
    undefined.
*)

val trim_mean : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return mean of array after trimming distribution from both tails.

If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of
scores. The input is sorted before slicing. Slices off less if proportion
results in a non-integer slice index (i.e., conservatively slices off
`proportiontocut` ).

Parameters
----------
a : array_like
    Input array.
proportiontocut : float
    Fraction to cut off of both tails of the distribution.
axis : int or None, optional
    Axis along which the trimmed means are computed. Default is 0.
    If None, compute over the whole array `a`.

Returns
-------
trim_mean : ndarray
    Mean of trimmed array.

See Also
--------
trimboth
tmean : Compute the trimmed mean ignoring values outside given `limits`.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.trim_mean(x, 0.1)
9.5
>>> x2 = x.reshape(5, 4)
>>> x2
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15],
       [16, 17, 18, 19]])
>>> stats.trim_mean(x2, 0.25)
array([  8.,   9.,  10.,  11.])
>>> stats.trim_mean(x2, 0.25, axis=1)
array([  1.5,   5.5,   9.5,  13.5,  17.5])
*)

val trimboth : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Slice off a proportion of items from both ends of an array.

Slice off the passed proportion of items from both ends of the passed
array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**
rightmost 10% of scores). The trimmed values are the lowest and
highest ones.
Slice off less if proportion results in a non-integer slice index (i.e.
conservatively slices off `proportiontocut`).

Parameters
----------
a : array_like
    Data to trim.
proportiontocut : float
    Proportion (in range 0-1) of total data set to trim of each end.
axis : int or None, optional
    Axis along which to trim data. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
out : ndarray
    Trimmed version of array `a`. The order of the trimmed content
    is undefined.

See Also
--------
trim_mean

Examples
--------
>>> from scipy import stats
>>> a = np.arange(20)
>>> b = stats.trimboth(a, 0.1)
>>> b.shape
(16,)
*)

val tsem : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed standard error of the mean.

This function finds the standard error of the mean for given
values, ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tsem : float
    Trimmed standard error of the mean.

Notes
-----
`tsem` uses unbiased sample standard deviation, i.e. it uses a
correction factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tsem(x)
1.3228756555322954
>>> stats.tsem(x, (3,17))
1.1547005383792515
*)

val tstd : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed sample standard deviation.

This function finds the sample standard deviation of given values,
ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tstd : float
    Trimmed sample standard deviation.

Notes
-----
`tstd` computes the unbiased sample standard deviation, i.e. it uses a
correction factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tstd(x)
5.9160797830996161
>>> stats.tstd(x, (3,17))
4.4721359549995796
*)

val ttest_1samp : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the T-test for the mean of ONE group of scores.

This is a two-sided test for the null hypothesis that the expected value
(mean) of a sample of independent observations `a` is equal to the given
population mean, `popmean`.

Parameters
----------
a : array_like
    Sample observation.
popmean : float or array_like
    Expected value in null hypothesis. If array_like, then it must have the
    same shape as `a` excluding the axis dimension.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    t-statistic.
pvalue : float or array
    Two-sided p-value.

Examples
--------
>>> from scipy import stats

>>> np.random.seed(7654567)  # fix seed to get the same result
>>> rvs = stats.norm.rvs(loc=5, scale=10, size=(50,2))

Test if mean of random sample is equal to true mean, and different mean.
We reject the null hypothesis in the second case and don't reject it in
the first case.

>>> stats.ttest_1samp(rvs,5.0)
(array([-0.68014479, -0.04323899]), array([ 0.49961383,  0.96568674]))
>>> stats.ttest_1samp(rvs,0.0)
(array([ 2.77025808,  4.11038784]), array([ 0.00789095,  0.00014999]))

Examples using axis and non-scalar dimension for population mean.

>>> stats.ttest_1samp(rvs,[5.0,0.0])
(array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))
>>> stats.ttest_1samp(rvs.T,[5.0,0.0],axis=1)
(array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))
>>> stats.ttest_1samp(rvs,[[5.0],[0.0]])
(array([[-0.68014479, -0.04323899],
       [ 2.77025808,  4.11038784]]), array([[  4.99613833e-01,   9.65686743e-01],
       [  7.89094663e-03,   1.49986458e-04]]))
*)

val ttest_ind : ?axis:[`I of int | `None] -> ?equal_var:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the T-test for the means of *two independent* samples of scores.

This is a two-sided test for the null hypothesis that 2 independent samples
have identical average (expected) values. This test assumes that the
populations have identical variances by default.

Parameters
----------
a, b : array_like
    The arrays must have the same shape, except in the dimension
    corresponding to `axis` (the first, by default).
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
equal_var : bool, optional
    If True (default), perform a standard independent 2 sample test
    that assumes equal population variances [1]_.
    If False, perform Welch's t-test, which does not assume equal
    population variance [2]_.

    .. versionadded:: 0.11.0
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values


Returns
-------
statistic : float or array
    The calculated t-statistic.
pvalue : float or array
    The two-tailed p-value.

Notes
-----
We can use this test, if we observe two independent samples from
the same or different population, e.g. exam scores of boys and
girls or of two ethnic groups. The test measures whether the
average (expected) value differs significantly across samples. If
we observe a large p-value, for example larger than 0.05 or 0.1,
then we cannot reject the null hypothesis of identical average scores.
If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,
then we reject the null hypothesis of equal averages.

References
----------
.. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test

.. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)

Test with sample with identical means:

>>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> stats.ttest_ind(rvs1,rvs2)
(0.26833823296239279, 0.78849443369564776)
>>> stats.ttest_ind(rvs1,rvs2, equal_var = False)
(0.26833823296239279, 0.78849452749500748)

`ttest_ind` underestimates p for unequal variances:

>>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)
>>> stats.ttest_ind(rvs1, rvs3)
(-0.46580283298287162, 0.64145827413436174)
>>> stats.ttest_ind(rvs1, rvs3, equal_var = False)
(-0.46580283298287162, 0.64149646246569292)

When n1 != n2, the equal variance t-statistic is no longer equal to the
unequal variance t-statistic:

>>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs4)
(-0.99882539442782481, 0.3182832709103896)
>>> stats.ttest_ind(rvs1, rvs4, equal_var = False)
(-0.69712570584654099, 0.48716927725402048)

T-test with different means, variance, and n:

>>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs5)
(-1.4679669854490653, 0.14263895620529152)
>>> stats.ttest_ind(rvs1, rvs5, equal_var = False)
(-0.94365973617132992, 0.34744170334794122)
*)

val ttest_ind_from_stats : ?equal_var:bool -> mean1:[>`Ndarray] Np.Obj.t -> std1:[>`Ndarray] Np.Obj.t -> nobs1:[>`Ndarray] Np.Obj.t -> mean2:[>`Ndarray] Np.Obj.t -> std2:[>`Ndarray] Np.Obj.t -> nobs2:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
T-test for means of two independent samples from descriptive statistics.

This is a two-sided test for the null hypothesis that two independent
samples have identical average (expected) values.

Parameters
----------
mean1 : array_like
    The mean(s) of sample 1.
std1 : array_like
    The standard deviation(s) of sample 1.
nobs1 : array_like
    The number(s) of observations of sample 1.
mean2 : array_like
    The mean(s) of sample 2.
std2 : array_like
    The standard deviations(s) of sample 2.
nobs2 : array_like
    The number(s) of observations of sample 2.
equal_var : bool, optional
    If True (default), perform a standard independent 2 sample test
    that assumes equal population variances [1]_.
    If False, perform Welch's t-test, which does not assume equal
    population variance [2]_.

Returns
-------
statistic : float or array
    The calculated t-statistics.
pvalue : float or array
    The two-tailed p-value.

See Also
--------
scipy.stats.ttest_ind

Notes
-----
.. versionadded:: 0.16.0

References
----------
.. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test

.. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test

Examples
--------
Suppose we have the summary data for two samples, as follows::

                     Sample   Sample
               Size   Mean   Variance
    Sample 1    13    15.0     87.5
    Sample 2    11    12.0     39.0

Apply the t-test to this data (with the assumption that the population
variances are equal):

>>> from scipy.stats import ttest_ind_from_stats
>>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,
...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)
Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)

For comparison, here is the data from which those summary statistics
were taken.  With this data, we can compute the same result using
`scipy.stats.ttest_ind`:

>>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])
>>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])
>>> from scipy.stats import ttest_ind
>>> ttest_ind(a, b)
Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)

Suppose we instead have binary data and would like to apply a t-test to
compare the proportion of 1s in two independent groups::

                      Number of    Sample     Sample
                Size    ones        Mean     Variance
    Sample 1    150      30         0.2        0.16
    Sample 2    200      45         0.225      0.174375

The sample mean :math:`\hat{p}` is the proportion of ones in the sample
and the variance for a binary observation is estimated by
:math:`\hat{p}(1-\hat{p})`.

>>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,
...                      mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)
Ttest_indResult(statistic=-0.564327545549774, pvalue=0.5728947691244874)

For comparison, we could compute the t statistic and p-value using
arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.

>>> group1 = np.array([1]*30 + [0]*(150-30))
>>> group2 = np.array([1]*45 + [0]*(200-45))
>>> ttest_ind(group1, group2)
Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)
*)

val ttest_rel : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the t-test on TWO RELATED samples of scores, a and b.

This is a two-sided test for the null hypothesis that 2 related or
repeated samples have identical average (expected) values.

Parameters
----------
a, b : array_like
    The arrays must have the same shape.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    t-statistic.
pvalue : float or array
    Two-sided p-value.

Notes
-----
Examples for use are scores of the same set of student in
different exams, or repeated sampling from the same units. The
test measures whether the average score differs significantly
across samples (e.g. exams). If we observe a large p-value, for
example greater than 0.05 or 0.1 then we cannot reject the null
hypothesis of identical average scores. If the p-value is smaller
than the threshold, e.g. 1%, 5% or 10%, then we reject the null
hypothesis of equal averages. Small p-values are associated with
large t-statistics.

References
----------
https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678) # fix random seed to get same numbers

>>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> rvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) +
...         stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs2)
(0.24101764965300962, 0.80964043445811562)
>>> rvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) +
...         stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs3)
(-3.9995108708727933, 7.3082402191726459e-005)
*)

val tvar : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed variance.

This function computes the sample variance of an array of values,
while ignoring values which are outside of given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tvar : float
    Trimmed variance.

Notes
-----
`tvar` computes the unbiased sample variance, i.e. it uses a correction
factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tvar(x)
35.0
>>> stats.tvar(x, (3,17))
20.0
*)

val variation : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the coefficient of variation.

The coefficient of variation is the ratio of the biased standard
deviation to the mean.

Parameters
----------
a : array_like
    Input array.
axis : int or None, optional
    Axis along which to calculate the coefficient of variation. Default
    is 0. If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
variation : ndarray
    The calculated variation along the requested axis.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.

Examples
--------
>>> from scipy.stats import variation
>>> variation([1, 2, 3, 4, 5])
0.47140452079103173
*)

val wasserstein_distance : ?u_weights:Py.Object.t -> ?v_weights:Py.Object.t -> u_values:Py.Object.t -> v_values:Py.Object.t -> unit -> float
(**
Compute the first Wasserstein distance between two 1D distributions.

This distance is also known as the earth mover's distance, since it can be
seen as the minimum amount of 'work' required to transform :math:`u` into
:math:`v`, where 'work' is measured as the amount of distribution weight
that must be moved, multiplied by the distance it has to be moved.

.. versionadded:: 1.0.0

Parameters
----------
u_values, v_values : array_like
    Values observed in the (empirical) distribution.
u_weights, v_weights : array_like, optional
    Weight for each value. If unspecified, each value is assigned the same
    weight.
    `u_weights` (resp. `v_weights`) must have the same length as
    `u_values` (resp. `v_values`). If the weight sum differs from 1, it
    must still be positive and finite so that the weights can be normalized
    to sum to 1.

Returns
-------
distance : float
    The computed distance between the distributions.

Notes
-----
The first Wasserstein distance between the distributions :math:`u` and
:math:`v` is:

.. math::

    l_1 (u, v) = \inf_{\pi \in \Gamma (u, v)} \int_{\mathbb{R} \times
    \mathbb{R}} |x-y| \mathrm{d} \pi (x, y)

where :math:`\Gamma (u, v)` is the set of (probability) distributions on
:math:`\mathbb{R} \times \mathbb{R}` whose marginals are :math:`u` and
:math:`v` on the first and second factors respectively.

If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and
:math:`v`, this distance also equals to:

.. math::

    l_1(u, v) = \int_{-\infty}^{+\infty} |U-V|

See [2]_ for a proof of the equivalence of both definitions.

The input distributions can be empirical, therefore coming from samples
whose values are effectively inputs of the function, or they can be seen as
generalized functions, in which case they are weighted sums of Dirac delta
functions located at the specified values.

References
----------
.. [1] 'Wasserstein metric', https://en.wikipedia.org/wiki/Wasserstein_metric
.. [2] Ramdas, Garcia, Cuturi 'On Wasserstein Two Sample Testing and Related
       Families of Nonparametric Tests' (2015). :arXiv:`1509.02237`.

Examples
--------
>>> from scipy.stats import wasserstein_distance
>>> wasserstein_distance([0, 1, 3], [5, 6, 8])
5.0
>>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])
0.25
>>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],
...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])
4.0781331438047861
*)

val weightedtau : ?rank:[`Bool of bool | `Array_like_of_ints of Py.Object.t] -> ?weigher:Py.Object.t -> ?additive:bool -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute a weighted version of Kendall's :math:`\tau`.

The weighted :math:`\tau` is a weighted version of Kendall's
:math:`\tau` in which exchanges of high weight are more influential than
exchanges of low weight. The default parameters compute the additive
hyperbolic version of the index, :math:`\tau_\mathrm h`, which has
been shown to provide the best balance between important and
unimportant elements [1]_.

The weighting is defined by means of a rank array, which assigns a
nonnegative rank to each element, and a weigher function, which
assigns a weight based from the rank to each element. The weight of an
exchange is then the sum or the product of the weights of the ranks of
the exchanged elements. The default parameters compute
:math:`\tau_\mathrm h`: an exchange between elements with rank
:math:`r` and :math:`s` (starting from zero) has weight
:math:`1/(r+1) + 1/(s+1)`.

Specifying a rank array is meaningful only if you have in mind an
external criterion of importance. If, as it usually happens, you do
not have in mind a specific rank, the weighted :math:`\tau` is
defined by averaging the values obtained using the decreasing
lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the
behavior with default parameters.

Note that if you are computing the weighted :math:`\tau` on arrays of
ranks, rather than of scores (i.e., a larger value implies a lower
rank) you must negate the ranks, so that elements of higher rank are
associated with a larger value.

Parameters
----------
x, y : array_like
    Arrays of scores, of the same shape. If arrays are not 1-D, they will
    be flattened to 1-D.
rank : array_like of ints or bool, optional
    A nonnegative rank assigned to each element. If it is None, the
    decreasing lexicographical rank by (`x`, `y`) will be used: elements of
    higher rank will be those with larger `x`-values, using `y`-values to
    break ties (in particular, swapping `x` and `y` will give a different
    result). If it is False, the element indices will be used
    directly as ranks. The default is True, in which case this
    function returns the average of the values obtained using the
    decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).
weigher : callable, optional
    The weigher function. Must map nonnegative integers (zero
    representing the most important element) to a nonnegative weight.
    The default, None, provides hyperbolic weighing, that is,
    rank :math:`r` is mapped to weight :math:`1/(r+1)`.
additive : bool, optional
    If True, the weight of an exchange is computed by adding the
    weights of the ranks of the exchanged elements; otherwise, the weights
    are multiplied. The default is True.

Returns
-------
correlation : float
   The weighted :math:`\tau` correlation index.
pvalue : float
   Presently ``np.nan``, as the null statistics is unknown (even in the
   additive hyperbolic case).

See Also
--------
kendalltau : Calculates Kendall's tau.
spearmanr : Calculates a Spearman rank-order correlation coefficient.
theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).

Notes
-----
This function uses an :math:`O(n \log n)`, mergesort-based algorithm
[1]_ that is a weighted extension of Knight's algorithm for Kendall's
:math:`\tau` [2]_. It can compute Shieh's weighted :math:`\tau` [3]_
between rankings without ties (i.e., permutations) by setting
`additive` and `rank` to False, as the definition given in [1]_ is a
generalization of Shieh's.

NaNs are considered the smallest possible score.

.. versionadded:: 0.19.0

References
----------
.. [1] Sebastiano Vigna, 'A weighted correlation index for rankings with
       ties', Proceedings of the 24th international conference on World
       Wide Web, pp. 1166-1176, ACM, 2015.
.. [2] W.R. Knight, 'A Computer Method for Calculating Kendall's Tau with
       Ungrouped Data', Journal of the American Statistical Association,
       Vol. 61, No. 314, Part 1, pp. 436-439, 1966.
.. [3] Grace S. Shieh. 'A weighted Kendall's tau statistic', Statistics &
       Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.

Examples
--------
>>> from scipy import stats
>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> tau, p_value = stats.weightedtau(x, y)
>>> tau
-0.56694968153682723
>>> p_value
nan
>>> tau, p_value = stats.weightedtau(x, y, additive=False)
>>> tau
-0.62205716951801038

NaNs are considered the smallest possible score:

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, np.nan]
>>> tau, _ = stats.weightedtau(x, y)
>>> tau
-0.56694968153682723

This is exactly Kendall's tau:

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)
>>> tau
-0.47140452079103173

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> stats.weightedtau(x, y, rank=None)
WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)
>>> stats.weightedtau(y, x, rank=None)
WeightedTauResult(correlation=-0.7181341329699028, pvalue=nan)
*)

val zmap : ?axis:[`I of int | `None] -> ?ddof:int -> scores:[>`Ndarray] Np.Obj.t -> compare:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the relative z-scores.

Return an array of z-scores, i.e., scores that are standardized to
zero mean and unit variance, where mean and variance are calculated
from the comparison array.

Parameters
----------
scores : array_like
    The input for which z-scores are calculated.
compare : array_like
    The input from which the mean and standard deviation of the
    normalization are taken; assumed to have the same dimension as
    `scores`.
axis : int or None, optional
    Axis over which mean and variance of `compare` are calculated.
    Default is 0. If None, compute over the whole array `scores`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.

Returns
-------
zscore : array_like
    Z-scores, in the same shape as `scores`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> from scipy.stats import zmap
>>> a = [0.5, 2.0, 2.5, 3]
>>> b = [0, 1, 2, 3, 4]
>>> zmap(a, b)
array([-1.06066017,  0.        ,  0.35355339,  0.70710678])
*)

val zscore : ?axis:[`I of int | `None] -> ?ddof:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the z score.

Compute the z score of each value in the sample, relative to the
sample mean and standard deviation.

Parameters
----------
a : array_like
    An array like object containing the sample data.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
zscore : array_like
    The z-scores, standardized by mean and standard deviation of
    input array `a`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,
...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])
>>> from scipy import stats
>>> stats.zscore(a)
array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,
        0.6748, -1.1488, -1.3324])

Computing along a specified axis, using n-1 degrees of freedom
(``ddof=1``) to calculate the standard deviation:

>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],
...               [ 0.7149,  0.0775,  0.6072,  0.9656],
...               [ 0.6341,  0.1403,  0.9759,  0.4064],
...               [ 0.5918,  0.6948,  0.904 ,  0.3721],
...               [ 0.0921,  0.2481,  0.1188,  0.1366]])
>>> stats.zscore(b, axis=1, ddof=1)
array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],
       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],
       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],
       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],
       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])
*)


end

val alpha : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Alpha_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An alpha continuous random variable.

As an instance of the `rv_continuous` class, `alpha` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `alpha` ([1]_, [2]_) is:

.. math::

    f(x, a) = \frac{1}{x^2 \Phi(a) \sqrt{2\pi}} *
              \exp(-\frac{1}{2} (a-1/x)^2)

where :math:`\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.

`alpha` takes ``a`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``alpha.pdf(x, a, loc, scale)`` is identically
equivalent to ``alpha.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Johnson, Kotz, and Balakrishnan, 'Continuous Univariate
       Distributions, Volume 1', Second Edition, John Wiley and Sons,
       p. 173 (1994).
.. [2] Anthony A. Salvia, 'Reliability applications of the Alpha
       Distribution', IEEE Transactions on Reliability, Vol. R-34,
       No. 3, pp. 251-252 (1985).

Examples
--------
>>> from scipy.stats import alpha
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 3.57
>>> mean, var, skew, kurt = alpha.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(alpha.ppf(0.01, a),
...                 alpha.ppf(0.99, a), 100)
>>> ax.plot(x, alpha.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='alpha pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = alpha(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = alpha.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], alpha.cdf(vals, a))
True

Generate random numbers:

>>> r = alpha.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val anderson : ?dist:[`Expon | `Logistic | `Gumbel | `Gumbel_l | `PyObject of Py.Object.t] -> x:[>`Ndarray] Np.Obj.t -> unit -> (float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Anderson-Darling test for data coming from a particular distribution.

The Anderson-Darling tests the null hypothesis that a sample is
drawn from a population that follows a particular distribution.
For the Anderson-Darling test, the critical values depend on
which distribution is being tested against.  This function works
for normal, exponential, logistic, or Gumbel (Extreme Value
Type I) distributions.

Parameters
----------
x : array_like
    Array of sample data.
dist : {'norm','expon','logistic','gumbel','gumbel_l', gumbel_r',
    'extreme1'}, optional
    the type of distribution to test against.  The default is 'norm'
    and 'extreme1', 'gumbel_l' and 'gumbel' are synonyms.

Returns
-------
statistic : float
    The Anderson-Darling test statistic.
critical_values : list
    The critical values for this distribution.
significance_level : list
    The significance levels for the corresponding critical values
    in percents.  The function returns critical values for a
    differing set of significance levels depending on the
    distribution that is being tested against.

See Also
--------
kstest : The Kolmogorov-Smirnov test for goodness-of-fit.

Notes
-----
Critical values provided are for the following significance levels:

normal/exponenential
    15%, 10%, 5%, 2.5%, 1%
logistic
    25%, 10%, 5%, 2.5%, 1%, 0.5%
Gumbel
    25%, 10%, 5%, 2.5%, 1%

If the returned statistic is larger than these critical values then
for the corresponding significance level, the null hypothesis that
the data come from the chosen distribution can be rejected.
The returned statistic is referred to as 'A2' in the references.

References
----------
.. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm
.. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and
       Some Comparisons, Journal of the American Statistical Association,
       Vol. 69, pp. 730-737.
.. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit
       Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,
       pp. 357-369.
.. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value
       Distribution, Biometrika, Vol. 64, pp. 583-588.
.. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference
       to Tests for Exponentiality , Technical Report No. 262,
       Department of Statistics, Stanford University, Stanford, CA.
.. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution
       Based on the Empirical Distribution Function, Biometrika, Vol. 66,
       pp. 591-595.
*)

val anderson_ksamp : ?midrank:bool -> samples:Py.Object.t -> unit -> (float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
The Anderson-Darling test for k-samples.

The k-sample Anderson-Darling test is a modification of the
one-sample Anderson-Darling test. It tests the null hypothesis
that k-samples are drawn from the same population without having
to specify the distribution function of that population. The
critical values depend on the number of samples.

Parameters
----------
samples : sequence of 1-D array_like
    Array of sample data in arrays.
midrank : bool, optional
    Type of Anderson-Darling test which is computed. Default
    (True) is the midrank test applicable to continuous and
    discrete populations. If False, the right side empirical
    distribution is used.

Returns
-------
statistic : float
    Normalized k-sample Anderson-Darling test statistic.
critical_values : array
    The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,
    0.5%, 0.1%.
significance_level : float
    An approximate significance level at which the null hypothesis for the
    provided samples can be rejected. The value is floored / capped at
    0.1% / 25%.

Raises
------
ValueError
    If less than 2 samples are provided, a sample is empty, or no
    distinct observations are in the samples.

See Also
--------
ks_2samp : 2 sample Kolmogorov-Smirnov test
anderson : 1 sample Anderson-Darling test

Notes
-----
[1]_ defines three versions of the k-sample Anderson-Darling test:
one for continuous distributions and two for discrete
distributions, in which ties between samples may occur. The
default of this routine is to compute the version based on the
midrank empirical distribution function. This test is applicable
to continuous and discrete data. If midrank is set to False, the
right side empirical distribution is used for a test for discrete
data. According to [1]_, the two discrete test statistics differ
only slightly if a few collisions due to round-off errors occur in
the test not adjusted for ties between samples.

The critical values corresponding to the significance levels from 0.01
to 0.25 are taken from [1]_. p-values are floored / capped
at 0.1% / 25%. Since the range of critical values might be extended in
future releases, it is recommended not to test ``p == 0.25``, but rather
``p >= 0.25`` (analogously for the lower bound).

.. versionadded:: 0.14.0

References
----------
.. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample
       Anderson-Darling Tests, Journal of the American Statistical
       Association, Vol. 82, pp. 918-924.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(314159)

The null hypothesis that the two random samples come from the same
distribution can be rejected at the 5% level because the returned
test value is greater than the critical value for 5% (1.961) but
not at the 2.5% level. The interpolation gives an approximate
significance level of 3.2%:

>>> stats.anderson_ksamp([np.random.normal(size=50),
... np.random.normal(loc=0.5, size=30)])
(2.4615796189876105,
  array([ 0.325,  1.226,  1.961,  2.718,  3.752, 4.592, 6.546]),
  0.03176687568842282)


The null hypothesis cannot be rejected for three samples from an
identical distribution. The reported p-value (25%) has been capped and
may not be very accurate (since it corresponds to the value 0.449
whereas the statistic is -0.731):

>>> stats.anderson_ksamp([np.random.normal(size=50),
... np.random.normal(size=30), np.random.normal(size=20)])
(-0.73091722665244196,
  array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,
  4.07210043, 5.56419101]),
  0.25)
*)

val anglit : ?loc:float -> ?scale:float -> unit -> [`Anglit_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An anglit continuous random variable.

As an instance of the `rv_continuous` class, `anglit` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `anglit` is:

.. math::

    f(x) = \sin(2x + \pi/2) = \cos(2x)

for :math:`-\pi/4 \le x \le \pi/4`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``anglit.pdf(x, loc, scale)`` is identically
equivalent to ``anglit.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import anglit
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = anglit.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(anglit.ppf(0.01),
...                 anglit.ppf(0.99), 100)
>>> ax.plot(x, anglit.pdf(x),
...        'r-', lw=5, alpha=0.6, label='anglit pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = anglit()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = anglit.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], anglit.cdf(vals))
True

Generate random numbers:

>>> r = anglit.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val ansari : x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Perform the Ansari-Bradley test for equal scale parameters.

The Ansari-Bradley test is a non-parametric test for the equality
of the scale parameter of the distributions from which two
samples were drawn.

Parameters
----------
x, y : array_like
    Arrays of sample data.

Returns
-------
statistic : float
    The Ansari-Bradley test statistic.
pvalue : float
    The p-value of the hypothesis test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
mood : A non-parametric test for the equality of two scale parameters

Notes
-----
The p-value given is exact when the sample sizes are both less than
55 and there are no ties, otherwise a normal approximation for the
p-value is used.

References
----------
.. [1] Sprent, Peter and N.C. Smeeton.  Applied nonparametric statistical
       methods.  3rd ed. Chapman and Hall/CRC. 2001.  Section 5.8.2.
*)

val arcsine : ?loc:float -> ?scale:float -> unit -> [`Arcsine_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An arcsine continuous random variable.

As an instance of the `rv_continuous` class, `arcsine` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `arcsine` is:

.. math::

    f(x) = \frac{1}{\pi \sqrt{x (1-x)}}

for :math:`0 < x < 1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``arcsine.pdf(x, loc, scale)`` is identically
equivalent to ``arcsine.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import arcsine
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = arcsine.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(arcsine.ppf(0.01),
...                 arcsine.ppf(0.99), 100)
>>> ax.plot(x, arcsine.pdf(x),
...        'r-', lw=5, alpha=0.6, label='arcsine pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = arcsine()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = arcsine.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], arcsine.cdf(vals))
True

Generate random numbers:

>>> r = arcsine.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val argus : ?loc:float -> ?scale:float -> chi:Py.Object.t -> unit -> [`Argus_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Argus distribution

As an instance of the `rv_continuous` class, `argus` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(chi, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, chi, loc=0, scale=1)
    Probability density function.
logpdf(x, chi, loc=0, scale=1)
    Log of the probability density function.
cdf(x, chi, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, chi, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, chi, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, chi, loc=0, scale=1)
    Log of the survival function.
ppf(q, chi, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, chi, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, chi, loc=0, scale=1)
    Non-central moment of order n
stats(chi, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(chi, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, chi, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(chi,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(chi, loc=0, scale=1)
    Median of the distribution.
mean(chi, loc=0, scale=1)
    Mean of the distribution.
var(chi, loc=0, scale=1)
    Variance of the distribution.
std(chi, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, chi, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `argus` is:

.. math::

    f(x, \chi) = \frac{\chi^3}{\sqrt{2\pi} \Psi(\chi)} x \sqrt{1-x^2}
                 \exp(-\chi^2 (1 - x^2)/2)

for :math:`0 < x < 1`, where

.. math::

    \Psi(\chi) = \Phi(\chi) - \chi \phi(\chi) - 1/2

with :math:`\Phi` and :math:`\phi` being the CDF and PDF of a standard
normal distribution, respectively.

`argus` takes :math:`\chi` as shape a parameter.

References
----------

.. [1] 'ARGUS distribution',
       https://en.wikipedia.org/wiki/ARGUS_distribution

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``argus.pdf(x, chi, loc, scale)`` is identically
equivalent to ``argus.pdf(y, chi) / scale`` with
``y = (x - loc) / scale``.

.. versionadded:: 0.19.0

Examples
--------
>>> from scipy.stats import argus
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> chi = 1
>>> mean, var, skew, kurt = argus.stats(chi, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(argus.ppf(0.01, chi),
...                 argus.ppf(0.99, chi), 100)
>>> ax.plot(x, argus.pdf(x, chi),
...        'r-', lw=5, alpha=0.6, label='argus pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = argus(chi)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = argus.ppf([0.001, 0.5, 0.999], chi)
>>> np.allclose([0.001, 0.5, 0.999], argus.cdf(vals, chi))
True

Generate random numbers:

>>> r = argus.rvs(chi, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val bartlett : Py.Object.t list -> (float * float)
(**
Perform Bartlett's test for equal variances.

Bartlett's test tests the null hypothesis that all input samples
are from populations with equal variances.  For samples
from significantly non-normal populations, Levene's test
`levene` is more robust.

Parameters
----------
sample1, sample2,... : array_like
    arrays of sample data.  Only 1d arrays are accepted, they may have
    different lengths.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value of the test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
levene : A robust parametric test for equality of k variances

Notes
-----
Conover et al. (1981) examine many of the existing parametric and
nonparametric tests by extensive simulations and they conclude that the
tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be
superior in terms of robustness of departures from normality and power
([3]_).

References
----------
.. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm

.. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical
          Methods, Eighth Edition, Iowa State University Press.

.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.

.. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical
       Tests. Proceedings of the Royal Society of London. Series A,
       Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.
*)

val bayes_mvs : ?alpha:float -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Bayesian confidence intervals for the mean, var, and std.

Parameters
----------
data : array_like
    Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.
    Requires 2 or more data points.
alpha : float, optional
    Probability that the returned confidence interval contains
    the true parameter.

Returns
-------
mean_cntr, var_cntr, std_cntr : tuple
    The three results are for the mean, variance and standard deviation,
    respectively.  Each result is a tuple of the form::

        (center, (lower, upper))

    with `center` the mean of the conditional pdf of the value given the
    data, and `(lower, upper)` a confidence interval, centered on the
    median, containing the estimate to a probability ``alpha``.

See Also
--------
mvsdist

Notes
-----
Each tuple of mean, variance, and standard deviation estimates represent
the (center, (lower, upper)) with center the mean of the conditional pdf
of the value given the data and (lower, upper) is a confidence interval
centered on the median, containing the estimate to a probability
``alpha``.

Converts data to 1-D and assumes all data has the same mean and variance.
Uses Jeffrey's prior for variance and std.

Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``

References
----------
T.E. Oliphant, 'A Bayesian perspective on estimating mean, variance, and
standard-deviation from data', https://scholarsarchive.byu.edu/facpub/278,
2006.

Examples
--------
First a basic example to demonstrate the outputs:

>>> from scipy import stats
>>> data = [6, 9, 12, 7, 8, 8, 13]
>>> mean, var, std = stats.bayes_mvs(data)
>>> mean
Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))
>>> var
Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))
>>> std
Std_dev(statistic=2.9724954732045084, minmax=(1.7823367265645143, 4.945614605014631))

Now we generate some normally distributed random data, and get estimates of
mean and standard deviation with 95% confidence intervals for those
estimates:

>>> n_samples = 100000
>>> data = stats.norm.rvs(size=n_samples)
>>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)

>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.hist(data, bins=100, density=True, label='Histogram of data')
>>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')
>>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',
...            alpha=0.2, label=r'Estimated mean (95% limits)')
>>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')
>>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,
...            label=r'Estimated scale (95% limits)')

>>> ax.legend(fontsize=10)
>>> ax.set_xlim([-4, 4])
>>> ax.set_ylim([0, 0.5])
>>> plt.show()
*)

val bernoulli : ?loc:float -> p:Py.Object.t -> unit -> [`Bernoulli_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Bernoulli discrete random variable.

As an instance of the `rv_discrete` class, `bernoulli` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `bernoulli` is:

.. math::

   f(k) = \begin{cases}1-p  &\text{if } k = 0\\
                       p    &\text{if } k = 1\end{cases}

for :math:`k` in :math:`\{0, 1\}`.

`bernoulli` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``bernoulli.pmf(k, p, loc)`` is identically
equivalent to ``bernoulli.pmf(k - loc, p)``.

Examples
--------
>>> from scipy.stats import bernoulli
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.3
>>> mean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(bernoulli.ppf(0.01, p),
...               bernoulli.ppf(0.99, p))
>>> ax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')
>>> ax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = bernoulli(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = bernoulli.cdf(x, p)
>>> np.allclose(x, bernoulli.ppf(prob, p))
True

Generate random numbers:

>>> r = bernoulli.rvs(p, size=1000)
*)

val beta : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Beta_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A beta continuous random variable.

As an instance of the `rv_continuous` class, `beta` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `beta` is:

.. math::

    f(x, a, b) = \frac{\Gamma(a+b) x^{a-1} (1-x)^{b-1}}
                      {\Gamma(a) \Gamma(b)}

for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`beta` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``beta.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import beta
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 2.31, 0.627
>>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(beta.ppf(0.01, a, b),
...                 beta.ppf(0.99, a, b), 100)
>>> ax.plot(x, beta.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='beta pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = beta(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))
True

Generate random numbers:

>>> r = beta.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val betabinom : ?loc:float -> n:Py.Object.t -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Betabinom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A beta-binomial discrete random variable.

As an instance of the `rv_discrete` class, `betabinom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, a, b, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, a, b, loc=0)
    Probability mass function.
logpmf(k, n, a, b, loc=0)
    Log of the probability mass function.
cdf(k, n, a, b, loc=0)
    Cumulative distribution function.
logcdf(k, n, a, b, loc=0)
    Log of the cumulative distribution function.
sf(k, n, a, b, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, a, b, loc=0)
    Log of the survival function.
ppf(q, n, a, b, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, a, b, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, a, b, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, a, b, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, a, b, loc=0)
    Median of the distribution.
mean(n, a, b, loc=0)
    Mean of the distribution.
var(n, a, b, loc=0)
    Variance of the distribution.
std(n, a, b, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, a, b, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The beta-binomial distribution is a binomial distribution with a
probability of success `p` that follows a beta distribution.

The probability mass function for `betabinom` is:

.. math::

   f(k) = \binom{n}{k} \frac{B(k + a, n - k + b)}{B(a, b)}

for ``k`` in ``{0, 1,..., n}``, :math:`n \geq 0`, :math:`a > 0`,
:math:`b > 0`, where :math:`B(a, b)` is the beta function.

`betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.

References
----------
.. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``betabinom.pmf(k, n, a, b, loc)`` is identically
equivalent to ``betabinom.pmf(k - loc, n, a, b)``.

.. versionadded:: 1.4.0

See Also
--------
beta, binom

Examples
--------
>>> from scipy.stats import betabinom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, a, b = 5, 2.3, 0.63
>>> mean, var, skew, kurt = betabinom.stats(n, a, b, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(betabinom.ppf(0.01, n, a, b),
...               betabinom.ppf(0.99, n, a, b))
>>> ax.plot(x, betabinom.pmf(x, n, a, b), 'bo', ms=8, label='betabinom pmf')
>>> ax.vlines(x, 0, betabinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = betabinom(n, a, b)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = betabinom.cdf(x, n, a, b)
>>> np.allclose(x, betabinom.ppf(prob, n, a, b))
True

Generate random numbers:

>>> r = betabinom.rvs(n, a, b, size=1000)
*)

val betaprime : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Betaprime_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A beta prime continuous random variable.

As an instance of the `rv_continuous` class, `betaprime` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `betaprime` is:

.. math::

    f(x, a, b) = \frac{x^{a-1} (1+x)^{-a-b}}{\beta(a, b)}

for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where
:math:`\beta(a, b)` is the beta function (see `scipy.special.beta`).

`betaprime` takes ``a`` and ``b`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``betaprime.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``betaprime.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import betaprime
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 5, 6
>>> mean, var, skew, kurt = betaprime.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(betaprime.ppf(0.01, a, b),
...                 betaprime.ppf(0.99, a, b), 100)
>>> ax.plot(x, betaprime.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='betaprime pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = betaprime(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = betaprime.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], betaprime.cdf(vals, a, b))
True

Generate random numbers:

>>> r = betaprime.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val binned_statistic : ?statistic:[`Callable of Py.Object.t | `S of string] -> ?bins:[`Sequence_of_scalars of Py.Object.t | `I of int] -> ?range:[`Tuple of (float * float) | `T_float_float_ of Py.Object.t] -> x:[>`Ndarray] Np.Obj.t -> values:[`Ndarray of [>`Ndarray] Np.Obj.t | `List_array_like of Py.Object.t] -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * Py.Object.t * Py.Object.t)
(**
Compute a binned statistic for one or more sets of data.

This is a generalization of a histogram function.  A histogram divides
the space into bins, and returns the count of the number of points in
each bin.  This function allows the computation of the sum, mean, median,
or other statistic of the values (or set of values) within each bin.

Parameters
----------
x : (N,) array_like
    A sequence of values to be binned.
values : (N,) array_like or list of (N,) array_like
    The data on which the statistic will be computed.  This must be
    the same shape as `x`, or a set of sequences - each the same shape as
    `x`.  If `values` is a set of sequences, the statistic will be computed
    on each independently.
statistic : string or callable, optional
    The statistic to compute (default is 'mean').
    The following statistics are available:

      * 'mean' : compute the mean of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'std' : compute the standard deviation within each bin. This
        is implicitly calculated with ddof=0.
      * 'median' : compute the median of values for points within each
        bin. Empty bins will be represented by NaN.
      * 'count' : compute the count of points within each bin.  This is
        identical to an unweighted histogram.  `values` array is not
        referenced.
      * 'sum' : compute the sum of values for points within each bin.
        This is identical to a weighted histogram.
      * 'min' : compute the minimum of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'max' : compute the maximum of values for point within each bin.
        Empty bins will be represented by NaN.
      * function : a user-defined function which takes a 1D array of
        values, and outputs a single numerical statistic. This function
        will be called on the values in each bin.  Empty bins will be
        represented by function([]), or NaN if this returns an error.

bins : int or sequence of scalars, optional
    If `bins` is an int, it defines the number of equal-width bins in the
    given range (10 by default).  If `bins` is a sequence, it defines the
    bin edges, including the rightmost edge, allowing for non-uniform bin
    widths.  Values in `x` that are smaller than lowest bin edge are
    assigned to bin number 0, values beyond the highest bin are assigned to
    ``bins[-1]``.  If the bin edges are specified, the number of bins will
    be, (nx = len(bins)-1).
range : (float, float) or [(float, float)], optional
    The lower and upper range of the bins.  If not provided, range
    is simply ``(x.min(), x.max())``.  Values outside the range are
    ignored.

Returns
-------
statistic : array
    The values of the selected statistic in each bin.
bin_edges : array of dtype float
    Return the bin edges ``(length(statistic)+1)``.
binnumber: 1-D ndarray of ints
    Indices of the bins (corresponding to `bin_edges`) in which each value
    of `x` belongs.  Same length as `values`.  A binnumber of `i` means the
    corresponding value is between (bin_edges[i-1], bin_edges[i]).

See Also
--------
numpy.digitize, numpy.histogram, binned_statistic_2d, binned_statistic_dd

Notes
-----
All but the last (righthand-most) bin is half-open.  In other words, if
`bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,
but excluding 2) and the second ``[2, 3)``.  The last bin, however, is
``[3, 4]``, which *includes* 4.

.. versionadded:: 0.11.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

First some basic examples:

Create two evenly spaced bins in the range of the given sample, and sum the
corresponding values in each of those bins:

>>> values = [1.0, 1.0, 2.0, 1.5, 3.0]
>>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)
BinnedStatisticResult(statistic=array([4. , 4.5]), bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))

Multiple arrays of values can also be passed.  The statistic is calculated
on each set independently:

>>> values = [[1.0, 1.0, 2.0, 1.5, 3.0], [2.0, 2.0, 4.0, 3.0, 6.0]]
>>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)
BinnedStatisticResult(statistic=array([[4. , 4.5],
       [8. , 9. ]]), bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))

>>> stats.binned_statistic([1, 2, 1, 2, 4], np.arange(5), statistic='mean',
...                        bins=3)
BinnedStatisticResult(statistic=array([1., 2., 4.]), bin_edges=array([1., 2., 3., 4.]), binnumber=array([1, 2, 1, 2, 3]))

As a second example, we now generate some random data of sailing boat speed
as a function of wind speed, and then determine how fast our boat is for
certain wind speeds:

>>> windspeed = 8 * np.random.rand(500)
>>> boatspeed = .3 * windspeed**.5 + .2 * np.random.rand(500)
>>> bin_means, bin_edges, binnumber = stats.binned_statistic(windspeed,
...                 boatspeed, statistic='median', bins=[1,2,3,4,5,6,7])
>>> plt.figure()
>>> plt.plot(windspeed, boatspeed, 'b.', label='raw data')
>>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,
...            label='binned statistic of data')
>>> plt.legend()

Now we can use ``binnumber`` to select all datapoints with a windspeed
below 1:

>>> low_boatspeed = boatspeed[binnumber == 0]

As a final example, we will use ``bin_edges`` and ``binnumber`` to make a
plot of a distribution that shows the mean and distribution around that
mean per bin, on top of a regular histogram and the probability
distribution function:

>>> x = np.linspace(0, 5, num=500)
>>> x_pdf = stats.maxwell.pdf(x)
>>> samples = stats.maxwell.rvs(size=10000)

>>> bin_means, bin_edges, binnumber = stats.binned_statistic(x, x_pdf,
...         statistic='mean', bins=25)
>>> bin_width = (bin_edges[1] - bin_edges[0])
>>> bin_centers = bin_edges[1:] - bin_width/2

>>> plt.figure()
>>> plt.hist(samples, bins=50, density=True, histtype='stepfilled',
...          alpha=0.2, label='histogram of data')
>>> plt.plot(x, x_pdf, 'r-', label='analytical pdf')
>>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=2,
...            label='binned statistic of data')
>>> plt.plot((binnumber - 0.5) * bin_width, x_pdf, 'g.', alpha=0.5)
>>> plt.legend(fontsize=10)
>>> plt.show()
*)

val binned_statistic_2d : ?statistic:[`Callable of Py.Object.t | `S of string] -> ?bins:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int | `PyObject of Py.Object.t] -> ?range:Py.Object.t -> ?expand_binnumbers:bool -> x:[>`Ndarray] Np.Obj.t -> y:[>`Ndarray] Np.Obj.t -> values:[`Ndarray of [>`Ndarray] Np.Obj.t | `List_array_like of Py.Object.t] -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t * Py.Object.t)
(**
Compute a bidimensional binned statistic for one or more sets of data.

This is a generalization of a histogram2d function.  A histogram divides
the space into bins, and returns the count of the number of points in
each bin.  This function allows the computation of the sum, mean, median,
or other statistic of the values (or set of values) within each bin.

Parameters
----------
x : (N,) array_like
    A sequence of values to be binned along the first dimension.
y : (N,) array_like
    A sequence of values to be binned along the second dimension.
values : (N,) array_like or list of (N,) array_like
    The data on which the statistic will be computed.  This must be
    the same shape as `x`, or a list of sequences - each with the same
    shape as `x`.  If `values` is such a list, the statistic will be
    computed on each independently.
statistic : string or callable, optional
    The statistic to compute (default is 'mean').
    The following statistics are available:

      * 'mean' : compute the mean of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'std' : compute the standard deviation within each bin. This
        is implicitly calculated with ddof=0.
      * 'median' : compute the median of values for points within each
        bin. Empty bins will be represented by NaN.
      * 'count' : compute the count of points within each bin.  This is
        identical to an unweighted histogram.  `values` array is not
        referenced.
      * 'sum' : compute the sum of values for points within each bin.
        This is identical to a weighted histogram.
      * 'min' : compute the minimum of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'max' : compute the maximum of values for point within each bin.
        Empty bins will be represented by NaN.
      * function : a user-defined function which takes a 1D array of
        values, and outputs a single numerical statistic. This function
        will be called on the values in each bin.  Empty bins will be
        represented by function([]), or NaN if this returns an error.

bins : int or [int, int] or array_like or [array, array], optional
    The bin specification:

      * the number of bins for the two dimensions (nx = ny = bins),
      * the number of bins in each dimension (nx, ny = bins),
      * the bin edges for the two dimensions (x_edge = y_edge = bins),
      * the bin edges in each dimension (x_edge, y_edge = bins).

    If the bin edges are specified, the number of bins will be,
    (nx = len(x_edge)-1, ny = len(y_edge)-1).

range : (2,2) array_like, optional
    The leftmost and rightmost edges of the bins along each dimension
    (if not specified explicitly in the `bins` parameters):
    [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be
    considered outliers and not tallied in the histogram.
expand_binnumbers : bool, optional
    'False' (default): the returned `binnumber` is a shape (N,) array of
    linearized bin indices.
    'True': the returned `binnumber` is 'unraveled' into a shape (2,N)
    ndarray, where each row gives the bin numbers in the corresponding
    dimension.
    See the `binnumber` returned value, and the `Examples` section.

    .. versionadded:: 0.17.0

Returns
-------
statistic : (nx, ny) ndarray
    The values of the selected statistic in each two-dimensional bin.
x_edge : (nx + 1) ndarray
    The bin edges along the first dimension.
y_edge : (ny + 1) ndarray
    The bin edges along the second dimension.
binnumber : (N,) array of ints or (2,N) ndarray of ints
    This assigns to each element of `sample` an integer that represents the
    bin in which this observation falls.  The representation depends on the
    `expand_binnumbers` argument.  See `Notes` for details.


See Also
--------
numpy.digitize, numpy.histogram2d, binned_statistic, binned_statistic_dd

Notes
-----
Binedges:
All but the last (righthand-most) bin is half-open.  In other words, if
`bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,
but excluding 2) and the second ``[2, 3)``.  The last bin, however, is
``[3, 4]``, which *includes* 4.

`binnumber`:
This returned argument assigns to each element of `sample` an integer that
represents the bin in which it belongs.  The representation depends on the
`expand_binnumbers` argument. If 'False' (default): The returned
`binnumber` is a shape (N,) array of linearized indices mapping each
element of `sample` to its corresponding bin (using row-major ordering).
If 'True': The returned `binnumber` is a shape (2,N) ndarray where
each row indicates bin placements for each dimension respectively.  In each
dimension, a binnumber of `i` means the corresponding value is between
(D_edge[i-1], D_edge[i]), where 'D' is either 'x' or 'y'.

.. versionadded:: 0.11.0

Examples
--------
>>> from scipy import stats

Calculate the counts with explicit bin-edges:

>>> x = [0.1, 0.1, 0.1, 0.6]
>>> y = [2.1, 2.6, 2.1, 2.1]
>>> binx = [0.0, 0.5, 1.0]
>>> biny = [2.0, 2.5, 3.0]
>>> ret = stats.binned_statistic_2d(x, y, x, 'count', bins=[binx,biny])
>>> ret.statistic
array([[2., 1.],
       [1., 0.]])

The bin in which each sample is placed is given by the `binnumber`
returned parameter.  By default, these are the linearized bin indices:

>>> ret.binnumber
array([5, 6, 5, 9])

The bin indices can also be expanded into separate entries for each
dimension using the `expand_binnumbers` parameter:

>>> ret = stats.binned_statistic_2d(x, y, x, 'count', bins=[binx,biny],
...                                 expand_binnumbers=True)
>>> ret.binnumber
array([[1, 1, 1, 2],
       [1, 2, 1, 1]])

Which shows that the first three elements belong in the xbin 1, and the
fourth into xbin 2; and so on for y.
*)

val binned_statistic_dd : ?statistic:[`Callable of Py.Object.t | `S of string] -> ?bins:Py.Object.t -> ?range:Py.Object.t -> ?expand_binnumbers:bool -> ?binned_statistic_result:Py.Object.t -> sample:[>`Ndarray] Np.Obj.t -> values:[`Ndarray of [>`Ndarray] Np.Obj.t | `List_array_like of Py.Object.t] -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * Py.Object.t * Py.Object.t)
(**
Compute a multidimensional binned statistic for a set of data.

This is a generalization of a histogramdd function.  A histogram divides
the space into bins, and returns the count of the number of points in
each bin.  This function allows the computation of the sum, mean, median,
or other statistic of the values within each bin.

Parameters
----------
sample : array_like
    Data to histogram passed as a sequence of N arrays of length D, or
    as an (N,D) array.
values : (N,) array_like or list of (N,) array_like
    The data on which the statistic will be computed.  This must be
    the same shape as `sample`, or a list of sequences - each with the
    same shape as `sample`.  If `values` is such a list, the statistic
    will be computed on each independently.
statistic : string or callable, optional
    The statistic to compute (default is 'mean').
    The following statistics are available:

      * 'mean' : compute the mean of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'median' : compute the median of values for points within each
        bin. Empty bins will be represented by NaN.
      * 'count' : compute the count of points within each bin.  This is
        identical to an unweighted histogram.  `values` array is not
        referenced.
      * 'sum' : compute the sum of values for points within each bin.
        This is identical to a weighted histogram.
      * 'std' : compute the standard deviation within each bin. This
        is implicitly calculated with ddof=0.
      * 'min' : compute the minimum of values for points within each bin.
        Empty bins will be represented by NaN.
      * 'max' : compute the maximum of values for point within each bin.
        Empty bins will be represented by NaN.
      * function : a user-defined function which takes a 1D array of
        values, and outputs a single numerical statistic. This function
        will be called on the values in each bin.  Empty bins will be
        represented by function([]), or NaN if this returns an error.

bins : sequence or positive int, optional
    The bin specification must be in one of the following forms:

      * A sequence of arrays describing the bin edges along each dimension.
      * The number of bins for each dimension (nx, ny, ... = bins).
      * The number of bins for all dimensions (nx = ny = ... = bins).
range : sequence, optional
    A sequence of lower and upper bin edges to be used if the edges are
    not given explicitly in `bins`. Defaults to the minimum and maximum
    values along each dimension.
expand_binnumbers : bool, optional
    'False' (default): the returned `binnumber` is a shape (N,) array of
    linearized bin indices.
    'True': the returned `binnumber` is 'unraveled' into a shape (D,N)
    ndarray, where each row gives the bin numbers in the corresponding
    dimension.
    See the `binnumber` returned value, and the `Examples` section of
    `binned_statistic_2d`.
binned_statistic_result : binnedStatisticddResult
    Result of a previous call to the function in order to reuse bin edges
    and bin numbers with new values and/or a different statistic.
    To reuse bin numbers, `expand_binnumbers` must have been set to False
    (the default)

    .. versionadded:: 0.17.0

Returns
-------
statistic : ndarray, shape(nx1, nx2, nx3,...)
    The values of the selected statistic in each two-dimensional bin.
bin_edges : list of ndarrays
    A list of D arrays describing the (nxi + 1) bin edges for each
    dimension.
binnumber : (N,) array of ints or (D,N) ndarray of ints
    This assigns to each element of `sample` an integer that represents the
    bin in which this observation falls.  The representation depends on the
    `expand_binnumbers` argument.  See `Notes` for details.


See Also
--------
numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d

Notes
-----
Binedges:
All but the last (righthand-most) bin is half-open in each dimension.  In
other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is
``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The
last bin, however, is ``[3, 4]``, which *includes* 4.

`binnumber`:
This returned argument assigns to each element of `sample` an integer that
represents the bin in which it belongs.  The representation depends on the
`expand_binnumbers` argument. If 'False' (default): The returned
`binnumber` is a shape (N,) array of linearized indices mapping each
element of `sample` to its corresponding bin (using row-major ordering).
If 'True': The returned `binnumber` is a shape (D,N) ndarray where
each row indicates bin placements for each dimension respectively.  In each
dimension, a binnumber of `i` means the corresponding value is between
(bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.

.. versionadded:: 0.11.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> from mpl_toolkits.mplot3d import Axes3D

Take an array of 600 (x, y) coordinates as an example.
`binned_statistic_dd` can handle arrays of higher dimension `D`. But a plot
of dimension `D+1` is required.

>>> mu = np.array([0., 1.])
>>> sigma = np.array([[1., -0.5],[-0.5, 1.5]])
>>> multinormal = stats.multivariate_normal(mu, sigma)
>>> data = multinormal.rvs(size=600, random_state=235412)
>>> data.shape
(600, 2)

Create bins and count how many arrays fall in each bin:

>>> N = 60
>>> x = np.linspace(-3, 3, N)
>>> y = np.linspace(-3, 4, N)
>>> ret = stats.binned_statistic_dd(data, np.arange(600), bins=[x, y],
...                                 statistic='count')
>>> bincounts = ret.statistic

Set the volume and the location of bars:

>>> dx = x[1] - x[0]
>>> dy = y[1] - y[0]
>>> x, y = np.meshgrid(x[:-1]+dx/2, y[:-1]+dy/2)
>>> z = 0

>>> bincounts = bincounts.ravel()
>>> x = x.ravel()
>>> y = y.ravel()

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111, projection='3d')
>>> with np.errstate(divide='ignore'):   # silence random axes3d warning
...     ax.bar3d(x, y, z, dx, dy, bincounts)

Reuse bin numbers and bin edges with new values:

>>> ret2 = stats.binned_statistic_dd(data, -np.arange(600),
...                                  binned_statistic_result=ret,
...                                  statistic='mean')
*)

val binom : ?loc:float -> n:Py.Object.t -> p:Py.Object.t -> unit -> [`Binom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A binomial discrete random variable.

As an instance of the `rv_discrete` class, `binom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, p, loc=0)
    Probability mass function.
logpmf(k, n, p, loc=0)
    Log of the probability mass function.
cdf(k, n, p, loc=0)
    Cumulative distribution function.
logcdf(k, n, p, loc=0)
    Log of the cumulative distribution function.
sf(k, n, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, p, loc=0)
    Log of the survival function.
ppf(q, n, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, p, loc=0)
    Median of the distribution.
mean(n, p, loc=0)
    Mean of the distribution.
var(n, p, loc=0)
    Variance of the distribution.
std(n, p, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `binom` is:

.. math::

   f(k) = \binom{n}{k} p^k (1-p)^{n-k}

for ``k`` in ``{0, 1,..., n}``.

`binom` takes ``n`` and ``p`` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``binom.pmf(k, n, p, loc)`` is identically
equivalent to ``binom.pmf(k - loc, n, p)``.

Examples
--------
>>> from scipy.stats import binom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, p = 5, 0.4
>>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(binom.ppf(0.01, n, p),
...               binom.ppf(0.99, n, p))
>>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')
>>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = binom(n, p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = binom.cdf(x, n, p)
>>> np.allclose(x, binom.ppf(prob, n, p))
True

Generate random numbers:

>>> r = binom.rvs(n, p, size=1000)
*)

val binom_test : ?n:int -> ?p:float -> ?alternative:[`Two_sided | `Greater | `Less] -> x:[`Ndarray of [>`Ndarray] Np.Obj.t | `I of int] -> unit -> Py.Object.t
(**
Perform a test that the probability of success is p.

This is an exact, two-sided test of the null hypothesis
that the probability of success in a Bernoulli experiment
is `p`.

Parameters
----------
x : int or array_like
    The number of successes, or if x has length 2, it is the
    number of successes and the number of failures.
n : int
    The number of trials.  This is ignored if x gives both the
    number of successes and failures.
p : float, optional
    The hypothesized probability of success.  ``0 <= p <= 1``. The
    default value is ``p = 0.5``.
alternative : {'two-sided', 'greater', 'less'}, optional
    Indicates the alternative hypothesis. The default value is
    'two-sided'.

Returns
-------
p-value : float
    The p-value of the hypothesis test.

References
----------
.. [1] https://en.wikipedia.org/wiki/Binomial_test

Examples
--------
>>> from scipy import stats

A car manufacturer claims that no more than 10% of their cars are unsafe.
15 cars are inspected for safety, 3 were found to be unsafe. Test the
manufacturer's claim:

>>> stats.binom_test(3, n=15, p=0.1, alternative='greater')
0.18406106910639114

The null hypothesis cannot be rejected at the 5% level of significance
because the returned p-value is greater than the critical value of 5%.
*)

val boltzmann : ?loc:float -> lambda_:Py.Object.t -> n:Py.Object.t -> unit -> [`Boltzmann_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Boltzmann (Truncated Discrete Exponential) random variable.

As an instance of the `rv_discrete` class, `boltzmann` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lambda_, N, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, lambda_, N, loc=0)
    Probability mass function.
logpmf(k, lambda_, N, loc=0)
    Log of the probability mass function.
cdf(k, lambda_, N, loc=0)
    Cumulative distribution function.
logcdf(k, lambda_, N, loc=0)
    Log of the cumulative distribution function.
sf(k, lambda_, N, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, lambda_, N, loc=0)
    Log of the survival function.
ppf(q, lambda_, N, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lambda_, N, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(lambda_, N, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lambda_, N, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(lambda_, N), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(lambda_, N, loc=0)
    Median of the distribution.
mean(lambda_, N, loc=0)
    Mean of the distribution.
var(lambda_, N, loc=0)
    Variance of the distribution.
std(lambda_, N, loc=0)
    Standard deviation of the distribution.
interval(alpha, lambda_, N, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `boltzmann` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k) / (1-\exp(-\lambda N))

for :math:`k = 0,..., N-1`.

`boltzmann` takes :math:`\lambda > 0` and :math:`N > 0` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``boltzmann.pmf(k, lambda_, N, loc)`` is identically
equivalent to ``boltzmann.pmf(k - loc, lambda_, N)``.

Examples
--------
>>> from scipy.stats import boltzmann
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lambda_, N = 1.4, 19
>>> mean, var, skew, kurt = boltzmann.stats(lambda_, N, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(boltzmann.ppf(0.01, lambda_, N),
...               boltzmann.ppf(0.99, lambda_, N))
>>> ax.plot(x, boltzmann.pmf(x, lambda_, N), 'bo', ms=8, label='boltzmann pmf')
>>> ax.vlines(x, 0, boltzmann.pmf(x, lambda_, N), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = boltzmann(lambda_, N)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = boltzmann.cdf(x, lambda_, N)
>>> np.allclose(x, boltzmann.ppf(prob, lambda_, N))
True

Generate random numbers:

>>> r = boltzmann.rvs(lambda_, N, size=1000)
*)

val boxcox : ?lmbda:[`Bool of bool | `S of string | `I of int | `F of float] -> ?alpha:float -> x:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
Return a dataset transformed by a Box-Cox power transformation.

Parameters
----------
x : ndarray
    Input array.  Must be positive 1-dimensional.  Must not be constant.
lmbda : {None, scalar}, optional
    If `lmbda` is not None, do the transformation for that value.

    If `lmbda` is None, find the lambda that maximizes the log-likelihood
    function and return it as the second output argument.
alpha : {None, float}, optional
    If ``alpha`` is not None, return the ``100 * (1-alpha)%`` confidence
    interval for `lmbda` as the third output argument.
    Must be between 0.0 and 1.0.

Returns
-------
boxcox : ndarray
    Box-Cox power transformed array.
maxlog : float, optional
    If the `lmbda` parameter is None, the second returned argument is
    the lambda that maximizes the log-likelihood function.
(min_ci, max_ci) : tuple of float, optional
    If `lmbda` parameter is None and ``alpha`` is not None, this returned
    tuple of floats represents the minimum and maximum confidence limits
    given ``alpha``.

See Also
--------
probplot, boxcox_normplot, boxcox_normmax, boxcox_llf

Notes
-----
The Box-Cox transform is given by::

    y = (x**lmbda - 1) / lmbda,  for lmbda > 0
        log(x),                  for lmbda = 0

`boxcox` requires the input data to be positive.  Sometimes a Box-Cox
transformation provides a shift parameter to achieve this; `boxcox` does
not.  Such a shift parameter is equivalent to adding a positive constant to
`x` before calling `boxcox`.

The confidence limits returned when ``alpha`` is provided give the interval
where:

.. math::

    llf(\hat{\lambda}) - llf(\lambda) < \frac{1}{2}\chi^2(1 - \alpha, 1),

with ``llf`` the log-likelihood function and :math:`\chi^2` the chi-squared
function.

References
----------
G.E.P. Box and D.R. Cox, 'An Analysis of Transformations', Journal of the
Royal Statistical Society B, 26, 211-252 (1964).

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

We generate some random variates from a non-normal distribution and make a
probability plot for it, to show it is non-normal in the tails:

>>> fig = plt.figure()
>>> ax1 = fig.add_subplot(211)
>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)
>>> ax1.set_xlabel('')
>>> ax1.set_title('Probplot against normal distribution')

We now use `boxcox` to transform the data so it's closest to normal:

>>> ax2 = fig.add_subplot(212)
>>> xt, _ = stats.boxcox(x)
>>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)
>>> ax2.set_title('Probplot after Box-Cox transformation')

>>> plt.show()
*)

val boxcox_llf : lmb:[`F of float | `I of int | `Bool of bool | `S of string] -> data:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
The boxcox log-likelihood function.

Parameters
----------
lmb : scalar
    Parameter for Box-Cox transformation.  See `boxcox` for details.
data : array_like
    Data to calculate Box-Cox log-likelihood for.  If `data` is
    multi-dimensional, the log-likelihood is calculated along the first
    axis.

Returns
-------
llf : float or ndarray
    Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,
    an array otherwise.

See Also
--------
boxcox, probplot, boxcox_normplot, boxcox_normmax

Notes
-----
The Box-Cox log-likelihood function is defined here as

.. math::

    llf = (\lambda - 1) \sum_i(\log(x_i)) -
          N/2 \log(\sum_i (y_i - \bar{y})^2 / N),

where ``y`` is the Box-Cox transformed input data ``x``.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes
>>> np.random.seed(1245)

Generate some random variates and calculate Box-Cox log-likelihood values
for them for a range of ``lmbda`` values:

>>> x = stats.loggamma.rvs(5, loc=10, size=1000)
>>> lmbdas = np.linspace(-2, 10)
>>> llf = np.zeros(lmbdas.shape, dtype=float)
>>> for ii, lmbda in enumerate(lmbdas):
...     llf[ii] = stats.boxcox_llf(lmbda, x)

Also find the optimal lmbda value with `boxcox`:

>>> x_most_normal, lmbda_optimal = stats.boxcox(x)

Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a
horizontal line to check that that's really the optimum:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(lmbdas, llf, 'b.-')
>>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')
>>> ax.set_xlabel('lmbda parameter')
>>> ax.set_ylabel('Box-Cox log-likelihood')

Now add some probability plots to show that where the log-likelihood is
maximized the data transformed with `boxcox` looks closest to normal:

>>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'
>>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):
...     xt = stats.boxcox(x, lmbda=lmbda)
...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)
...     ax_inset = inset_axes(ax, width='20%', height='20%', loc=loc)
...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')
...     ax_inset.set_xticklabels([])
...     ax_inset.set_yticklabels([])
...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda)

>>> plt.show()
*)

val boxcox_normmax : ?brack:Py.Object.t -> ?method_:string -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute optimal Box-Cox transform parameter for input data.

Parameters
----------
x : array_like
    Input array.
brack : 2-tuple, optional
    The starting interval for a downhill bracket search with
    `optimize.brent`.  Note that this is in most cases not critical; the
    final result is allowed to be outside this bracket.
method : str, optional
    The method to determine the optimal transform parameter (`boxcox`
    ``lmbda`` parameter). Options are:

    'pearsonr'  (default)
        Maximizes the Pearson correlation coefficient between
        ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be
        normally-distributed.

    'mle'
        Minimizes the log-likelihood `boxcox_llf`.  This is the method used
        in `boxcox`.

    'all'
        Use all optimization methods available, and return all results.
        Useful to compare different methods.

Returns
-------
maxlog : float or ndarray
    The optimal transform parameter found.  An array instead of a scalar
    for ``method='all'``.

See Also
--------
boxcox, boxcox_llf, boxcox_normplot

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234)  # make this example reproducible

Generate some data and determine optimal ``lmbda`` in various ways:

>>> x = stats.loggamma.rvs(5, size=30) + 5
>>> y, lmax_mle = stats.boxcox(x)
>>> lmax_pearsonr = stats.boxcox_normmax(x)

>>> lmax_mle
7.177...
>>> lmax_pearsonr
7.916...
>>> stats.boxcox_normmax(x, method='all')
array([ 7.91667384,  7.17718692])

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)
>>> ax.axvline(lmax_mle, color='r')
>>> ax.axvline(lmax_pearsonr, color='g', ls='--')

>>> plt.show()
*)

val boxcox_normplot : ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> la:Py.Object.t -> lb:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Compute parameters for a Box-Cox normality plot, optionally show it.

A Box-Cox normality plot shows graphically what the best transformation
parameter is to use in `boxcox` to obtain a distribution that is close
to normal.

Parameters
----------
x : array_like
    Input array.
la, lb : scalar
    The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`
    for Box-Cox transformations.  These are also the limits of the
    horizontal axis of the plot if that is generated.
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `la` to `lb`).

Returns
-------
lmbdas : ndarray
    The ``lmbda`` values for which a Box-Cox transform was done.
ppcc : ndarray
    Probability Plot Correlelation Coefficient, as obtained from `probplot`
    when fitting the Box-Cox transformed input `x` against a normal
    distribution.

See Also
--------
probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max

Notes
-----
Even if `plot` is given, the figure is not shown or saved by
`boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``
should be used after calling `probplot`.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

Generate some non-normally distributed data, and create a Box-Cox plot:

>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)

Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in
the same plot:

>>> _, maxlog = stats.boxcox(x)
>>> ax.axvline(maxlog, color='r')

>>> plt.show()
*)

val bradford : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Bradford_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Bradford continuous random variable.

As an instance of the `rv_continuous` class, `bradford` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `bradford` is:

.. math::

    f(x, c) = \frac{c}{\log(1+c) (1+cx)}

for :math:`0 <= x <= 1` and :math:`c > 0`.

`bradford` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``bradford.pdf(x, c, loc, scale)`` is identically
equivalent to ``bradford.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import bradford
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.299
>>> mean, var, skew, kurt = bradford.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(bradford.ppf(0.01, c),
...                 bradford.ppf(0.99, c), 100)
>>> ax.plot(x, bradford.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='bradford pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = bradford(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = bradford.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], bradford.cdf(vals, c))
True

Generate random numbers:

>>> r = bradford.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val brunnermunzel : ?alternative:[`Two_sided | `Less | `Greater] -> ?distribution:[`T | `Normal] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Brunner-Munzel test on samples x and y.

The Brunner-Munzel test is a nonparametric test of the null hypothesis that
when values are taken one by one from each group, the probabilities of
getting large values in both groups are equal.
Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the
assumption of equivariance of two groups. Note that this does not assume
the distributions are same. This test works on two independent samples,
which may have different sizes.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided
distribution : {'t', 'normal'}, optional
    Defines how to get the p-value.
    The following options are available (default is 't'):

      * 't': get the p-value by t-distribution
      * 'normal': get the p-value by standard normal distribution.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The Brunner-Munzer W statistic.
pvalue : float
    p-value assuming an t distribution. One-sided or
    two-sided, depending on the choice of `alternative` and `distribution`.

See Also
--------
mannwhitneyu : Mann-Whitney rank test on two samples.

Notes
-----
Brunner and Munzel recommended to estimate the p-value by t-distribution
when the size of data is 50 or less. If the size is lower than 10, it would
be better to use permuted Brunner Munzel test (see [2]_).

References
----------
.. [1] Brunner, E. and Munzel, U. 'The nonparametric Benhrens-Fisher
       problem: Asymptotic theory and a small-sample approximation'.
       Biometrical Journal. Vol. 42(2000): 17-25.
.. [2] Neubert, K. and Brunner, E. 'A studentized permutation test for the
       non-parametric Behrens-Fisher problem'. Computational Statistics and
       Data Analysis. Vol. 51(2007): 5192-5204.

Examples
--------
>>> from scipy import stats
>>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]
>>> x2 = [3,3,4,3,1,2,3,1,1,5,4]
>>> w, p_value = stats.brunnermunzel(x1, x2)
>>> w
3.1374674823029505
>>> p_value
0.0057862086661515377
*)

val burr : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Burr_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Burr (Type III) continuous random variable.

As an instance of the `rv_continuous` class, `burr` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr12 : Burr Type XII distribution
mielke : Mielke Beta-Kappa / Dagum distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{-c - 1} / (1 + x^{-c})^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr` takes :math:`c` and :math:`d` as shape parameters.

This is the PDF corresponding to the third CDF given in Burr's list;
specifically, it is equation (11) in Burr's paper [1]_. The distribution
is also commonly referred to as the Dagum distribution [2]_. If the
parameter :math:`c < 1` then the mean of the distribution does not
exist and if :math:`c < 2` the variance does not exist [2]_.
The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``burr.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``burr.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).
.. [2] https://en.wikipedia.org/wiki/Dagum_distribution
.. [3] Kleiber, Christian. 'A guide to the Dagum distributions.'
   Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).

Examples
--------
>>> from scipy.stats import burr
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 10.5, 4.3
>>> mean, var, skew, kurt = burr.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(burr.ppf(0.01, c, d),
...                 burr.ppf(0.99, c, d), 100)
>>> ax.plot(x, burr.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='burr pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = burr(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = burr.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], burr.cdf(vals, c, d))
True

Generate random numbers:

>>> r = burr.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val burr12 : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Burr12_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Burr (Type XII) continuous random variable.

As an instance of the `rv_continuous` class, `burr12` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
fisk : a special case of either `burr` or `burr12` with ``d=1``
burr : Burr Type III distribution

Notes
-----
The probability density function for `burr` is:

.. math::

    f(x, c, d) = c d x^{c-1} / (1 + x^c)^{d + 1}

for :math:`x >= 0` and :math:`c, d > 0`.

`burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`
and :math:`d`.

This is the PDF corresponding to the twelfth CDF given in Burr's list;
specifically, it is equation (20) in Burr's paper [1]_.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``burr12.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``burr12.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

The Burr type 12 distribution is also sometimes referred to as
the Singh-Maddala distribution from NIST [2]_.

References
----------
.. [1] Burr, I. W. 'Cumulative frequency functions', Annals of
   Mathematical Statistics, 13(2), pp 215-232 (1942).

.. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm

.. [3] 'Burr distribution',
   https://en.wikipedia.org/wiki/Burr_distribution

Examples
--------
>>> from scipy.stats import burr12
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 10, 4
>>> mean, var, skew, kurt = burr12.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(burr12.ppf(0.01, c, d),
...                 burr12.ppf(0.99, c, d), 100)
>>> ax.plot(x, burr12.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='burr12 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = burr12(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = burr12.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], burr12.cdf(vals, c, d))
True

Generate random numbers:

>>> r = burr12.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val cauchy : ?loc:float -> ?scale:float -> unit -> [`Cauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `cauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `cauchy` is

.. math::

    f(x) = \frac{1}{\pi (1 + x^2)}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``cauchy.pdf(x, loc, scale)`` is identically
equivalent to ``cauchy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import cauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = cauchy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(cauchy.ppf(0.01),
...                 cauchy.ppf(0.99), 100)
>>> ax.plot(x, cauchy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='cauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = cauchy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = cauchy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], cauchy.cdf(vals))
True

Generate random numbers:

>>> r = cauchy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val chi : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Chi_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A chi continuous random variable.

As an instance of the `rv_continuous` class, `chi` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `chi` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2-1} \Gamma \left( k/2 \right)}
               x^{k-1} \exp \left( -x^2/2 \right)

for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation). :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

Special cases of `chi` are:

    - ``chi(1, loc, scale)`` is equivalent to `halfnorm`
    - ``chi(2, 0, scale)`` is equivalent to `rayleigh`
    - ``chi(3, 0, scale)`` is equivalent to `maxwell`

`chi` takes ``df`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``chi.pdf(x, df, loc, scale)`` is identically
equivalent to ``chi.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import chi
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 78
>>> mean, var, skew, kurt = chi.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(chi.ppf(0.01, df),
...                 chi.ppf(0.99, df), 100)
>>> ax.plot(x, chi.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='chi pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = chi(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = chi.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], chi.cdf(vals, df))
True

Generate random numbers:

>>> r = chi.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val chi2 : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Chi2_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A chi-squared continuous random variable.

As an instance of the `rv_continuous` class, `chi2` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `chi2` is:

.. math::

    f(x, k) = \frac{1}{2^{k/2} \Gamma \left( k/2 \right)}
               x^{k/2-1} \exp \left( -x/2 \right)

for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``
in the implementation).

`chi2` takes ``df`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``chi2.pdf(x, df, loc, scale)`` is identically
equivalent to ``chi2.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import chi2
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 55
>>> mean, var, skew, kurt = chi2.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(chi2.ppf(0.01, df),
...                 chi2.ppf(0.99, df), 100)
>>> ax.plot(x, chi2.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='chi2 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = chi2(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = chi2.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df))
True

Generate random numbers:

>>> r = chi2.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val chi2_contingency : ?correction:bool -> ?lambda_:[`S of string | `F of float] -> observed:[>`Ndarray] Np.Obj.t -> unit -> (float * float * int * Py.Object.t)
(**
Chi-square test of independence of variables in a contingency table.

This function computes the chi-square statistic and p-value for the
hypothesis test of independence of the observed frequencies in the
contingency table [1]_ `observed`.  The expected frequencies are computed
based on the marginal sums under the assumption of independence; see
`scipy.stats.contingency.expected_freq`.  The number of degrees of
freedom is (expressed using numpy functions and attributes)::

    dof = observed.size - sum(observed.shape) + observed.ndim - 1


Parameters
----------
observed : array_like
    The contingency table. The table contains the observed frequencies
    (i.e. number of occurrences) in each category.  In the two-dimensional
    case, the table is often described as an 'R x C table'.
correction : bool, optional
    If True, *and* the degrees of freedom is 1, apply Yates' correction
    for continuity.  The effect of the correction is to adjust each
    observed value by 0.5 towards the corresponding expected value.
lambda_ : float or str, optional.
    By default, the statistic computed in this test is Pearson's
    chi-squared statistic [2]_.  `lambda_` allows a statistic from the
    Cressie-Read power divergence family [3]_ to be used instead.  See
    `power_divergence` for details.

Returns
-------
chi2 : float
    The test statistic.
p : float
    The p-value of the test
dof : int
    Degrees of freedom
expected : ndarray, same shape as `observed`
    The expected frequencies, based on the marginal sums of the table.

See Also
--------
contingency.expected_freq
fisher_exact
chisquare
power_divergence

Notes
-----
An often quoted guideline for the validity of this calculation is that
the test should be used only if the observed and expected frequencies
in each cell are at least 5.

This is a test for the independence of different categories of a
population. The test is only meaningful when the dimension of
`observed` is two or more.  Applying the test to a one-dimensional
table will always result in `expected` equal to `observed` and a
chi-square statistic equal to 0.

This function does not handle masked arrays, because the calculation
does not make sense with missing values.

Like stats.chisquare, this function computes a chi-square statistic;
the convenience this function provides is to figure out the expected
frequencies and degrees of freedom from the given contingency table.
If these were already known, and if the Yates' correction was not
required, one could use stats.chisquare.  That is, if one calls::

    chi2, p, dof, ex = chi2_contingency(obs, correction=False)

then the following is true::

    (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),
                                 ddof=obs.size - 1 - dof)

The `lambda_` argument was added in version 0.13.0 of scipy.

References
----------
.. [1] 'Contingency table',
       https://en.wikipedia.org/wiki/Contingency_table
.. [2] 'Pearson's chi-squared test',
       https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test
.. [3] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
A two-way example (2 x 3):

>>> from scipy.stats import chi2_contingency
>>> obs = np.array([[10, 10, 20], [20, 20, 20]])
>>> chi2_contingency(obs)
(2.7777777777777777,
 0.24935220877729619,
 2,
 array([[ 12.,  12.,  16.],
        [ 18.,  18.,  24.]]))

Perform the test using the log-likelihood ratio (i.e. the 'G-test')
instead of Pearson's chi-squared statistic.

>>> g, p, dof, expctd = chi2_contingency(obs, lambda_='log-likelihood')
>>> g, p
(2.7688587616781319, 0.25046668010954165)

A four-way example (2 x 2 x 2 x 2):

>>> obs = np.array(
...     [[[[12, 17],
...        [11, 16]],
...       [[11, 12],
...        [15, 16]]],
...      [[[23, 15],
...        [30, 22]],
...       [[14, 17],
...        [15, 16]]]])
>>> chi2_contingency(obs)
(8.7584514426741897,
 0.64417725029295503,
 11,
 array([[[[ 14.15462386,  14.15462386],
          [ 16.49423111,  16.49423111]],
         [[ 11.2461395 ,  11.2461395 ],
          [ 13.10500554,  13.10500554]]],
        [[[ 19.5591166 ,  19.5591166 ],
          [ 22.79202844,  22.79202844]],
         [[ 15.54012004,  15.54012004],
          [ 18.10873492,  18.10873492]]]]))
*)

val chisquare : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate a one-way chi-square test.

The chi-square test tests the null hypothesis that the categorical data
has the given frequencies.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.

Returns
-------
chisq : float or ndarray
    The chi-squared test statistic.  The value is a float if `axis` is
    None or `f_obs` and `f_exp` are 1-D.
p : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `chisq` are scalars.

See Also
--------
scipy.stats.power_divergence

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not chi-square, in which case this test
is not appropriate.

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test

Examples
--------
When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.

>>> from scipy.stats import chisquare
>>> chisquare([16, 18, 16, 14, 12, 12])
(2.0, 0.84914503608460956)

With `f_exp` the expected frequencies can be given.

>>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])
(3.5, 0.62338762774958223)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> chisquare(obs)
(array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> chisquare(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> chisquare(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
chi-squared statistic with `ddof`.

>>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we use ``axis=1``:

>>> chisquare([16, 18, 16, 14, 12, 12],
...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],
...           axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val circmean : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular mean for samples in a range.

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular mean range.  Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular mean range.  Default is 0.
axis : int, optional
    Axis along which means are computed.  The default is to compute
    the mean of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circmean : float
    Circular mean.

Examples
--------
>>> from scipy.stats import circmean
>>> circmean([0.1, 2*np.pi+0.2, 6*np.pi+0.3])
0.2

>>> from scipy.stats import circmean
>>> circmean([0.2, 1.4, 2.6], high = 1, low = 0)
0.4
*)

val circstd : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular standard deviation for samples assumed to be in the
range [low to high].

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular standard deviation range.
    Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular standard deviation range.  Default is 0.
axis : int, optional
    Axis along which standard deviations are computed.  The default is
    to compute the standard deviation of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circstd : float
    Circular standard deviation.

Notes
-----
This uses a definition of circular standard deviation that in the limit of
small angles returns a number close to the 'linear' standard deviation.

Examples
--------
>>> from scipy.stats import circstd
>>> circstd([0, 0.1*np.pi/2, 0.001*np.pi, 0.03*np.pi/2])
0.063564063306
*)

val circvar : ?high:[`I of int | `F of float] -> ?low:[`I of int | `F of float] -> ?axis:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> samples:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the circular variance for samples assumed to be in a range.

Parameters
----------
samples : array_like
    Input array.
high : float or int, optional
    High boundary for circular variance range.  Default is ``2*pi``.
low : float or int, optional
    Low boundary for circular variance range.  Default is 0.
axis : int, optional
    Axis along which variances are computed.  The default is to compute
    the variance of the flattened array.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
circvar : float
    Circular variance.

Notes
-----
This uses a definition of circular variance that in the limit of small
angles returns a number close to the 'linear' variance.

Examples
--------
>>> from scipy.stats import circvar
>>> circvar([0, 2*np.pi/3, 5*np.pi/3])
2.19722457734
*)

val combine_pvalues : ?method_:[`Fisher | `Pearson | `Tippett | `Stouffer | `Mudholkar_george] -> ?weights:[`Ndarray of [>`Ndarray] Np.Obj.t | `T1_D of Py.Object.t] -> pvalues:[`Ndarray of [>`Ndarray] Np.Obj.t | `T1_D of Py.Object.t] -> unit -> (float * float)
(**
Combine p-values from independent tests bearing upon the same hypothesis.

Parameters
----------
pvalues : array_like, 1-D
    Array of p-values assumed to come from independent tests.
method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}, optional
    Name of method to use to combine p-values.
    The following methods are available (default is 'fisher'):

      * 'fisher': Fisher's method (Fisher's combined probability test), the
        sum of the logarithm of the p-values
      * 'pearson': Pearson's method (similar to Fisher's but uses sum of the
        complement of the p-values inside the logarithms)
      * 'tippett': Tippett's method (minimum of p-values)
      * 'stouffer': Stouffer's Z-score method
      * 'mudholkar_george': the difference of Fisher's and Pearson's methods
        divided by 2
weights : array_like, 1-D, optional
    Optional array of weights used only for Stouffer's Z-score method.

Returns
-------
statistic: float
    The statistic calculated by the specified method.
pval: float
    The combined p-value.

Notes
-----
Fisher's method (also known as Fisher's combined probability test) [1]_ uses
a chi-squared statistic to compute a combined p-value. The closely related
Stouffer's Z-score method [2]_ uses Z-scores rather than p-values. The
advantage of Stouffer's method is that it is straightforward to introduce
weights, which can make Stouffer's method more powerful than Fisher's
method when the p-values are from studies of different size [6]_ [7]_.
The Pearson's method uses :math:`log(1-p_i)` inside the sum whereas Fisher's
method uses :math:`log(p_i)` [4]_. For Fisher's and Pearson's method, the
sum of the logarithms is multiplied by -2 in the implementation. This
quantity has a chi-square distribution that determines the p-value. The
`mudholkar_george` method is the difference of the Fisher's and Pearson's
test statistics, each of which include the -2 factor [4]_. However, the
`mudholkar_george` method does not include these -2 factors. The test
statistic of `mudholkar_george` is the sum of logisitic random variables and
equation 3.6 in [3]_ is used to approximate the p-value based on Student's
t-distribution.

Fisher's method may be extended to combine p-values from dependent tests
[5]_. Extensions such as Brown's method and Kost's method are not currently
implemented.

.. versionadded:: 0.15.0

References
----------
.. [1] https://en.wikipedia.org/wiki/Fisher%27s_method
.. [2] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method
.. [3] George, E. O., and G. S. Mudholkar. 'On the convolution of logistic
       random variables.' Metrika 30.1 (1983): 1-13.
.. [4] Heard, N. and Rubin-Delanchey, P. 'Choosing between methods of
       combining p-values.'  Biometrika 105.1 (2018): 239-246.
.. [5] Whitlock, M. C. 'Combining probability from independent tests: the
       weighted Z-method is superior to Fisher's approach.' Journal of
       Evolutionary Biology 18, no. 5 (2005): 1368-1373.
.. [6] Zaykin, Dmitri V. 'Optimally weighted Z-test is a powerful method
       for combining probabilities in meta-analysis.' Journal of
       Evolutionary Biology 24, no. 8 (2011): 1836-1841.
.. [7] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method
*)

val cosine : ?loc:float -> ?scale:float -> unit -> [`Cosine_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A cosine continuous random variable.

As an instance of the `rv_continuous` class, `cosine` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The cosine distribution is an approximation to the normal distribution.
The probability density function for `cosine` is:

.. math::

    f(x) = \frac{1}{2\pi} (1+\cos(x))

for :math:`-\pi \le x \le \pi`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``cosine.pdf(x, loc, scale)`` is identically
equivalent to ``cosine.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import cosine
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = cosine.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(cosine.ppf(0.01),
...                 cosine.ppf(0.99), 100)
>>> ax.plot(x, cosine.pdf(x),
...        'r-', lw=5, alpha=0.6, label='cosine pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = cosine()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = cosine.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], cosine.cdf(vals))
True

Generate random numbers:

>>> r = cosine.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val crystalball : ?loc:float -> ?scale:float -> beta:Py.Object.t -> m:Py.Object.t -> unit -> [`Crystalball_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Crystalball distribution

As an instance of the `rv_continuous` class, `crystalball` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, m, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, m, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, m, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, m, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, m, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, m, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, m, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, m, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, m, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, m, loc=0, scale=1)
    Non-central moment of order n
stats(beta, m, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, m, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, m, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta, m), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, m, loc=0, scale=1)
    Median of the distribution.
mean(beta, m, loc=0, scale=1)
    Mean of the distribution.
var(beta, m, loc=0, scale=1)
    Variance of the distribution.
std(beta, m, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, m, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `crystalball` is:

.. math::

    f(x, \beta, m) =  \begin{cases}
                        N \exp(-x^2 / 2),  &\text{for } x > -\beta\\
                        N A (B - x)^{-m}  &\text{for } x \le -\beta
                      \end{cases}

where :math:`A = (m / |\beta|)^n  \exp(-\beta^2 / 2)`,
:math:`B = m/|\beta| - |\beta|` and :math:`N` is a normalisation constant.

`crystalball` takes :math:`\beta > 0` and :math:`m > 1` as shape
parameters.  :math:`\beta` defines the point where the pdf changes
from a power-law to a Gaussian distribution.  :math:`m` is the power
of the power-law tail.

References
----------
.. [1] 'Crystal Ball Function',
       https://en.wikipedia.org/wiki/Crystal_Ball_function

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``crystalball.pdf(x, beta, m, loc, scale)`` is identically
equivalent to ``crystalball.pdf(y, beta, m) / scale`` with
``y = (x - loc) / scale``.

.. versionadded:: 0.19.0

Examples
--------
>>> from scipy.stats import crystalball
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta, m = 2, 3
>>> mean, var, skew, kurt = crystalball.stats(beta, m, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(crystalball.ppf(0.01, beta, m),
...                 crystalball.ppf(0.99, beta, m), 100)
>>> ax.plot(x, crystalball.pdf(x, beta, m),
...        'r-', lw=5, alpha=0.6, label='crystalball pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = crystalball(beta, m)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = crystalball.ppf([0.001, 0.5, 0.999], beta, m)
>>> np.allclose([0.001, 0.5, 0.999], crystalball.cdf(vals, beta, m))
True

Generate random numbers:

>>> r = crystalball.rvs(beta, m, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val cumfreq : ?numbins:int -> ?defaultreallimits:Py.Object.t -> ?weights:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float * int)
(**
Return a cumulative frequency histogram, using the histogram function.

A cumulative histogram is a mapping that counts the cumulative number of
observations in all of the bins up to the specified bin.

Parameters
----------
a : array_like
    Input array.
numbins : int, optional
    The number of bins to use for the histogram. Default is 10.
defaultreallimits : tuple (lower, upper), optional
    The lower and upper values for the range of the histogram.
    If no value is given, a range slightly larger than the range of the
    values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,
    where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.
weights : array_like, optional
    The weights for each value in `a`. Default is None, which gives each
    value a weight of 1.0

Returns
-------
cumcount : ndarray
    Binned values of cumulative frequency.
lowerlimit : float
    Lower real limit
binsize : float
    Width of each bin.
extrapoints : int
    Extra points.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats
>>> x = [1, 4, 2, 1, 3, 1]
>>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
>>> res.cumcount
array([ 1.,  2.,  3.,  3.])
>>> res.extrapoints
3

Create a normal distribution with 1000 random values

>>> rng = np.random.RandomState(seed=12345)
>>> samples = stats.norm.rvs(size=1000, random_state=rng)

Calculate cumulative frequencies

>>> res = stats.cumfreq(samples, numbins=25)

Calculate space of values for x

>>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,
...                                  res.cumcount.size)

Plot histogram and cumulative histogram

>>> fig = plt.figure(figsize=(10, 4))
>>> ax1 = fig.add_subplot(1, 2, 1)
>>> ax2 = fig.add_subplot(1, 2, 2)
>>> ax1.hist(samples, bins=25)
>>> ax1.set_title('Histogram')
>>> ax2.bar(x, res.cumcount, width=res.binsize)
>>> ax2.set_title('Cumulative histogram')
>>> ax2.set_xlim([x.min(), x.max()])

>>> plt.show()
*)

val describe : ?axis:[`I of int | `None] -> ?ddof:int -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t * Py.Object.t)
(**
Compute several descriptive statistics of the passed array.

Parameters
----------
a : array_like
   Input data.
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.
ddof : int, optional
    Delta degrees of freedom (only for variance).  Default is 1.
bias : bool, optional
    If False, then the skewness and kurtosis calculations are corrected for
    statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
nobs : int or ndarray of ints
   Number of observations (length of data along `axis`).
   When 'omit' is chosen as nan_policy, each column is counted separately.
minmax: tuple of ndarrays or floats
   Minimum and maximum value of data array.
mean : ndarray or float
   Arithmetic mean of data along axis.
variance : ndarray or float
   Unbiased variance of the data along axis, denominator is number of
   observations minus one.
skewness : ndarray or float
   Skewness, based on moment calculations with denominator equal to
   the number of observations, i.e. no degrees of freedom correction.
kurtosis : ndarray or float
   Kurtosis (Fisher).  The kurtosis is normalized so that it is
   zero for the normal distribution.  No degrees of freedom are used.

See Also
--------
skew, kurtosis

Examples
--------
>>> from scipy import stats
>>> a = np.arange(10)
>>> stats.describe(a)
DescribeResult(nobs=10, minmax=(0, 9), mean=4.5, variance=9.166666666666666,
               skewness=0.0, kurtosis=-1.2242424242424244)
>>> b = [[1, 2], [3, 4]]
>>> stats.describe(b)
DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),
               mean=array([2., 3.]), variance=array([2., 2.]),
               skewness=array([0., 0.]), kurtosis=array([-2., -2.]))
*)

val dgamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Dgamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A double gamma continuous random variable.

As an instance of the `rv_continuous` class, `dgamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `dgamma` is:

.. math::

    f(x, a) = \frac{1}{2\Gamma(a)} |x|^{a-1} \exp(-|x|)

for a real number :math:`x` and :math:`a > 0`. :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`dgamma` takes ``a`` as a shape parameter for :math:`a`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``dgamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``dgamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import dgamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.1
>>> mean, var, skew, kurt = dgamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(dgamma.ppf(0.01, a),
...                 dgamma.ppf(0.99, a), 100)
>>> ax.plot(x, dgamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='dgamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = dgamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = dgamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], dgamma.cdf(vals, a))
True

Generate random numbers:

>>> r = dgamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val dirichlet : ?seed:Py.Object.t -> alpha:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
A Dirichlet random variable.

The `alpha` keyword specifies the concentration parameters of the
distribution.

.. versionadded:: 0.15.0

Methods
-------
``pdf(x, alpha)``
    Probability density function.
``logpdf(x, alpha)``
    Log of the probability density function.
``rvs(alpha, size=1, random_state=None)``
    Draw random samples from a Dirichlet distribution.
``mean(alpha)``
    The mean of the Dirichlet distribution
``var(alpha)``
    The variance of the Dirichlet distribution
``entropy(alpha)``
    Compute the differential entropy of the Dirichlet distribution.

Parameters
----------
x : array_like
    Quantiles, with the last axis of `x` denoting the components.
alpha : array_like
    The concentration parameters. The number of entries determines the
    dimensionality of the distribution.
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Alternatively, the object may be called (as a function) to fix
concentration parameters, returning a 'frozen' Dirichlet
random variable:

rv = dirichlet(alpha)
    - Frozen object with the same methods but holding the given
      concentration parameters fixed.

Notes
-----
Each :math:`\alpha` entry must be positive. The distribution has only
support on the simplex defined by

.. math::
    \sum_{i=1}^{K} x_i \le 1


The probability density function for `dirichlet` is

.. math::

    f(x) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1}

where

.. math::

    \mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}
                                 {\Gamma\bigl(\sum_{i=1}^K \alpha_i\bigr)}

and :math:`\boldsymbol\alpha=(\alpha_1,\ldots,\alpha_K)`, the
concentration parameters and :math:`K` is the dimension of the space
where :math:`x` takes values.

Note that the dirichlet interface is somewhat inconsistent.
The array returned by the rvs function is transposed
with respect to the format expected by the pdf and logpdf.

Examples
--------
>>> from scipy.stats import dirichlet

Generate a dirichlet random variable

>>> quantiles = np.array([0.2, 0.2, 0.6])  # specify quantiles
>>> alpha = np.array([0.4, 5, 15])  # specify concentration parameters
>>> dirichlet.pdf(quantiles, alpha)
0.2843831684937255

The same PDF but following a log scale

>>> dirichlet.logpdf(quantiles, alpha)
-1.2574327653159187

Once we specify the dirichlet distribution
we can then calculate quantities of interest

>>> dirichlet.mean(alpha)  # get the mean of the distribution
array([0.01960784, 0.24509804, 0.73529412])
>>> dirichlet.var(alpha) # get variance
array([0.00089829, 0.00864603, 0.00909517])
>>> dirichlet.entropy(alpha)  # calculate the differential entropy
-4.3280162474082715

We can also return random samples from the distribution

>>> dirichlet.rvs(alpha, size=1, random_state=1)
array([[0.00766178, 0.24670518, 0.74563305]])
>>> dirichlet.rvs(alpha, size=2, random_state=2)
array([[0.01639427, 0.1292273 , 0.85437844],
       [0.00156917, 0.19033695, 0.80809388]])
*)

val dlaplace : ?loc:float -> a:Py.Object.t -> unit -> [`Dlaplace_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A  Laplacian discrete random variable.

As an instance of the `rv_discrete` class, `dlaplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, a, loc=0)
    Probability mass function.
logpmf(k, a, loc=0)
    Log of the probability mass function.
cdf(k, a, loc=0)
    Cumulative distribution function.
logcdf(k, a, loc=0)
    Log of the cumulative distribution function.
sf(k, a, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, a, loc=0)
    Log of the survival function.
ppf(q, a, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(a, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0)
    Median of the distribution.
mean(a, loc=0)
    Mean of the distribution.
var(a, loc=0)
    Variance of the distribution.
std(a, loc=0)
    Standard deviation of the distribution.
interval(alpha, a, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `dlaplace` is:

.. math::

    f(k) = \tanh(a/2) \exp(-a |k|)

for integers :math:`k` and :math:`a > 0`.

`dlaplace` takes :math:`a` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``dlaplace.pmf(k, a, loc)`` is identically
equivalent to ``dlaplace.pmf(k - loc, a)``.

Examples
--------
>>> from scipy.stats import dlaplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 0.8
>>> mean, var, skew, kurt = dlaplace.stats(a, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(dlaplace.ppf(0.01, a),
...               dlaplace.ppf(0.99, a))
>>> ax.plot(x, dlaplace.pmf(x, a), 'bo', ms=8, label='dlaplace pmf')
>>> ax.vlines(x, 0, dlaplace.pmf(x, a), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = dlaplace(a)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = dlaplace.cdf(x, a)
>>> np.allclose(x, dlaplace.ppf(prob, a))
True

Generate random numbers:

>>> r = dlaplace.rvs(a, size=1000)
*)

val dweibull : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Dweibull_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A double Weibull continuous random variable.

As an instance of the `rv_continuous` class, `dweibull` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `dweibull` is given by

.. math::

    f(x, c) = c / 2 |x|^{c-1} \exp(-|x|^c)

for a real number :math:`x` and :math:`c > 0`.

`dweibull` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``dweibull.pdf(x, c, loc, scale)`` is identically
equivalent to ``dweibull.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import dweibull
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 2.07
>>> mean, var, skew, kurt = dweibull.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(dweibull.ppf(0.01, c),
...                 dweibull.ppf(0.99, c), 100)
>>> ax.plot(x, dweibull.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='dweibull pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = dweibull(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = dweibull.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], dweibull.cdf(vals, c))
True

Generate random numbers:

>>> r = dweibull.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val energy_distance : ?u_weights:Py.Object.t -> ?v_weights:Py.Object.t -> u_values:Py.Object.t -> v_values:Py.Object.t -> unit -> float
(**
Compute the energy distance between two 1D distributions.

.. versionadded:: 1.0.0

Parameters
----------
u_values, v_values : array_like
    Values observed in the (empirical) distribution.
u_weights, v_weights : array_like, optional
    Weight for each value. If unspecified, each value is assigned the same
    weight.
    `u_weights` (resp. `v_weights`) must have the same length as
    `u_values` (resp. `v_values`). If the weight sum differs from 1, it
    must still be positive and finite so that the weights can be normalized
    to sum to 1.

Returns
-------
distance : float
    The computed distance between the distributions.

Notes
-----
The energy distance between two distributions :math:`u` and :math:`v`, whose
respective CDFs are :math:`U` and :math:`V`, equals to:

.. math::

    D(u, v) = \left( 2\mathbb E|X - Y| - \mathbb E|X - X'| -
    \mathbb E|Y - Y'| \right)^{1/2}

where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are
independent random variables whose probability distribution is :math:`u`
(resp. :math:`v`).

As shown in [2]_, for one-dimensional real-valued variables, the energy
distance is linked to the non-distribution-free version of the Cramer-von
Mises distance:

.. math::

    D(u, v) = \sqrt{2} l_2(u, v) = \left( 2 \int_{-\infty}^{+\infty} (U-V)^2
    \right)^{1/2}

Note that the common Cramer-von Mises criterion uses the distribution-free
version of the distance. See [2]_ (section 2), for more details about both
versions of the distance.

The input distributions can be empirical, therefore coming from samples
whose values are effectively inputs of the function, or they can be seen as
generalized functions, in which case they are weighted sums of Dirac delta
functions located at the specified values.

References
----------
.. [1] 'Energy distance', https://en.wikipedia.org/wiki/Energy_distance
.. [2] Szekely 'E-statistics: The energy of statistical samples.' Bowling
       Green State University, Department of Mathematics and Statistics,
       Technical Report 02-16 (2002).
.. [3] Rizzo, Szekely 'Energy distance.' Wiley Interdisciplinary Reviews:
       Computational Statistics, 8(1):27-38 (2015).
.. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,
       Munos 'The Cramer Distance as a Solution to Biased Wasserstein
       Gradients' (2017). :arXiv:`1705.10743`.

Examples
--------
>>> from scipy.stats import energy_distance
>>> energy_distance([0], [2])
2.0000000000000004
>>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])
1.0000000000000002
>>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],
...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])
0.88003340976158217
*)

val entropy : ?qk:Py.Object.t -> ?base:float -> ?axis:int -> pk:Py.Object.t -> unit -> float
(**
Calculate the entropy of a distribution for given probability values.

If only probabilities `pk` are given, the entropy is calculated as
``S = -sum(pk * log(pk), axis=axis)``.

If `qk` is not None, then compute the Kullback-Leibler divergence
``S = sum(pk * log(pk / qk), axis=axis)``.

This routine will normalize `pk` and `qk` if they don't sum to 1.

Parameters
----------
pk : sequence
    Defines the (discrete) distribution. ``pk[i]`` is the (possibly
    unnormalized) probability of event ``i``.
qk : sequence, optional
    Sequence against which the relative entropy is computed. Should be in
    the same format as `pk`.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis: int, optional
    The axis along which the entropy is calculated. Default is 0.

Returns
-------
S : float
    The calculated entropy.

Examples
--------

>>> from scipy.stats import entropy

Bernoulli trial with different p.
The outcome of a fair coin is the most uncertain:

>>> entropy([1/2, 1/2], base=2)
1.0

The outcome of a biased coin is less uncertain:

>>> entropy([9/10, 1/10], base=2)
0.46899559358928117

Relative entropy:

>>> entropy([1/2, 1/2], qk=[9/10, 1/10])
0.5108256237659907
*)

val epps_singleton_2samp : ?t:[>`Ndarray] Np.Obj.t -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Epps-Singleton (ES) test statistic.

Test the null hypothesis that two samples have the same underlying
probability distribution.

Parameters
----------
x, y : array-like
    The two samples of observations to be tested. Input must not have more
    than one dimension. Samples can have different lengths.
t : array-like, optional
    The points (t1, ..., tn) where the empirical characteristic function is
    to be evaluated. It should be positive distinct numbers. The default
    value (0.4, 0.8) is proposed in [1]_. Input must not have more than
    one dimension.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The associated p-value based on the asymptotic chi2-distribution.

See Also
--------
ks_2samp, anderson_ksamp

Notes
-----
Testing whether two samples are generated by the same underlying
distribution is a classical question in statistics. A widely used test is
the Kolmogorov-Smirnov (KS) test which relies on the empirical
distribution function. Epps and Singleton introduce a test based on the
empirical characteristic function in [1]_.

One advantage of the ES test compared to the KS test is that is does
not assume a continuous distribution. In [1]_, the authors conclude
that the test also has a higher power than the KS test in many
examples. They recommend the use of the ES test for discrete samples as
well as continuous samples with at least 25 observations each, whereas
`anderson_ksamp` is recommended for smaller sample sizes in the
continuous case.

The p-value is computed from the asymptotic distribution of the test
statistic which follows a `chi2` distribution. If the sample size of both
`x` and `y` is below 25, the small sample correction proposed in [1]_ is
applied to the test statistic.

The default values of `t` are determined in [1]_ by considering
various distributions and finding good values that lead to a high power
of the test in general. Table III in [1]_ gives the optimal values for
the distributions tested in that study. The values of `t` are scaled by
the semi-interquartile range in the implementation, see [1]_.

References
----------
.. [1] T. W. Epps and K. J. Singleton, 'An omnibus test for the two-sample
   problem using the empirical characteristic function', Journal of
   Statistical Computation and Simulation 26, p. 177--203, 1986.

.. [2] S. J. Goerg and J. Kaiser, 'Nonparametric testing of distributions
   - the Epps-Singleton two-sample test using the empirical characteristic
   function', The Stata Journal 9(3), p. 454--465, 2009.
*)

val erlang : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Erlang_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An Erlang continuous random variable.

As an instance of the `rv_continuous` class, `erlang` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gamma

Notes
-----
The Erlang distribution is a special case of the Gamma distribution, with
the shape parameter `a` an integer.  Note that this restriction is not
enforced by `erlang`. It will, however, generate a warning the first time
a non-integer value is used for the shape parameter.

Refer to `gamma` for examples.
*)

val expon : ?loc:float -> ?scale:float -> unit -> [`Expon_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponential continuous random variable.

As an instance of the `rv_continuous` class, `expon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `expon` is:

.. math::

    f(x) = \exp(-x)

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``expon.pdf(x, loc, scale)`` is identically
equivalent to ``expon.pdf(y) / scale`` with
``y = (x - loc) / scale``.

A common parameterization for `expon` is in terms of the rate parameter
``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This
parameterization corresponds to using ``scale = 1 / lambda``.

Examples
--------
>>> from scipy.stats import expon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = expon.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(expon.ppf(0.01),
...                 expon.ppf(0.99), 100)
>>> ax.plot(x, expon.pdf(x),
...        'r-', lw=5, alpha=0.6, label='expon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = expon()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = expon.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], expon.cdf(vals))
True

Generate random numbers:

>>> r = expon.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponnorm : ?loc:float -> ?scale:float -> k:Py.Object.t -> unit -> [`Exponnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponentially modified Normal continuous random variable.

As an instance of the `rv_continuous` class, `exponnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(K, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, K, loc=0, scale=1)
    Probability density function.
logpdf(x, K, loc=0, scale=1)
    Log of the probability density function.
cdf(x, K, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, K, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, K, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, K, loc=0, scale=1)
    Log of the survival function.
ppf(q, K, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, K, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, K, loc=0, scale=1)
    Non-central moment of order n
stats(K, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(K, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, K, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(K,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(K, loc=0, scale=1)
    Median of the distribution.
mean(K, loc=0, scale=1)
    Mean of the distribution.
var(K, loc=0, scale=1)
    Variance of the distribution.
std(K, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, K, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `exponnorm` is:

.. math::

    f(x, K) = \frac{1}{2K} \exp\left(\frac{1}{2 K^2} - x / K \right)
              \text{erfc}\left(-\frac{x - 1/K}{\sqrt{2}}\right)

where :math:`x` is a real number and :math:`K > 0`.

It can be thought of as the sum of a standard normal random variable
and an independent exponentially distributed random variable with rate
``1/K``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponnorm.pdf(x, K, loc, scale)`` is identically
equivalent to ``exponnorm.pdf(y, K) / scale`` with
``y = (x - loc) / scale``.

An alternative parameterization of this distribution (for example, in
`Wikipedia <https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution>`_)
involves three parameters, :math:`\mu`, :math:`\lambda` and
:math:`\sigma`.
In the present parameterization this corresponds to having ``loc`` and
``scale`` equal to :math:`\mu` and :math:`\sigma`, respectively, and
shape parameter :math:`K = 1/(\sigma\lambda)`.

.. versionadded:: 0.16.0

Examples
--------
>>> from scipy.stats import exponnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> K = 1.5
>>> mean, var, skew, kurt = exponnorm.stats(K, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponnorm.ppf(0.01, K),
...                 exponnorm.ppf(0.99, K), 100)
>>> ax.plot(x, exponnorm.pdf(x, K),
...        'r-', lw=5, alpha=0.6, label='exponnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponnorm(K)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponnorm.ppf([0.001, 0.5, 0.999], K)
>>> np.allclose([0.001, 0.5, 0.999], exponnorm.cdf(vals, K))
True

Generate random numbers:

>>> r = exponnorm.rvs(K, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponpow : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Exponpow_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponential power continuous random variable.

As an instance of the `rv_continuous` class, `exponpow` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `exponpow` is:

.. math::

    f(x, b) = b x^{b-1} \exp(1 + x^b - \exp(x^b))

for :math:`x \ge 0`, :math:`b > 0`.  Note that this is a different
distribution from the exponential power distribution that is also known
under the names 'generalized normal' or 'generalized Gaussian'.

`exponpow` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponpow.pdf(x, b, loc, scale)`` is identically
equivalent to ``exponpow.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

References
----------
http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf

Examples
--------
>>> from scipy.stats import exponpow
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 2.7
>>> mean, var, skew, kurt = exponpow.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponpow.ppf(0.01, b),
...                 exponpow.ppf(0.99, b), 100)
>>> ax.plot(x, exponpow.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='exponpow pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponpow(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponpow.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], exponpow.cdf(vals, b))
True

Generate random numbers:

>>> r = exponpow.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val exponweib : ?loc:float -> ?scale:float -> a:Py.Object.t -> c:Py.Object.t -> unit -> [`Exponweib_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An exponentiated Weibull continuous random variable.

As an instance of the `rv_continuous` class, `exponweib` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, c, loc=0, scale=1)
    Median of the distribution.
mean(a, c, loc=0, scale=1)
    Mean of the distribution.
var(a, c, loc=0, scale=1)
    Variance of the distribution.
std(a, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min, numpy.random.mtrand.RandomState.weibull

Notes
-----
The probability density function for `exponweib` is:

.. math::

    f(x, a, c) = a c [1-\exp(-x^c)]^{a-1} \exp(-x^c) x^{c-1}

and its cumulative distribution function is:

.. math::

    F(x, a, c) = [1-\exp(-x^c)]^a

for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.

`exponweib` takes :math:`a` and :math:`c` as shape parameters:

* :math:`a` is the exponentiation parameter,
  with the special case :math:`a=1` corresponding to the
  (non-exponentiated) Weibull distribution `weibull_min`.
* :math:`c` is the shape parameter of the non-exponentiated Weibull law.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``exponweib.pdf(x, a, c, loc, scale)`` is identically
equivalent to ``exponweib.pdf(y, a, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution

Examples
--------
>>> from scipy.stats import exponweib
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, c = 2.89, 1.95
>>> mean, var, skew, kurt = exponweib.stats(a, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(exponweib.ppf(0.01, a, c),
...                 exponweib.ppf(0.99, a, c), 100)
>>> ax.plot(x, exponweib.pdf(x, a, c),
...        'r-', lw=5, alpha=0.6, label='exponweib pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = exponweib(a, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = exponweib.ppf([0.001, 0.5, 0.999], a, c)
>>> np.allclose([0.001, 0.5, 0.999], exponweib.cdf(vals, a, c))
True

Generate random numbers:

>>> r = exponweib.rvs(a, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val f : ?loc:float -> ?scale:float -> dfn:Py.Object.t -> dfd:Py.Object.t -> unit -> [`F_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An F continuous random variable.

As an instance of the `rv_continuous` class, `f` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(dfn, dfd, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, dfn, dfd, loc=0, scale=1)
    Probability density function.
logpdf(x, dfn, dfd, loc=0, scale=1)
    Log of the probability density function.
cdf(x, dfn, dfd, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, dfn, dfd, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, dfn, dfd, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, dfn, dfd, loc=0, scale=1)
    Log of the survival function.
ppf(q, dfn, dfd, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, dfn, dfd, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, dfn, dfd, loc=0, scale=1)
    Non-central moment of order n
stats(dfn, dfd, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(dfn, dfd, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, dfn, dfd, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(dfn, dfd), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(dfn, dfd, loc=0, scale=1)
    Median of the distribution.
mean(dfn, dfd, loc=0, scale=1)
    Mean of the distribution.
var(dfn, dfd, loc=0, scale=1)
    Variance of the distribution.
std(dfn, dfd, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, dfn, dfd, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `f` is:

.. math::

    f(x, df_1, df_2) = \frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}
                            {(df_2+df_1 x)^{(df_1+df_2)/2}
                             B(df_1/2, df_2/2)}

for :math:`x > 0`.

`f` takes ``dfn`` and ``dfd`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``f.pdf(x, dfn, dfd, loc, scale)`` is identically
equivalent to ``f.pdf(y, dfn, dfd) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import f
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> dfn, dfd = 29, 18
>>> mean, var, skew, kurt = f.stats(dfn, dfd, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(f.ppf(0.01, dfn, dfd),
...                 f.ppf(0.99, dfn, dfd), 100)
>>> ax.plot(x, f.pdf(x, dfn, dfd),
...        'r-', lw=5, alpha=0.6, label='f pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = f(dfn, dfd)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)
>>> np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd))
True

Generate random numbers:

>>> r = f.rvs(dfn, dfd, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val f_oneway : Py.Object.t list -> (float * float)
(**
Perform one-way ANOVA.

The one-way ANOVA tests the null hypothesis that two or more groups have
the same population mean.  The test is applied to samples from two or
more groups, possibly with differing sizes.

Parameters
----------
sample1, sample2, ... : array_like
    The sample measurements for each group.

Returns
-------
statistic : float
    The computed F-value of the test.
pvalue : float
    The associated p-value from the F-distribution.

Notes
-----
The ANOVA test has important assumptions that must be satisfied in order
for the associated p-value to be valid.

1. The samples are independent.
2. Each sample is from a normally distributed population.
3. The population standard deviations of the groups are all equal.  This
   property is known as homoscedasticity.

If these assumptions are not true for a given set of data, it may still be
possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) although
with some loss of power.

The algorithm is from Heiman[2], pp.394-7.

References
----------
.. [1] R. Lowry, 'Concepts and Applications of Inferential Statistics',
       Chapter 14, 2014, http://vassarstats.net/textbook/

.. [2] G.W. Heiman, 'Understanding research methods and statistics: An
       integrated introduction for psychology', Houghton, Mifflin and
       Company, 2001.

.. [3] G.H. McDonald, 'Handbook of Biological Statistics', One-way ANOVA.
       http://www.biostathandbook.com/onewayanova.html

Examples
--------
>>> import scipy.stats as stats

[3]_ Here are some data on a shell measurement (the length of the anterior
adductor muscle scar, standardized by dividing by length) in the mussel
Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;
Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a
much larger data set used in McDonald et al. (1991).

>>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,
...              0.0659, 0.0923, 0.0836]
>>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,
...            0.0725]
>>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]
>>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,
...            0.0689]
>>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]
>>> stats.f_oneway(tillamook, newport, petersburg, magadan, tvarminne)
(7.1210194716424473, 0.00028122423145345439)
*)

val fatiguelife : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Fatiguelife_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A fatigue-life (Birnbaum-Saunders) continuous random variable.

As an instance of the `rv_continuous` class, `fatiguelife` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `fatiguelife` is:

.. math::

    f(x, c) = \frac{x+1}{2c\sqrt{2\pi x^3}} \exp(-\frac{(x-1)^2}{2x c^2})

for :math:`x >= 0` and :math:`c > 0`.

`fatiguelife` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``fatiguelife.pdf(x, c, loc, scale)`` is identically
equivalent to ``fatiguelife.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] 'Birnbaum-Saunders distribution',
       https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution

Examples
--------
>>> from scipy.stats import fatiguelife
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 29
>>> mean, var, skew, kurt = fatiguelife.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(fatiguelife.ppf(0.01, c),
...                 fatiguelife.ppf(0.99, c), 100)
>>> ax.plot(x, fatiguelife.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='fatiguelife pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = fatiguelife(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = fatiguelife.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], fatiguelife.cdf(vals, c))
True

Generate random numbers:

>>> r = fatiguelife.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val find_repeats : [>`Ndarray] Np.Obj.t -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Find repeats and repeat counts.

Parameters
----------
arr : array_like
    Input array. This is cast to float64.

Returns
-------
values : ndarray
    The unique values from the (flattened) input that are repeated.

counts : ndarray
    Number of times the corresponding 'value' is repeated.

Notes
-----
In numpy >= 1.9 `numpy.unique` provides similar functionality. The main
difference is that `find_repeats` only returns repeated values.

Examples
--------
>>> from scipy import stats
>>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])
RepeatedResults(values=array([2.]), counts=array([4]))

>>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])
RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))
*)

val fisher_exact : ?alternative:[`Two_sided | `Less | `Greater] -> table:Py.Object.t -> unit -> (float * float)
(**
Perform a Fisher exact test on a 2x2 contingency table.

Parameters
----------
table : array_like of ints
    A 2x2 contingency table.  Elements should be non-negative integers.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided

Returns
-------
oddsratio : float
    This is prior odds ratio and not a posterior estimate.
p_value : float
    P-value, the probability of obtaining a distribution at least as
    extreme as the one that was actually observed, assuming that the
    null hypothesis is true.

See Also
--------
chi2_contingency : Chi-square test of independence of variables in a
    contingency table.

Notes
-----
The calculated odds ratio is different from the one R uses. This scipy
implementation returns the (more common) 'unconditional Maximum
Likelihood Estimate', while R uses the 'conditional Maximum Likelihood
Estimate'.

For tables with large numbers, the (inexact) chi-square test implemented
in the function `chi2_contingency` can also be used.

Examples
--------
Say we spend a few days counting whales and sharks in the Atlantic and
Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the
Indian ocean 2 whales and 5 sharks. Then our contingency table is::

            Atlantic  Indian
    whales     8        2
    sharks     1        5

We use this table to find the p-value:

>>> import scipy.stats as stats
>>> oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])
>>> pvalue
0.0349...

The probability that we would observe this or an even more imbalanced ratio
by chance is about 3.5%.  A commonly used significance level is 5%--if we
adopt that, we can therefore conclude that our observed imbalance is
statistically significant; whales prefer the Atlantic while sharks prefer
the Indian ocean.
*)

val fisk : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Fisk_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Fisk continuous random variable.

The Fisk distribution is also known as the log-logistic distribution.

As an instance of the `rv_continuous` class, `fisk` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `fisk` is:

.. math::

    f(x, c) = c x^{-c-1} (1 + x^{-c})^{-2}

for :math:`x >= 0` and :math:`c > 0`.

`fisk` takes ``c`` as a shape parameter for :math:`c`.

`fisk` is a special case of `burr` or `burr12` with ``d=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``fisk.pdf(x, c, loc, scale)`` is identically
equivalent to ``fisk.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
burr

Examples
--------
>>> from scipy.stats import fisk
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.09
>>> mean, var, skew, kurt = fisk.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(fisk.ppf(0.01, c),
...                 fisk.ppf(0.99, c), 100)
>>> ax.plot(x, fisk.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='fisk pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = fisk(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = fisk.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], fisk.cdf(vals, c))
True

Generate random numbers:

>>> r = fisk.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val fligner : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Perform Fligner-Killeen test for equality of variance.

Fligner's test tests the null hypothesis that all input samples
are from populations with equal variances.  Fligner-Killeen's test is
distribution free when populations are identical [2]_.

Parameters
----------
sample1, sample2, ... : array_like
    Arrays of sample data.  Need not be the same length.
center : {'mean', 'median', 'trimmed'}, optional
    Keyword argument controlling which function of the data is used in
    computing the test statistic.  The default is 'median'.
proportiontocut : float, optional
    When `center` is 'trimmed', this gives the proportion of data points
    to cut from each end. (See `scipy.stats.trim_mean`.)
    Default is 0.05.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value for the hypothesis test.

See Also
--------
bartlett : A parametric test for equality of k variances in normal samples
levene : A robust parametric test for equality of k variances

Notes
-----
As with Levene's test there are three variants of Fligner's test that
differ by the measure of central tendency used in the test.  See `levene`
for more information.

Conover et al. (1981) examine many of the existing parametric and
nonparametric tests by extensive simulations and they conclude that the
tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be
superior in terms of robustness of departures from normality and power [3]_.

References
----------
.. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.
       https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf

.. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample
       tests for scale. 'Journal of the American Statistical Association.'
       71(353), 210-213.

.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and
       Hypothesis Testing based on Quadratic Inference Function. Technical
       Report #99-03, Center for Likelihood Studies, Pennsylvania State
       University.

.. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A
       comparative study of tests for homogeneity of variances, with
       applications to the outer continental shelf biding data.
       Technometrics, 23(4), 351-361.
*)

val foldcauchy : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Foldcauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A folded Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `foldcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `foldcauchy` is:

.. math::

    f(x, c) = \frac{1}{\pi (1+(x-c)^2)} + \frac{1}{\pi (1+(x+c)^2)}

for :math:`x \ge 0`.

`foldcauchy` takes ``c`` as a shape parameter for :math:`c`.

Examples
--------
>>> from scipy.stats import foldcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 4.72
>>> mean, var, skew, kurt = foldcauchy.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(foldcauchy.ppf(0.01, c),
...                 foldcauchy.ppf(0.99, c), 100)
>>> ax.plot(x, foldcauchy.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='foldcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = foldcauchy(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = foldcauchy.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], foldcauchy.cdf(vals, c))
True

Generate random numbers:

>>> r = foldcauchy.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val foldnorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Foldnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A folded normal continuous random variable.

As an instance of the `rv_continuous` class, `foldnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `foldnorm` is:

.. math::

    f(x, c) = \sqrt{2/\pi} cosh(c x) \exp(-\frac{x^2+c^2}{2})

for :math:`c \ge 0`.

`foldnorm` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``foldnorm.pdf(x, c, loc, scale)`` is identically
equivalent to ``foldnorm.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import foldnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.95
>>> mean, var, skew, kurt = foldnorm.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(foldnorm.ppf(0.01, c),
...                 foldnorm.ppf(0.99, c), 100)
>>> ax.plot(x, foldnorm.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='foldnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = foldnorm(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = foldnorm.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], foldnorm.cdf(vals, c))
True

Generate random numbers:

>>> r = foldnorm.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val frechet_l : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Frechet_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Frechet left (or Weibull maximum) continuous random variable.

As an instance of the `rv_continuous` class, `frechet_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_max : The same distribution as `frechet_l`.

Notes
-----
The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``frechet_l.pdf(x, c, loc, scale)`` is identically
equivalent to ``frechet_l.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import frechet_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.63
>>> mean, var, skew, kurt = frechet_l.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(frechet_l.ppf(0.01, c),
...                 frechet_l.ppf(0.99, c), 100)
>>> ax.plot(x, frechet_l.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='frechet_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = frechet_l(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = frechet_l.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], frechet_l.cdf(vals, c))
True

Generate random numbers:

>>> r = frechet_l.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val frechet_r : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Frechet_r_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Frechet right (or Weibull minimum) continuous random variable.

As an instance of the `rv_continuous` class, `frechet_r` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min : The same distribution as `frechet_r`.

Notes
-----
The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``frechet_r.pdf(x, c, loc, scale)`` is identically
equivalent to ``frechet_r.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import frechet_r
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.89
>>> mean, var, skew, kurt = frechet_r.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(frechet_r.ppf(0.01, c),
...                 frechet_r.ppf(0.99, c), 100)
>>> ax.plot(x, frechet_r.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='frechet_r pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = frechet_r(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = frechet_r.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], frechet_r.cdf(vals, c))
True

Generate random numbers:

>>> r = frechet_r.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val friedmanchisquare : Py.Object.t list -> (float * float)
(**
Compute the Friedman test for repeated measurements.

The Friedman test tests the null hypothesis that repeated measurements of
the same individuals have the same distribution.  It is often used
to test for consistency among measurements obtained in different ways.
For example, if two measurement techniques are used on the same set of
individuals, the Friedman test can be used to determine if the two
measurement techniques are consistent.

Parameters
----------
measurements1, measurements2, measurements3... : array_like
    Arrays of measurements.  All of the arrays must have the same number
    of elements.  At least 3 sets of measurements must be given.

Returns
-------
statistic : float
    The test statistic, correcting for ties.
pvalue : float
    The associated p-value assuming that the test statistic has a chi
    squared distribution.

Notes
-----
Due to the assumption that the test statistic has a chi squared
distribution, the p-value is only reliable for n > 10 and more than
6 repeated measurements.

References
----------
.. [1] https://en.wikipedia.org/wiki/Friedman_test
*)

val gamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Gamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A gamma continuous random variable.

As an instance of the `rv_continuous` class, `gamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
erlang, expon

Notes
-----
The probability density function for `gamma` is:

.. math::

    f(x, a) = \frac{x^{a-1} \exp(-x)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`. Here :math:`\Gamma(a)` refers to the
gamma function.

`gamma` takes ``a`` as a shape parameter for :math:`a`.

When :math:`a` is an integer, `gamma` reduces to the Erlang
distribution, and when :math:`a=1` to the exponential distribution.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``gamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.99
>>> mean, var, skew, kurt = gamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gamma.ppf(0.01, a),
...                 gamma.ppf(0.99, a), 100)
>>> ax.plot(x, gamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='gamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a))
True

Generate random numbers:

>>> r = gamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gausshyper : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> c:Py.Object.t -> z:Py.Object.t -> unit -> [`Gausshyper_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gauss hypergeometric continuous random variable.

As an instance of the `rv_continuous` class, `gausshyper` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, c, z, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, c, z, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, c, z, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, c, z, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, c, z, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, c, z, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, c, z, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, c, z, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, c, z, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, c, z, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, c, z, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, c, z, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, c, z, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b, c, z), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, c, z, loc=0, scale=1)
    Median of the distribution.
mean(a, b, c, z, loc=0, scale=1)
    Mean of the distribution.
var(a, b, c, z, loc=0, scale=1)
    Variance of the distribution.
std(a, b, c, z, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, c, z, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gausshyper` is:

.. math::

    f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}

for :math:`0 \le x \le 1`, :math:`a > 0`, :math:`b > 0`, and
:math:`C = \frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.
:math:`F[2, 1]` is the Gauss hypergeometric function
`scipy.special.hyp2f1`.

`gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape
parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gausshyper.pdf(x, a, b, c, z, loc, scale)`` is identically
equivalent to ``gausshyper.pdf(y, a, b, c, z) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gausshyper
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b, c, z = 13.8, 3.12, 2.51, 5.18
>>> mean, var, skew, kurt = gausshyper.stats(a, b, c, z, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gausshyper.ppf(0.01, a, b, c, z),
...                 gausshyper.ppf(0.99, a, b, c, z), 100)
>>> ax.plot(x, gausshyper.pdf(x, a, b, c, z),
...        'r-', lw=5, alpha=0.6, label='gausshyper pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gausshyper(a, b, c, z)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gausshyper.ppf([0.001, 0.5, 0.999], a, b, c, z)
>>> np.allclose([0.001, 0.5, 0.999], gausshyper.cdf(vals, a, b, c, z))
True

Generate random numbers:

>>> r = gausshyper.rvs(a, b, c, z, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genexpon : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> c:Py.Object.t -> unit -> [`Genexpon_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized exponential continuous random variable.

As an instance of the `rv_continuous` class, `genexpon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, c, loc=0, scale=1)
    Median of the distribution.
mean(a, b, c, loc=0, scale=1)
    Mean of the distribution.
var(a, b, c, loc=0, scale=1)
    Variance of the distribution.
std(a, b, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genexpon` is:

.. math::

    f(x, a, b, c) = (a + b (1 - \exp(-c x)))
                    \exp(-a x - b x + \frac{b}{c}  (1-\exp(-c x)))

for :math:`x \ge 0`, :math:`a, b, c > 0`.

`genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genexpon.pdf(x, a, b, c, loc, scale)`` is identically
equivalent to ``genexpon.pdf(y, a, b, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
H.K. Ryu, 'An Extension of Marshall and Olkin's Bivariate Exponential
Distribution', Journal of the American Statistical Association, 1993.

N. Balakrishnan, 'The Exponential Distribution: Theory, Methods and
Applications', Asit P. Basu.

Examples
--------
>>> from scipy.stats import genexpon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b, c = 9.13, 16.2, 3.28
>>> mean, var, skew, kurt = genexpon.stats(a, b, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genexpon.ppf(0.01, a, b, c),
...                 genexpon.ppf(0.99, a, b, c), 100)
>>> ax.plot(x, genexpon.pdf(x, a, b, c),
...        'r-', lw=5, alpha=0.6, label='genexpon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genexpon(a, b, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genexpon.ppf([0.001, 0.5, 0.999], a, b, c)
>>> np.allclose([0.001, 0.5, 0.999], genexpon.cdf(vals, a, b, c))
True

Generate random numbers:

>>> r = genexpon.rvs(a, b, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genextreme : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genextreme_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized extreme value continuous random variable.

As an instance of the `rv_continuous` class, `genextreme` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_r

Notes
-----
For :math:`c=0`, `genextreme` is equal to `gumbel_r`.
The probability density function for `genextreme` is:

.. math::

    f(x, c) = \begin{cases}
                \exp(-\exp(-x)) \exp(-x)              &\text{for } c = 0\\
                \exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1}  &\text{for }
                                                        x \le 1/c, c > 0
              \end{cases}


Note that several sources and software packages use the opposite
convention for the sign of the shape parameter :math:`c`.

`genextreme` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genextreme.pdf(x, c, loc, scale)`` is identically
equivalent to ``genextreme.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genextreme
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = -0.1
>>> mean, var, skew, kurt = genextreme.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genextreme.ppf(0.01, c),
...                 genextreme.ppf(0.99, c), 100)
>>> ax.plot(x, genextreme.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genextreme pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genextreme(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genextreme.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c))
True

Generate random numbers:

>>> r = genextreme.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gengamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> c:Py.Object.t -> unit -> [`Gengamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized gamma continuous random variable.

As an instance of the `rv_continuous` class, `gengamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, c, loc=0, scale=1)
    Probability density function.
logpdf(x, a, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, c, loc=0, scale=1)
    Non-central moment of order n
stats(a, c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, c, loc=0, scale=1)
    Median of the distribution.
mean(a, c, loc=0, scale=1)
    Mean of the distribution.
var(a, c, loc=0, scale=1)
    Variance of the distribution.
std(a, c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gengamma` is:

.. math::

    f(x, a, c) = \frac{ |c| x^{c a-1} \exp(-x^c)}{\Gamma(a)}

for :math:`x \ge 0`, :math:`a > 0`, and :math:`c \ne 0`.
:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gengamma` takes :math:`a` and :math:`c` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gengamma.pdf(x, a, c, loc, scale)`` is identically
equivalent to ``gengamma.pdf(y, a, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gengamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, c = 4.42, -3.12
>>> mean, var, skew, kurt = gengamma.stats(a, c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gengamma.ppf(0.01, a, c),
...                 gengamma.ppf(0.99, a, c), 100)
>>> ax.plot(x, gengamma.pdf(x, a, c),
...        'r-', lw=5, alpha=0.6, label='gengamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gengamma(a, c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gengamma.ppf([0.001, 0.5, 0.999], a, c)
>>> np.allclose([0.001, 0.5, 0.999], gengamma.cdf(vals, a, c))
True

Generate random numbers:

>>> r = gengamma.rvs(a, c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genhalflogistic : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genhalflogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized half-logistic continuous random variable.

As an instance of the `rv_continuous` class, `genhalflogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genhalflogistic` is:

.. math::

    f(x, c) = \frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}

for :math:`0 \le x \le 1/c`, and :math:`c > 0`.

`genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genhalflogistic.pdf(x, c, loc, scale)`` is identically
equivalent to ``genhalflogistic.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genhalflogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.773
>>> mean, var, skew, kurt = genhalflogistic.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genhalflogistic.ppf(0.01, c),
...                 genhalflogistic.ppf(0.99, c), 100)
>>> ax.plot(x, genhalflogistic.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genhalflogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genhalflogistic(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genhalflogistic.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genhalflogistic.cdf(vals, c))
True

Generate random numbers:

>>> r = genhalflogistic.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val geninvgauss : ?loc:float -> ?scale:float -> p:Py.Object.t -> b:Py.Object.t -> unit -> [`Geninvgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Generalized Inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `geninvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, p, b, loc=0, scale=1)
    Probability density function.
logpdf(x, p, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, p, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, p, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, p, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, p, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, p, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, p, b, loc=0, scale=1)
    Non-central moment of order n
stats(p, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, p, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(p, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, b, loc=0, scale=1)
    Median of the distribution.
mean(p, b, loc=0, scale=1)
    Mean of the distribution.
var(p, b, loc=0, scale=1)
    Variance of the distribution.
std(p, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, p, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `geninvgauss` is:

.. math::

    f(x, p, b) = x^{p-1} \exp(-b (x + 1/x) / 2) / (2 K_p(b))

where `x > 0`, and the parameters `p, b` satisfy `b > 0` ([1]_).
:math:`K_p` is the modified Bessel function of second kind of order `p`
(`scipy.special.kv`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``geninvgauss.pdf(x, p, b, loc, scale)`` is identically
equivalent to ``geninvgauss.pdf(y, p, b) / scale`` with
``y = (x - loc) / scale``.

The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of
`geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.

Generating random variates is challenging for this distribution. The
implementation is based on [2]_.

References
----------
.. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, 'First hitting time
   models for the generalized inverse gaussian distribution',
   Stochastic Processes and their Applications 7, pp. 49--54, 1978.

.. [2] W. Hoermann and J. Leydold, 'Generating generalized inverse Gaussian
   random variates', Statistics and Computing, 24(4), p. 547--557, 2014.

Examples
--------
>>> from scipy.stats import geninvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p, b = 2.3, 1.5
>>> mean, var, skew, kurt = geninvgauss.stats(p, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(geninvgauss.ppf(0.01, p, b),
...                 geninvgauss.ppf(0.99, p, b), 100)
>>> ax.plot(x, geninvgauss.pdf(x, p, b),
...        'r-', lw=5, alpha=0.6, label='geninvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = geninvgauss(p, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = geninvgauss.ppf([0.001, 0.5, 0.999], p, b)
>>> np.allclose([0.001, 0.5, 0.999], geninvgauss.cdf(vals, p, b))
True

Generate random numbers:

>>> r = geninvgauss.rvs(p, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genlogistic : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genlogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized logistic continuous random variable.

As an instance of the `rv_continuous` class, `genlogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genlogistic` is:

.. math::

    f(x, c) = c \frac{\exp(-x)}
                     {(1 + \exp(-x))^{c+1}}

for :math:`x >= 0`, :math:`c > 0`.

`genlogistic` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genlogistic.pdf(x, c, loc, scale)`` is identically
equivalent to ``genlogistic.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genlogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.412
>>> mean, var, skew, kurt = genlogistic.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genlogistic.ppf(0.01, c),
...                 genlogistic.ppf(0.99, c), 100)
>>> ax.plot(x, genlogistic.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genlogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genlogistic(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genlogistic.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genlogistic.cdf(vals, c))
True

Generate random numbers:

>>> r = genlogistic.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gennorm : ?loc:float -> ?scale:float -> beta:Py.Object.t -> unit -> [`Gennorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized normal continuous random variable.

As an instance of the `rv_continuous` class, `gennorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, loc=0, scale=1)
    Non-central moment of order n
stats(beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, loc=0, scale=1)
    Median of the distribution.
mean(beta, loc=0, scale=1)
    Mean of the distribution.
var(beta, loc=0, scale=1)
    Variance of the distribution.
std(beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gennorm` is [1]_:

.. math::

    f(x, \beta) = \frac{\beta}{2 \Gamma(1/\beta)} \exp(-|x|^\beta)

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to a Laplace distribution.
For :math:`\beta = 2`, it is identical to a normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
laplace : Laplace distribution
norm : normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

Examples
--------
>>> from scipy.stats import gennorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta = 1.3
>>> mean, var, skew, kurt = gennorm.stats(beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gennorm.ppf(0.01, beta),
...                 gennorm.ppf(0.99, beta), 100)
>>> ax.plot(x, gennorm.pdf(x, beta),
...        'r-', lw=5, alpha=0.6, label='gennorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gennorm(beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gennorm.ppf([0.001, 0.5, 0.999], beta)
>>> np.allclose([0.001, 0.5, 0.999], gennorm.cdf(vals, beta))
True

Generate random numbers:

>>> r = gennorm.rvs(beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val genpareto : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Genpareto_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A generalized Pareto continuous random variable.

As an instance of the `rv_continuous` class, `genpareto` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `genpareto` is:

.. math::

    f(x, c) = (1 + c x)^{-1 - 1/c}

defined for :math:`x \ge 0` if :math:`c \ge 0`, and for
:math:`0 \le x \le -1/c` if :math:`c < 0`.

`genpareto` takes ``c`` as a shape parameter for :math:`c`.

For :math:`c=0`, `genpareto` reduces to the exponential
distribution, `expon`:

.. math::

    f(x, 0) = \exp(-x)

For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:

.. math::

    f(x, -1) = 1

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``genpareto.pdf(x, c, loc, scale)`` is identically
equivalent to ``genpareto.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import genpareto
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.1
>>> mean, var, skew, kurt = genpareto.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(genpareto.ppf(0.01, c),
...                 genpareto.ppf(0.99, c), 100)
>>> ax.plot(x, genpareto.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='genpareto pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = genpareto(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = genpareto.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], genpareto.cdf(vals, c))
True

Generate random numbers:

>>> r = genpareto.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val geom : ?loc:float -> p:Py.Object.t -> unit -> [`Geom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A geometric discrete random variable.

As an instance of the `rv_discrete` class, `geom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `geom` is:

.. math::

    f(k) = (1-p)^{k-1} p

for :math:`k \ge 1`.

`geom` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``geom.pmf(k, p, loc)`` is identically
equivalent to ``geom.pmf(k - loc, p)``.

See Also
--------
planck

Examples
--------
>>> from scipy.stats import geom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.5
>>> mean, var, skew, kurt = geom.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(geom.ppf(0.01, p),
...               geom.ppf(0.99, p))
>>> ax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')
>>> ax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = geom(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = geom.cdf(x, p)
>>> np.allclose(x, geom.ppf(prob, p))
True

Generate random numbers:

>>> r = geom.rvs(p, size=1000)
*)

val gilbrat : ?loc:float -> ?scale:float -> unit -> [`Gilbrat_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gilbrat continuous random variable.

As an instance of the `rv_continuous` class, `gilbrat` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gilbrat` is:

.. math::

    f(x) = \frac{1}{x \sqrt{2\pi}} \exp(-\frac{1}{2} (\log(x))^2)

`gilbrat` is a special case of `lognorm` with ``s=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gilbrat.pdf(x, loc, scale)`` is identically
equivalent to ``gilbrat.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gilbrat
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gilbrat.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gilbrat.ppf(0.01),
...                 gilbrat.ppf(0.99), 100)
>>> ax.plot(x, gilbrat.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gilbrat pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gilbrat()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gilbrat.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gilbrat.cdf(vals))
True

Generate random numbers:

>>> r = gilbrat.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the geometric mean along the specified axis.

Return the geometric average of the array elements.
That is:  n-th root of (x1 * x2 * ... * xn)

Parameters
----------
a : array_like
    Input array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the geometric mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If dtype is not specified, it defaults to the
    dtype of a, unless a has an integer dtype with a precision less than
    that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
gmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
hmean : Harmonic mean

Notes
-----
The geometric average is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity because masked
arrays automatically mask any non-finite values.

Examples
--------
>>> from scipy.stats import gmean
>>> gmean([1, 4])
2.0
>>> gmean([1, 2, 3, 4, 5, 6, 7])
3.3800151591412964
*)

val gompertz : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Gompertz_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Gompertz (or truncated Gumbel) continuous random variable.

As an instance of the `rv_continuous` class, `gompertz` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `gompertz` is:

.. math::

    f(x, c) = c \exp(x) \exp(-c (e^x-1))

for :math:`x \ge 0`, :math:`c > 0`.

`gompertz` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gompertz.pdf(x, c, loc, scale)`` is identically
equivalent to ``gompertz.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gompertz
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.947
>>> mean, var, skew, kurt = gompertz.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gompertz.ppf(0.01, c),
...                 gompertz.ppf(0.99, c), 100)
>>> ax.plot(x, gompertz.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='gompertz pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gompertz(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gompertz.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], gompertz.cdf(vals, c))
True

Generate random numbers:

>>> r = gompertz.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gstd : ?axis:[`Tuple of Py.Object.t | `I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the geometric standard deviation of an array.

The geometric standard deviation describes the spread of a set of numbers
where the geometric mean is preferred. It is a multiplicative factor, and
so a dimensionless quantity.

It is defined as the exponent of the standard deviation of ``log(a)``.
Mathematically the population geometric standard deviation can be
evaluated as::

    gstd = exp(std(log(a)))

.. versionadded:: 1.3.0

Parameters
----------
a : array_like
    An array like object containing the sample data.
axis : int, tuple or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Degree of freedom correction in the calculation of the
    geometric standard deviation. Default is 1.

Returns
-------
ndarray or float
    An array of the geometric standard deviation. If `axis` is None or `a`
    is a 1d array a float is returned.

Notes
-----
As the calculation requires the use of logarithms the geometric standard
deviation only supports strictly positive values. Any non-positive or
infinite values will raise a `ValueError`.
The geometric standard deviation is sometimes confused with the exponent of
the standard deviation, ``exp(std(a))``. Instead the geometric standard
deviation is ``exp(std(log(a)))``.
The default value for `ddof` is different to the default value (0) used
by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.

Examples
--------
Find the geometric standard deviation of a log-normally distributed sample.
Note that the standard deviation of the distribution is one, on a
log scale this evaluates to approximately ``exp(1)``.

>>> from scipy.stats import gstd
>>> np.random.seed(123)
>>> sample = np.random.lognormal(mean=0, sigma=1, size=1000)
>>> gstd(sample)
2.7217860664589946

Compute the geometric standard deviation of a multidimensional array and
of a given axis.

>>> a = np.arange(1, 25).reshape(2, 3, 4)
>>> gstd(a, axis=None)
2.2944076136018947
>>> gstd(a, axis=2)
array([[1.82424757, 1.22436866, 1.13183117],
       [1.09348306, 1.07244798, 1.05914985]])
>>> gstd(a, axis=(1,2))
array([2.12939215, 1.22120169])

The geometric standard deviation further handles masked arrays.

>>> a = np.arange(1, 25).reshape(2, 3, 4)
>>> ma = np.ma.masked_where(a > 16, a)
>>> ma
masked_array(
  data=[[[1, 2, 3, 4],
         [5, 6, 7, 8],
         [9, 10, 11, 12]],
        [[13, 14, 15, 16],
         [--, --, --, --],
         [--, --, --, --]]],
  mask=[[[False, False, False, False],
         [False, False, False, False],
         [False, False, False, False]],
        [[False, False, False, False],
         [ True,  True,  True,  True],
         [ True,  True,  True,  True]]],
  fill_value=999999)
>>> gstd(ma, axis=2)
masked_array(
  data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],
        [1.0934830582350938, --, --]],
  mask=[[False, False, False],
        [False,  True,  True]],
  fill_value=999999)
*)

val gumbel_l : ?loc:float -> ?scale:float -> unit -> [`Gumbel_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A left-skewed Gumbel continuous random variable.

As an instance of the `rv_continuous` class, `gumbel_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_r, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_l` is:

.. math::

    f(x) = \exp(x - e^x)

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gumbel_l.pdf(x, loc, scale)`` is identically
equivalent to ``gumbel_l.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gumbel_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gumbel_l.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gumbel_l.ppf(0.01),
...                 gumbel_l.ppf(0.99), 100)
>>> ax.plot(x, gumbel_l.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gumbel_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gumbel_l()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gumbel_l.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gumbel_l.cdf(vals))
True

Generate random numbers:

>>> r = gumbel_l.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val gumbel_r : ?loc:float -> ?scale:float -> unit -> [`Gumbel_r_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A right-skewed Gumbel continuous random variable.

As an instance of the `rv_continuous` class, `gumbel_r` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
gumbel_l, gompertz, genextreme

Notes
-----
The probability density function for `gumbel_r` is:

.. math::

    f(x) = \exp(-(x + e^{-x}))

The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett
distribution.  It is also related to the extreme value distribution,
log-Weibull and Gompertz distributions.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``gumbel_r.pdf(x, loc, scale)`` is identically
equivalent to ``gumbel_r.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import gumbel_r
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = gumbel_r.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(gumbel_r.ppf(0.01),
...                 gumbel_r.ppf(0.99), 100)
>>> ax.plot(x, gumbel_r.pdf(x),
...        'r-', lw=5, alpha=0.6, label='gumbel_r pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = gumbel_r()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = gumbel_r.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], gumbel_r.cdf(vals))
True

Generate random numbers:

>>> r = gumbel_r.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfcauchy : ?loc:float -> ?scale:float -> unit -> [`Halfcauchy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Half-Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `halfcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfcauchy` is:

.. math::

    f(x) = \frac{2}{\pi (1 + x^2)}

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halfcauchy.pdf(x, loc, scale)`` is identically
equivalent to ``halfcauchy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halfcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halfcauchy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfcauchy.ppf(0.01),
...                 halfcauchy.ppf(0.99), 100)
>>> ax.plot(x, halfcauchy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halfcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfcauchy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfcauchy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halfcauchy.cdf(vals))
True

Generate random numbers:

>>> r = halfcauchy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfgennorm : ?loc:float -> ?scale:float -> beta:Py.Object.t -> unit -> [`Halfgennorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
The upper half of a generalized normal continuous random variable.

As an instance of the `rv_continuous` class, `halfgennorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, beta, loc=0, scale=1)
    Non-central moment of order n
stats(beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(beta, loc=0, scale=1)
    Median of the distribution.
mean(beta, loc=0, scale=1)
    Mean of the distribution.
var(beta, loc=0, scale=1)
    Variance of the distribution.
std(beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfgennorm` is:

.. math::

    f(x, \beta) = \frac{\beta}{\Gamma(1/\beta)} \exp(-|x|^\beta)

for :math:`x > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`gennorm` takes ``beta`` as a shape parameter for :math:`\beta`.
For :math:`\beta = 1`, it is identical to an exponential distribution.
For :math:`\beta = 2`, it is identical to a half normal distribution
(with ``scale=1/sqrt(2)``).

See Also
--------
gennorm : generalized normal distribution
expon : exponential distribution
halfnorm : half normal distribution

References
----------

.. [1] 'Generalized normal distribution, Version 1',
       https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1

Examples
--------
>>> from scipy.stats import halfgennorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> beta = 0.675
>>> mean, var, skew, kurt = halfgennorm.stats(beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfgennorm.ppf(0.01, beta),
...                 halfgennorm.ppf(0.99, beta), 100)
>>> ax.plot(x, halfgennorm.pdf(x, beta),
...        'r-', lw=5, alpha=0.6, label='halfgennorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfgennorm(beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfgennorm.ppf([0.001, 0.5, 0.999], beta)
>>> np.allclose([0.001, 0.5, 0.999], halfgennorm.cdf(vals, beta))
True

Generate random numbers:

>>> r = halfgennorm.rvs(beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halflogistic : ?loc:float -> ?scale:float -> unit -> [`Halflogistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A half-logistic continuous random variable.

As an instance of the `rv_continuous` class, `halflogistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halflogistic` is:

.. math::

    f(x) = \frac{ 2 e^{-x} }{ (1+e^{-x})^2 }
         = \frac{1}{2} \text{sech}(x/2)^2

for :math:`x \ge 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halflogistic.pdf(x, loc, scale)`` is identically
equivalent to ``halflogistic.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halflogistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halflogistic.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halflogistic.ppf(0.01),
...                 halflogistic.ppf(0.99), 100)
>>> ax.plot(x, halflogistic.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halflogistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halflogistic()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halflogistic.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halflogistic.cdf(vals))
True

Generate random numbers:

>>> r = halflogistic.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val halfnorm : ?loc:float -> ?scale:float -> unit -> [`Halfnorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A half-normal continuous random variable.

As an instance of the `rv_continuous` class, `halfnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `halfnorm` is:

.. math::

    f(x) = \sqrt{2/\pi} \exp(-x^2 / 2)

for :math:`x >= 0`.

`halfnorm` is a special case of `chi` with ``df=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``halfnorm.pdf(x, loc, scale)`` is identically
equivalent to ``halfnorm.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import halfnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = halfnorm.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(halfnorm.ppf(0.01),
...                 halfnorm.ppf(0.99), 100)
>>> ax.plot(x, halfnorm.pdf(x),
...        'r-', lw=5, alpha=0.6, label='halfnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = halfnorm()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = halfnorm.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], halfnorm.cdf(vals))
True

Generate random numbers:

>>> r = halfnorm.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val hmean : ?axis:[`I of int | `None] -> ?dtype:Np.Dtype.t -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the harmonic mean along the specified axis.

That is:  n / (1/x1 + 1/x2 + ... + 1/xn)

Parameters
----------
a : array_like
    Input array, masked array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the harmonic mean is computed. Default is 0.
    If None, compute over the whole array `a`.
dtype : dtype, optional
    Type of the returned array and of the accumulator in which the
    elements are summed. If `dtype` is not specified, it defaults to the
    dtype of `a`, unless `a` has an integer `dtype` with a precision less
    than that of the default platform integer. In that case, the default
    platform integer is used.

Returns
-------
hmean : ndarray
    See `dtype` parameter above.

See Also
--------
numpy.mean : Arithmetic average
numpy.average : Weighted average
gmean : Geometric mean

Notes
-----
The harmonic mean is computed over a single dimension of the input
array, axis=0 by default, or all values in the array if axis=None.
float64 intermediate and return values are used for integer inputs.

Use masked arrays to ignore any non-finite values in the input or that
arise in the calculations such as Not a Number and infinity.

Examples
--------
>>> from scipy.stats import hmean
>>> hmean([1, 4])
1.6000000000000001
>>> hmean([1, 2, 3, 4, 5, 6, 7])
2.6997245179063363
*)

val hypergeom : ?loc:float -> m:Py.Object.t -> n:Py.Object.t -> n':Py.Object.t -> unit -> [`Hypergeom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A hypergeometric discrete random variable.

The hypergeometric distribution models drawing objects from a bin.
`M` is the total number of objects, `n` is total number of Type I objects.
The random variate represents the number of Type I objects in `N` drawn
without replacement from the total population.

As an instance of the `rv_discrete` class, `hypergeom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(M, n, N, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, M, n, N, loc=0)
    Probability mass function.
logpmf(k, M, n, N, loc=0)
    Log of the probability mass function.
cdf(k, M, n, N, loc=0)
    Cumulative distribution function.
logcdf(k, M, n, N, loc=0)
    Log of the cumulative distribution function.
sf(k, M, n, N, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, M, n, N, loc=0)
    Log of the survival function.
ppf(q, M, n, N, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, M, n, N, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(M, n, N, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(M, n, N, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(M, n, N), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(M, n, N, loc=0)
    Median of the distribution.
mean(M, n, N, loc=0)
    Mean of the distribution.
var(M, n, N, loc=0)
    Variance of the distribution.
std(M, n, N, loc=0)
    Standard deviation of the distribution.
interval(alpha, M, n, N, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not
universally accepted.  See the Examples for a clarification of the
definitions used here.

The probability mass function is defined as,

.. math:: p(k, M, n, N) = \frac{\binom{n}{k} \binom{M - n}{N - k}}
                               {\binom{M}{N}}

for :math:`k \in [\max(0, N - M + n), \min(n, N)]`, where the binomial
coefficients are defined as,

.. math:: \binom{n}{k} \equiv \frac{n!}{k! (n - k)!}.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``hypergeom.pmf(k, M, n, N, loc)`` is identically
equivalent to ``hypergeom.pmf(k - loc, M, n, N)``.

Examples
--------
>>> from scipy.stats import hypergeom
>>> import matplotlib.pyplot as plt

Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if
we want to know the probability of finding a given number of dogs if we
choose at random 12 of the 20 animals, we can initialize a frozen
distribution and plot the probability mass function:

>>> [M, n, N] = [20, 7, 12]
>>> rv = hypergeom(M, n, N)
>>> x = np.arange(0, n+1)
>>> pmf_dogs = rv.pmf(x)

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, pmf_dogs, 'bo')
>>> ax.vlines(x, 0, pmf_dogs, lw=2)
>>> ax.set_xlabel('# of dogs in our group of chosen animals')
>>> ax.set_ylabel('hypergeom PMF')
>>> plt.show()

Instead of using a frozen distribution we can also use `hypergeom`
methods directly.  To for example obtain the cumulative distribution
function, use:

>>> prb = hypergeom.cdf(x, M, n, N)

And to generate random numbers:

>>> R = hypergeom.rvs(M, n, N, size=10)
*)

val hypsecant : ?loc:float -> ?scale:float -> unit -> [`Hypsecant_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A hyperbolic secant continuous random variable.

As an instance of the `rv_continuous` class, `hypsecant` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `hypsecant` is:

.. math::

    f(x) = \frac{1}{\pi} \text{sech}(x)

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``hypsecant.pdf(x, loc, scale)`` is identically
equivalent to ``hypsecant.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import hypsecant
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = hypsecant.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(hypsecant.ppf(0.01),
...                 hypsecant.ppf(0.99), 100)
>>> ax.plot(x, hypsecant.pdf(x),
...        'r-', lw=5, alpha=0.6, label='hypsecant pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = hypsecant()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = hypsecant.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], hypsecant.cdf(vals))
True

Generate random numbers:

>>> r = hypsecant.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invgamma : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Invgamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverted gamma continuous random variable.

As an instance of the `rv_continuous` class, `invgamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invgamma` is:

.. math::

    f(x, a) = \frac{x^{-a-1}}{\Gamma(a)} \exp(-\frac{1}{x})

for :math:`x >= 0`, :math:`a > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

`invgamma` takes ``a`` as a shape parameter for :math:`a`.

`invgamma` is a special case of `gengamma` with ``c=-1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invgamma.pdf(x, a, loc, scale)`` is identically
equivalent to ``invgamma.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import invgamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 4.07
>>> mean, var, skew, kurt = invgamma.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invgamma.ppf(0.01, a),
...                 invgamma.ppf(0.99, a), 100)
>>> ax.plot(x, invgamma.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='invgamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invgamma(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invgamma.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], invgamma.cdf(vals, a))
True

Generate random numbers:

>>> r = invgamma.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invgauss : ?loc:float -> ?scale:float -> mu:Py.Object.t -> unit -> [`Invgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `invgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, mu, loc=0, scale=1)
    Probability density function.
logpdf(x, mu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, mu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, mu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, mu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, mu, loc=0, scale=1)
    Log of the survival function.
ppf(q, mu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, mu, loc=0, scale=1)
    Non-central moment of order n
stats(mu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, mu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0, scale=1)
    Median of the distribution.
mean(mu, loc=0, scale=1)
    Mean of the distribution.
var(mu, loc=0, scale=1)
    Variance of the distribution.
std(mu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2 \pi x^3}}
                \exp(-\frac{(x-\mu)^2}{2 x \mu^2})

for :math:`x >= 0` and :math:`\mu > 0`.

`invgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invgauss.pdf(x, mu, loc, scale)`` is identically
equivalent to ``invgauss.pdf(y, mu) / scale`` with
``y = (x - loc) / scale``.

When :math:`\mu` is too small, evaluating the cumulative distribution
function will be inaccurate due to ``cdf(mu -> 0) = inf * 0``.
NaNs are returned for :math:`\mu \le 0.0028`.

Examples
--------
>>> from scipy.stats import invgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.145
>>> mean, var, skew, kurt = invgauss.stats(mu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invgauss.ppf(0.01, mu),
...                 invgauss.ppf(0.99, mu), 100)
>>> ax.plot(x, invgauss.pdf(x, mu),
...        'r-', lw=5, alpha=0.6, label='invgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invgauss(mu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invgauss.ppf([0.001, 0.5, 0.999], mu)
>>> np.allclose([0.001, 0.5, 0.999], invgauss.cdf(vals, mu))
True

Generate random numbers:

>>> r = invgauss.rvs(mu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invweibull : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Invweibull_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An inverted Weibull continuous random variable.

This distribution is also known as the Frchet distribution or the
type II extreme value distribution.

As an instance of the `rv_continuous` class, `invweibull` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `invweibull` is:

.. math::

    f(x, c) = c x^{-c-1} \exp(-x^{-c})

for :math:`x > 0`, :math:`c > 0`.

`invweibull` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``invweibull.pdf(x, c, loc, scale)`` is identically
equivalent to ``invweibull.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, 'The generalized inverse
Weibull distribution', Stat. Papers, vol. 52, pp. 591-619, 2011.

Examples
--------
>>> from scipy.stats import invweibull
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 10.6
>>> mean, var, skew, kurt = invweibull.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(invweibull.ppf(0.01, c),
...                 invweibull.ppf(0.99, c), 100)
>>> ax.plot(x, invweibull.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='invweibull pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = invweibull(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = invweibull.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], invweibull.cdf(vals, c))
True

Generate random numbers:

>>> r = invweibull.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val invwishart : ?df:int -> ?scale:float -> ?seed:Py.Object.t -> unit -> Py.Object.t
(**
An inverse Wishart random variable.

The `df` keyword specifies the degrees of freedom. The `scale` keyword
specifies the scale matrix, which must be symmetric and positive definite.
In this context, the scale matrix is often interpreted in terms of a
multivariate normal covariance matrix.

Methods
-------
``pdf(x, df, scale)``
    Probability density function.
``logpdf(x, df, scale)``
    Log of the probability density function.
``rvs(df, scale, size=1, random_state=None)``
    Draw random samples from an inverse Wishart distribution.

Parameters
----------
x : array_like
    Quantiles, with the last axis of `x` denoting the components.
df : int
    Degrees of freedom, must be greater than or equal to dimension of the
    scale matrix
scale : array_like
    Symmetric positive definite scale matrix of the distribution
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Alternatively, the object may be called (as a function) to fix the degrees
of freedom and scale parameters, returning a 'frozen' inverse Wishart
random variable:

rv = invwishart(df=1, scale=1)
    - Frozen object with the same methods but holding the given
      degrees of freedom and scale fixed.

See Also
--------
wishart

Notes
-----


The scale matrix `scale` must be a symmetric positive definite
matrix. Singular matrices, including the symmetric positive semi-definite
case, are not supported.

The inverse Wishart distribution is often denoted

.. math::

    W_p^{-1}(\nu, \Psi)

where :math:`\nu` is the degrees of freedom and :math:`\Psi` is the
:math:`p \times p` scale matrix.

The probability density function for `invwishart` has support over positive
definite matrices :math:`S`; if :math:`S \sim W^{-1}_p(\nu, \Sigma)`,
then its PDF is given by:

.. math::

    f(S) = \frac{ |\Sigma|^\frac{\nu}{2}}{2^{ \frac{\nu p}{2} }
           |S|^{\frac{\nu + p + 1}{2}} \Gamma_p \left(\frac{\nu}{2} \right)}
           \exp\left( -tr(\Sigma S^{-1}) / 2 \right)

If :math:`S \sim W_p^{-1}(\nu, \Psi)` (inverse Wishart) then
:math:`S^{-1} \sim W_p(\nu, \Psi^{-1})` (Wishart).

If the scale matrix is 1-dimensional and equal to one, then the inverse
Wishart distribution :math:`W_1(\nu, 1)` collapses to the
inverse Gamma distribution with parameters shape = :math:`\frac{\nu}{2}`
and scale = :math:`\frac{1}{2}`.

.. versionadded:: 0.16.0

References
----------
.. [1] M.L. Eaton, 'Multivariate Statistics: A Vector Space Approach',
       Wiley, 1983.
.. [2] M.C. Jones, 'Generating Inverse Wishart Matrices', Communications
       in Statistics - Simulation and Computation, vol. 14.2, pp.511-514,
       1985.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy.stats import invwishart, invgamma
>>> x = np.linspace(0.01, 1, 100)
>>> iw = invwishart.pdf(x, df=6, scale=1)
>>> iw[:3]
array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])
>>> ig = invgamma.pdf(x, 6/2., scale=1./2)
>>> ig[:3]
array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])
>>> plt.plot(x, iw)

The input quantiles can be any shape of array, as long as the last
axis labels the components.
*)

val iqr : ?axis:[`Sequence_of_int of Py.Object.t | `I of int] -> ?rng:Py.Object.t -> ?scale:float -> ?nan_policy:[`Propagate | `Raise | `Omit] -> ?interpolation:[`Linear | `Lower | `Higher | `Midpoint | `Nearest] -> ?keepdims:bool -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the interquartile range of the data along the specified axis.

The interquartile range (IQR) is the difference between the 75th and
25th percentile of the data. It is a measure of the dispersion
similar to standard deviation or variance, but is much more robust
against outliers [2]_.

The ``rng`` parameter allows this function to compute other
percentile ranges than the actual IQR. For example, setting
``rng=(0, 100)`` is equivalent to `numpy.ptp`.

The IQR of an empty array is `np.nan`.

.. versionadded:: 0.18.0

Parameters
----------
x : array_like
    Input array or object that can be converted to an array.
axis : int or sequence of int, optional
    Axis along which the range is computed. The default is to
    compute the IQR for the entire array.
rng : Two-element sequence containing floats in range of [0,100] optional
    Percentiles over which to compute the range. Each must be
    between 0 and 100, inclusive. The default is the true IQR:
    `(25, 75)`. The order of the elements is not important.
scale : scalar or str, optional
    The numerical value of scale will be divided out of the final
    result. The following string values are recognized:

      'raw' : No scaling, just return the raw IQR.
      'normal' : Scale by :math:`2 \sqrt{2} erf^{-1}(\frac{1}{2}) \approx 1.349`.

    The default is 'raw'. Array-like scale is also allowed, as long
    as it broadcasts correctly to the output such that
    ``out / scale`` is a valid operation. The output dimensions
    depend on the input array, `x`, the `axis` argument, and the
    `keepdims` flag.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values
interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}, optional
    Specifies the interpolation method to use when the percentile
    boundaries lie between two data points `i` and `j`.
    The following options are available (default is 'linear'):

      * 'linear': `i + (j - i) * fraction`, where `fraction` is the
        fractional part of the index surrounded by `i` and `j`.
      * 'lower': `i`.
      * 'higher': `j`.
      * 'nearest': `i` or `j` whichever is nearest.
      * 'midpoint': `(i + j) / 2`.

keepdims : bool, optional
    If this is set to `True`, the reduced axes are left in the
    result as dimensions with size one. With this option, the result
    will broadcast correctly against the original array `x`.

Returns
-------
iqr : scalar or ndarray
    If ``axis=None``, a scalar is returned. If the input contains
    integers or floats of smaller precision than ``np.float64``, then the
    output data-type is ``np.float64``. Otherwise, the output data-type is
    the same as that of the input.

See Also
--------
numpy.std, numpy.var

Notes
-----
This function is heavily dependent on the version of `numpy` that is
installed. Versions greater than 1.11.0b3 are highly recommended, as they
include a number of enhancements and fixes to `numpy.percentile` and
`numpy.nanpercentile` that affect the operation of this function. The
following modifications apply:

Below 1.10.0 : `nan_policy` is poorly defined.
    The default behavior of `numpy.percentile` is used for 'propagate'. This
    is a hybrid of 'omit' and 'propagate' that mostly yields a skewed
    version of 'omit' since NaNs are sorted to the end of the data. A
    warning is raised if there are NaNs in the data.
Below 1.9.0: `numpy.nanpercentile` does not exist.
    This means that `numpy.percentile` is used regardless of `nan_policy`
    and a warning is issued. See previous item for a description of the
    behavior.
Below 1.9.0: `keepdims` and `interpolation` are not supported.
    The keywords get ignored with a warning if supplied with non-default
    values. However, multiple axes are still supported.

References
----------
.. [1] 'Interquartile range' https://en.wikipedia.org/wiki/Interquartile_range
.. [2] 'Robust measures of scale' https://en.wikipedia.org/wiki/Robust_measures_of_scale
.. [3] 'Quantile' https://en.wikipedia.org/wiki/Quantile

Examples
--------
>>> from scipy.stats import iqr
>>> x = np.array([[10, 7, 4], [3, 2, 1]])
>>> x
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> iqr(x)
4.0
>>> iqr(x, axis=0)
array([ 3.5,  2.5,  1.5])
>>> iqr(x, axis=1)
array([ 3.,  1.])
>>> iqr(x, axis=1, keepdims=True)
array([[ 3.],
       [ 1.]])
*)

val itemfreq : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> Py.Object.t
(**
`itemfreq` is deprecated!
`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`

    Return a 2-D array of item frequencies.

    Parameters
    ----------
    a : (N,) array_like
        Input array.

    Returns
    -------
    itemfreq : (K, 2) ndarray
        A 2-D frequency table.  Column 1 contains sorted, unique values from
        `a`, column 2 contains their respective counts.

    Examples
    --------
    >>> from scipy import stats
    >>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])
    >>> stats.itemfreq(a)
    array([[ 0.,  2.],
           [ 1.,  4.],
           [ 2.,  2.],
           [ 4.,  1.],
           [ 5.,  1.]])
    >>> np.bincount(a)
    array([2, 4, 2, 0, 1, 1])

    >>> stats.itemfreq(a/10.)
    array([[ 0. ,  2. ],
           [ 0.1,  4. ],
           [ 0.2,  2. ],
           [ 0.4,  1. ],
           [ 0.5,  1. ]])

    
*)

val jarque_bera : [>`Ndarray] Np.Obj.t -> (float * float)
(**
Perform the Jarque-Bera goodness of fit test on sample data.

The Jarque-Bera test tests whether the sample data has the skewness and
kurtosis matching a normal distribution.

Note that this test only works for a large enough number of data samples
(>2000) as the test statistic asymptotically has a Chi-squared distribution
with 2 degrees of freedom.

Parameters
----------
x : array_like
    Observations of a random variable.

Returns
-------
jb_value : float
    The test statistic.
p : float
    The p-value for the hypothesis test.

References
----------
.. [1] Jarque, C. and Bera, A. (1980) 'Efficient tests for normality,
       homoscedasticity and serial independence of regression residuals',
       6 Econometric Letters 255-259.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(987654321)
>>> x = np.random.normal(0, 1, 100000)
>>> y = np.random.rayleigh(1, 100000)
>>> stats.jarque_bera(x)
(4.7165707989581342, 0.09458225503041906)
>>> stats.jarque_bera(y)
(6713.7098548143422, 0.0)
*)

val johnsonsb : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Johnsonsb_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Johnson SB continuous random variable.

As an instance of the `rv_continuous` class, `johnsonsb` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
johnsonsu

Notes
-----
The probability density function for `johnsonsb` is:

.. math::

    f(x, a, b) = \frac{b}{x(1-x)}  \phi(a + b \log \frac{x}{1-x} )

for :math:`0 <= x < =1` and :math:`a, b > 0`, and :math:`\phi` is the normal
pdf.

`johnsonsb` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``johnsonsb.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``johnsonsb.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import johnsonsb
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 4.32, 3.18
>>> mean, var, skew, kurt = johnsonsb.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(johnsonsb.ppf(0.01, a, b),
...                 johnsonsb.ppf(0.99, a, b), 100)
>>> ax.plot(x, johnsonsb.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='johnsonsb pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = johnsonsb(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = johnsonsb.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], johnsonsb.cdf(vals, a, b))
True

Generate random numbers:

>>> r = johnsonsb.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val johnsonsu : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Johnsonsu_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Johnson SU continuous random variable.

As an instance of the `rv_continuous` class, `johnsonsu` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
johnsonsb

Notes
-----
The probability density function for `johnsonsu` is:

.. math::

    f(x, a, b) = \frac{b}{\sqrt{x^2 + 1}}
                 \phi(a + b \log(x + \sqrt{x^2 + 1}))

for all :math:`x, a, b > 0`, and :math:`\phi` is the normal pdf.

`johnsonsu` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``johnsonsu.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``johnsonsu.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import johnsonsu
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 2.55, 2.25
>>> mean, var, skew, kurt = johnsonsu.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(johnsonsu.ppf(0.01, a, b),
...                 johnsonsu.ppf(0.99, a, b), 100)
>>> ax.plot(x, johnsonsu.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='johnsonsu pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = johnsonsu(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = johnsonsu.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], johnsonsu.cdf(vals, a, b))
True

Generate random numbers:

>>> r = johnsonsu.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kappa3 : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Kappa3_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kappa 3 parameter distribution.

As an instance of the `rv_continuous` class, `kappa3` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `kappa3` is:

.. math::

    f(x, a) = a (a + x^a)^{-(a + 1)/a}

for :math:`x > 0` and :math:`a > 0`.

`kappa3` takes ``a`` as a shape parameter for :math:`a`.

References
----------
P.W. Mielke and E.S. Johnson, 'Three-Parameter Kappa Distribution Maximum
Likelihood and Likelihood Ratio Tests', Methods in Weather Research,
701-707, (September, 1973),
https://doi.org/10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2

B. Kumphon, 'Maximum Entropy and Maximum Likelihood Estimation for the
Three-Parameter Kappa Distribution', Open Journal of Statistics, vol 2,
415-419 (2012), https://doi.org/10.4236/ojs.2012.24050

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kappa3.pdf(x, a, loc, scale)`` is identically
equivalent to ``kappa3.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import kappa3
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1
>>> mean, var, skew, kurt = kappa3.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kappa3.ppf(0.01, a),
...                 kappa3.ppf(0.99, a), 100)
>>> ax.plot(x, kappa3.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='kappa3 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kappa3(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kappa3.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], kappa3.cdf(vals, a))
True

Generate random numbers:

>>> r = kappa3.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kappa4 : ?loc:float -> ?scale:float -> h:Py.Object.t -> k:Py.Object.t -> unit -> [`Kappa4_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kappa 4 parameter distribution.

As an instance of the `rv_continuous` class, `kappa4` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(h, k, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, h, k, loc=0, scale=1)
    Probability density function.
logpdf(x, h, k, loc=0, scale=1)
    Log of the probability density function.
cdf(x, h, k, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, h, k, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, h, k, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, h, k, loc=0, scale=1)
    Log of the survival function.
ppf(q, h, k, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, h, k, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, h, k, loc=0, scale=1)
    Non-central moment of order n
stats(h, k, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(h, k, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, h, k, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(h, k), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(h, k, loc=0, scale=1)
    Median of the distribution.
mean(h, k, loc=0, scale=1)
    Mean of the distribution.
var(h, k, loc=0, scale=1)
    Variance of the distribution.
std(h, k, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, h, k, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for kappa4 is:

.. math::

    f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}

if :math:`h` and :math:`k` are not equal to 0.

If :math:`h` or :math:`k` are zero then the pdf can be simplified:

h = 0 and k != 0::

    kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*
                          exp(-(1.0 - k*x)**(1.0/k))

h != 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)

h = 0 and k = 0::

    kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))

kappa4 takes :math:`h` and :math:`k` as shape parameters.

The kappa4 distribution returns other distributions when certain
:math:`h` and :math:`k` values are used.

+------+-------------+----------------+------------------+
| h    | k=0.0       | k=1.0          | -inf<=k<=inf     |
+======+=============+================+==================+
| -1.0 | Logistic    |                | Generalized      |
|      |             |                | Logistic(1)      |
|      |             |                |                  |
|      | logistic(x) |                |                  |
+------+-------------+----------------+------------------+
|  0.0 | Gumbel      | Reverse        | Generalized      |
|      |             | Exponential(2) | Extreme Value    |
|      |             |                |                  |
|      | gumbel_r(x) |                | genextreme(x, k) |
+------+-------------+----------------+------------------+
|  1.0 | Exponential | Uniform        | Generalized      |
|      |             |                | Pareto           |
|      |             |                |                  |
|      | expon(x)    | uniform(x)     | genpareto(x, -k) |
+------+-------------+----------------+------------------+

(1) There are at least five generalized logistic distributions.
    Four are described here:
    https://en.wikipedia.org/wiki/Generalized_logistic_distribution
    The 'fifth' one is the one kappa4 should match which currently
    isn't implemented in scipy:
    https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution
    https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html
(2) This distribution is currently not in scipy.

References
----------
J.C. Finney, 'Optimization of a Skewed Logistic Distribution With Respect
to the Kolmogorov-Smirnov Test', A Dissertation Submitted to the Graduate
Faculty of the Louisiana State University and Agricultural and Mechanical
College, (August, 2004),
https://digitalcommons.lsu.edu/gradschool_dissertations/3672

J.R.M. Hosking, 'The four-parameter kappa distribution'. IBM J. Res.
Develop. 38 (3), 25 1-258 (1994).

B. Kumphon, A. Kaew-Man, P. Seenoi, 'A Rainfall Distribution for the Lampao
Site in the Chi River Basin, Thailand', Journal of Water Resource and
Protection, vol. 4, 866-869, (2012).
https://doi.org/10.4236/jwarp.2012.410101

C. Winchester, 'On Estimation of the Four-Parameter Kappa Distribution', A
Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March
2000).
http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kappa4.pdf(x, h, k, loc, scale)`` is identically
equivalent to ``kappa4.pdf(y, h, k) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import kappa4
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> h, k = 0.1, 0
>>> mean, var, skew, kurt = kappa4.stats(h, k, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kappa4.ppf(0.01, h, k),
...                 kappa4.ppf(0.99, h, k), 100)
>>> ax.plot(x, kappa4.pdf(x, h, k),
...        'r-', lw=5, alpha=0.6, label='kappa4 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kappa4(h, k)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kappa4.ppf([0.001, 0.5, 0.999], h, k)
>>> np.allclose([0.001, 0.5, 0.999], kappa4.cdf(vals, h, k))
True

Generate random numbers:

>>> r = kappa4.rvs(h, k, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kendalltau : ?initial_lexsort:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> ?method_:[`Auto | `Asymptotic | `Exact] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Calculate Kendall's tau, a correlation measure for ordinal data.

Kendall's tau is a measure of the correspondence between two rankings.
Values close to 1 indicate strong agreement, values close to -1 indicate
strong disagreement.  This is the 1945 'tau-b' version of Kendall's
tau [2]_, which can account for ties and which reduces to the 1938 'tau-a'
version [1]_ in absence of ties.

Parameters
----------
x, y : array_like
    Arrays of rankings, of the same shape. If arrays are not 1-D, they will
    be flattened to 1-D.
initial_lexsort : bool, optional
    Unused (deprecated).
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values
method : {'auto', 'asymptotic', 'exact'}, optional
    Defines which method is used to calculate the p-value [5]_.
    The following options are available (default is 'auto'):

      * 'auto': selects the appropriate method based on a trade-off between
        speed and accuracy
      * 'asymptotic': uses a normal approximation valid for large samples
      * 'exact': computes the exact p-value, but can only be used if no ties
        are present

Returns
-------
correlation : float
   The tau statistic.
pvalue : float
   The two-sided p-value for a hypothesis test whose null hypothesis is
   an absence of association, tau = 0.

See Also
--------
spearmanr : Calculates a Spearman rank-order correlation coefficient.
theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).
weightedtau : Computes a weighted version of Kendall's tau.

Notes
-----
The definition of Kendall's tau that is used is [2]_::

  tau = (P - Q) / sqrt((P + Q + T) * (P + Q + U))

where P is the number of concordant pairs, Q the number of discordant
pairs, T the number of ties only in `x`, and U the number of ties only in
`y`.  If a tie occurs for the same pair in both `x` and `y`, it is not
added to either T or U.

References
----------
.. [1] Maurice G. Kendall, 'A New Measure of Rank Correlation', Biometrika
       Vol. 30, No. 1/2, pp. 81-93, 1938.
.. [2] Maurice G. Kendall, 'The treatment of ties in ranking problems',
       Biometrika Vol. 33, No. 3, pp. 239-251. 1945.
.. [3] Gottfried E. Noether, 'Elements of Nonparametric Statistics', John
       Wiley & Sons, 1967.
.. [4] Peter M. Fenwick, 'A new data structure for cumulative frequency
       tables', Software: Practice and Experience, Vol. 24, No. 3,
       pp. 327-336, 1994.
.. [5] Maurice G. Kendall, 'Rank Correlation Methods' (4th Edition),
       Charles Griffin & Co., 1970.

Examples
--------
>>> from scipy import stats
>>> x1 = [12, 2, 1, 12, 2]
>>> x2 = [1, 4, 7, 1, 0]
>>> tau, p_value = stats.kendalltau(x1, x2)
>>> tau
-0.47140452079103173
>>> p_value
0.2827454599327748
*)

val kruskal : ?kwargs:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Compute the Kruskal-Wallis H-test for independent samples.

The Kruskal-Wallis H-test tests the null hypothesis that the population
median of all of the groups are equal.  It is a non-parametric version of
ANOVA.  The test works on 2 or more independent samples, which may have
different sizes.  Note that rejecting the null hypothesis does not
indicate which of the groups differs.  Post hoc comparisons between
groups are required to determine which groups are different.

Parameters
----------
sample1, sample2, ... : array_like
   Two or more arrays with the sample measurements can be given as
   arguments.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
   The Kruskal-Wallis H statistic, corrected for ties.
pvalue : float
   The p-value for the test using the assumption that H has a chi
   square distribution.

See Also
--------
f_oneway : 1-way ANOVA.
mannwhitneyu : Mann-Whitney rank test on two samples.
friedmanchisquare : Friedman test for repeated measurements.

Notes
-----
Due to the assumption that H has a chi square distribution, the number
of samples in each group must not be too small.  A typical rule is
that each sample must have at least 5 measurements.

References
----------
.. [1] W. H. Kruskal & W. W. Wallis, 'Use of Ranks in
   One-Criterion Variance Analysis', Journal of the American Statistical
   Association, Vol. 47, Issue 260, pp. 583-621, 1952.
.. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance

Examples
--------
>>> from scipy import stats
>>> x = [1, 3, 5, 7, 9]
>>> y = [2, 4, 6, 8, 10]
>>> stats.kruskal(x, y)
KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)

>>> x = [1, 1, 1]
>>> y = [2, 2, 2]
>>> z = [2, 2]
>>> stats.kruskal(x, y, z)
KruskalResult(statistic=7.0, pvalue=0.0301973834223185)
*)

val ks_2samp : ?alternative:[`Two_sided | `Less | `Greater] -> ?mode:[`Auto | `Exact | `Asymp] -> data1:Py.Object.t -> data2:Py.Object.t -> unit -> (float * float)
(**
Compute the Kolmogorov-Smirnov statistic on 2 samples.

This is a two-sided test for the null hypothesis that 2 independent samples
are drawn from the same continuous distribution.  The alternative hypothesis
can be either 'two-sided' (default), 'less' or 'greater'.

Parameters
----------
data1, data2 : sequence of 1-D ndarrays
    Two arrays of sample observations assumed to be drawn from a continuous
    distribution, sample sizes can be different.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided, see explanation in Notes
      * 'greater': one-sided, see explanation in Notes
mode : {'auto', 'exact', 'asymp'}, optional
    Defines the method used for calculating the p-value.
    The following options are available (default is 'auto'):

      * 'auto' : use 'exact' for small size arrays, 'asymp' for large
      * 'exact' : use approximation to exact distribution of test statistic
      * 'asymp' : use asymptotic distribution of test statistic

Returns
-------
statistic : float
    KS statistic.
pvalue : float
    Two-tailed p-value.

See Also
--------
kstest

Notes
-----
This tests whether 2 samples are drawn from the same distribution. Note
that, like in the case of the one-sample KS test, the distribution is
assumed to be continuous.

In the one-sided test, the alternative is that the empirical
cumulative distribution function F(x) of the data1 variable is 'less'
or 'greater' than the empirical cumulative distribution function G(x)
of the data2 variable, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.

If the KS statistic is small or the p-value is high, then we cannot
reject the hypothesis that the distributions of the two samples
are the same.

If the mode is 'auto', the computation is exact if the sample sizes are
less than 10000.  For larger sizes, the computation uses the
Kolmogorov-Smirnov distributions to compute an approximate value.

We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.

References
----------
.. [1] Hodges, J.L. Jr.,  'The Significance Probability of the Smirnov
       Two-Sample Test,' Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.


Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)  #fix random seed to get the same result
>>> n1 = 200  # size of first sample
>>> n2 = 300  # size of second sample

For a different distribution, we can reject the null hypothesis since the
pvalue is below 1%:

>>> rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)
>>> rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)
>>> stats.ks_2samp(rvs1, rvs2)
(0.20833333333333334, 5.129279597781977e-05)

For a slightly different distribution, we cannot reject the null hypothesis
at a 10% or lower alpha since the p-value at 0.144 is higher than 10%

>>> rvs3 = stats.norm.rvs(size=n2, loc=0.01, scale=1.0)
>>> stats.ks_2samp(rvs1, rvs3)
(0.10333333333333333, 0.14691437867433876)

For an identical distribution, we cannot reject the null hypothesis since
the p-value is high, 41%:

>>> rvs4 = stats.norm.rvs(size=n2, loc=0.0, scale=1.0)
>>> stats.ks_2samp(rvs1, rvs4)
(0.07999999999999996, 0.41126949729859719)
*)

val ksone : ?loc:float -> ?scale:float -> n:Py.Object.t -> unit -> [`Ksone_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
General Kolmogorov-Smirnov one-sided test.

This is the distribution of the one-sided Kolmogorov-Smirnov (KS)
statistics :math:`D_n^+` and :math:`D_n^-`
for a finite sample size ``n`` (the shape parameter).

As an instance of the `rv_continuous` class, `ksone` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, n, loc=0, scale=1)
    Probability density function.
logpdf(x, n, loc=0, scale=1)
    Log of the probability density function.
cdf(x, n, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, n, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, n, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, n, loc=0, scale=1)
    Log of the survival function.
ppf(q, n, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, n, loc=0, scale=1)
    Non-central moment of order n
stats(n, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, n, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, loc=0, scale=1)
    Median of the distribution.
mean(n, loc=0, scale=1)
    Mean of the distribution.
var(n, loc=0, scale=1)
    Variance of the distribution.
std(n, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, n, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
:math:`D_n^+` and :math:`D_n^-` are given by

.. math::

    D_n^+ &= \text{sup}_x (F_n(x) - F(x)),\\
    D_n^- &= \text{sup}_x (F(x) - F_n(x)),\\

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `ksone`
describes the distribution under the null hypothesis of the KS test
that the empirical CDF corresponds to :math:`n` i.i.d. random variates
with CDF :math:`F`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ksone.pdf(x, n, loc, scale)`` is identically
equivalent to ``ksone.pdf(y, n) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
kstwobign, kstest

References
----------
.. [1] Birnbaum, Z. W. and Tingey, F.H. 'One-sided confidence contours
   for probability distribution functions', The Annals of Mathematical
   Statistics, 22(4), pp 592-596 (1951).

Examples
--------
>>> from scipy.stats import ksone
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n = 1e+03
>>> mean, var, skew, kurt = ksone.stats(n, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ksone.ppf(0.01, n),
...                 ksone.ppf(0.99, n), 100)
>>> ax.plot(x, ksone.pdf(x, n),
...        'r-', lw=5, alpha=0.6, label='ksone pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ksone(n)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ksone.ppf([0.001, 0.5, 0.999], n)
>>> np.allclose([0.001, 0.5, 0.999], ksone.cdf(vals, n))
True

Generate random numbers:

>>> r = ksone.rvs(n, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kstat : ?n:[`Three | `I of int | `Two | `PyObject of Py.Object.t] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Return the nth k-statistic (1<=n<=4 so far).

The nth k-statistic k_n is the unique symmetric unbiased estimator of the
nth cumulant kappa_n.

Parameters
----------
data : array_like
    Input array. Note that n-D input gets flattened.
n : int, {1, 2, 3, 4}, optional
    Default is equal to 2.

Returns
-------
kstat : float
    The nth k-statistic.

See Also
--------
kstatvar: Returns an unbiased estimator of the variance of the k-statistic.
moment: Returns the n-th central moment about the mean for a sample.

Notes
-----
For a sample size n, the first few k-statistics are given by:

.. math::

    k_{1} = \mu
    k_{2} = \frac{n}{n-1} m_{2}
    k_{3} = \frac{ n^{2} } {(n-1) (n-2)} m_{3}
    k_{4} = \frac{ n^{2} [(n + 1)m_{4} - 3(n - 1) m^2_{2}]} {(n-1) (n-2) (n-3)}

where :math:`\mu` is the sample mean, :math:`m_2` is the sample
variance, and :math:`m_i` is the i-th sample central moment.

References
----------
http://mathworld.wolfram.com/k-Statistic.html

http://mathworld.wolfram.com/Cumulant.html

Examples
--------
>>> from scipy import stats
>>> rndm = np.random.RandomState(1234)

As sample size increases, n-th moment and n-th k-statistic converge to the
same number (although they aren't identical). In the case of the normal
distribution, they converge to zero.

>>> for n in [2, 3, 4, 5, 6, 7]:
...     x = rndm.normal(size=10**n)
...     m, k = stats.moment(x, 3), stats.kstat(x, 3)
...     print('%.3g %.3g %.3g' % (m, k, m-k))
-0.631 -0.651 0.0194
0.0282 0.0283 -8.49e-05
-0.0454 -0.0454 1.36e-05
7.53e-05 7.53e-05 -2.26e-09
0.00166 0.00166 -4.99e-09
-2.88e-06 -2.88e-06 8.63e-13
*)

val kstatvar : ?n:[`I of int | `PyObject of Py.Object.t] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Return an unbiased estimator of the variance of the k-statistic.

See `kstat` for more details of the k-statistic.

Parameters
----------
data : array_like
    Input array. Note that n-D input gets flattened.
n : int, {1, 2}, optional
    Default is equal to 2.

Returns
-------
kstatvar : float
    The nth k-statistic variance.

See Also
--------
kstat: Returns the n-th k-statistic.
moment: Returns the n-th central moment about the mean for a sample.

Notes
-----
The variances of the first few k-statistics are given by:

.. math::

    var(k_{1}) = \frac{\kappa^2}{n}
    var(k_{2}) = \frac{\kappa^4}{n} + \frac{2\kappa^2_{2}}{n - 1}
    var(k_{3}) = \frac{\kappa^6}{n} + \frac{9 \kappa_2 \kappa_4}{n - 1} +
                 \frac{9 \kappa^2_{3}}{n - 1} +
                 \frac{6 n \kappa^3_{2}}{(n-1) (n-2)}
    var(k_{4}) = \frac{\kappa^8}{n} + \frac{16 \kappa_2 \kappa_6}{n - 1} +
                 \frac{48 \kappa_{3} \kappa_5}{n - 1} +
                 \frac{34 \kappa^2_{4}}{n-1} + \frac{72 n \kappa^2_{2} \kappa_4}{(n - 1) (n - 2)} +
                 \frac{144 n \kappa_{2} \kappa^2_{3}}{(n - 1) (n - 2)} +
                 \frac{24 (n + 1) n \kappa^4_{2}}{(n - 1) (n - 2) (n - 3)}
*)

val kstest : ?args:Py.Object.t -> ?n:int -> ?alternative:[`Two_sided | `Less | `Greater] -> ?mode:[`Approx | `Asymp] -> rvs:[`Ndarray of [>`Ndarray] Np.Obj.t | `Callable of Py.Object.t | `S of string] -> cdf:[`Callable of Py.Object.t | `S of string] -> unit -> (float * float)
(**
Perform the Kolmogorov-Smirnov test for goodness of fit.

This performs a test of the distribution F(x) of an observed
random variable against a given distribution G(x). Under the null
hypothesis, the two distributions are identical, F(x)=G(x). The
alternative hypothesis can be either 'two-sided' (default), 'less'
or 'greater'. The KS test is only valid for continuous distributions.

Parameters
----------
rvs : str, array_like, or callable
    If a string, it should be the name of a distribution in `scipy.stats`.
    If an array, it should be a 1-D array of observations of random
    variables.
    If a callable, it should be a function to generate random variables;
    it is required to have a keyword argument `size`.
cdf : str or callable
    If a string, it should be the name of a distribution in `scipy.stats`.
    If `rvs` is a string then `cdf` can be False or the same as `rvs`.
    If a callable, that callable is used to calculate the cdf.
args : tuple, sequence, optional
    Distribution parameters, used if `rvs` or `cdf` are strings.
N : int, optional
    Sample size if `rvs` is string or callable.  Default is 20.
alternative : {'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is 'two-sided'):

      * 'two-sided'
      * 'less': one-sided, see explanation in Notes
      * 'greater': one-sided, see explanation in Notes
mode : {'approx', 'asymp'}, optional
    Defines the distribution used for calculating the p-value.
    The following options are available (default is 'approx'):

      * 'approx': use approximation to exact distribution of test statistic
      * 'asymp': use asymptotic distribution of test statistic

Returns
-------
statistic : float
    KS test statistic, either D, D+ or D-.
pvalue :  float
    One-tailed or two-tailed p-value.

See Also
--------
ks_2samp

Notes
-----
In the one-sided test, the alternative is that the empirical
cumulative distribution function of the random variable is 'less'
or 'greater' than the cumulative distribution function G(x) of the
hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.

Examples
--------
>>> from scipy import stats

>>> x = np.linspace(-15, 15, 9)
>>> stats.kstest(x, 'norm')
(0.44435602715924361, 0.038850142705171065)

>>> np.random.seed(987654321) # set random seed to get the same result
>>> stats.kstest('norm', False, N=100)
(0.058352892479417884, 0.88531190944151261)

The above lines are equivalent to:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.norm.rvs(size=100), 'norm')
(0.058352892479417884, 0.88531190944151261)

*Test against one-sided alternative hypothesis*

Shift distribution to larger values, so that ``cdf_dgp(x) < norm.cdf(x)``:

>>> np.random.seed(987654321)
>>> x = stats.norm.rvs(loc=0.2, size=100)
>>> stats.kstest(x,'norm', alternative = 'less')
(0.12464329735846891, 0.040989164077641749)

Reject equal distribution against alternative hypothesis: less

>>> stats.kstest(x,'norm', alternative = 'greater')
(0.0072115233216311081, 0.98531158590396395)

Don't reject equal distribution against alternative hypothesis: greater

>>> stats.kstest(x,'norm', mode='asymp')
(0.12464329735846891, 0.08944488871182088)

*Testing t distributed random variables against normal distribution*

With 100 degrees of freedom the t distribution looks close to the normal
distribution, and the K-S test does not reject the hypothesis that the
sample came from the normal distribution:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.t.rvs(100,size=100),'norm')
(0.072018929165471257, 0.67630062862479168)

With 3 degrees of freedom the t distribution looks sufficiently different
from the normal distribution, that we can reject the hypothesis that the
sample came from the normal distribution at the 10% level:

>>> np.random.seed(987654321)
>>> stats.kstest(stats.t.rvs(3,size=100),'norm')
(0.131016895759829, 0.058826222555312224)
*)

val kstwobign : ?loc:float -> ?scale:float -> unit -> [`Kstwobign_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
Kolmogorov-Smirnov two-sided test for large N.

This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov
statistic :math:`\sqrt{n} D_n` that measures the maximum absolute
distance of the theoretical CDF from the empirical CDF (see `kstest`).

As an instance of the `rv_continuous` class, `kstwobign` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
:math:`\sqrt{n} D_n` is given by

.. math::

    D_n = \text{sup}_x |F_n(x) - F(x)|

where :math:`F` is a CDF and :math:`F_n` is an empirical CDF. `kstwobign`
describes the asymptotic distribution (i.e. the limit of
:math:`\sqrt{n} D_n`) under the null hypothesis of the KS test that the
empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``kstwobign.pdf(x, loc, scale)`` is identically
equivalent to ``kstwobign.pdf(y) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
ksone, kstest

References
----------
.. [1] Marsaglia, G. et al. 'Evaluating Kolmogorov's distribution',
   Journal of Statistical Software, 8(18), 2003.

Examples
--------
>>> from scipy.stats import kstwobign
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = kstwobign.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(kstwobign.ppf(0.01),
...                 kstwobign.ppf(0.99), 100)
>>> ax.plot(x, kstwobign.pdf(x),
...        'r-', lw=5, alpha=0.6, label='kstwobign pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = kstwobign()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = kstwobign.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], kstwobign.cdf(vals))
True

Generate random numbers:

>>> r = kstwobign.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val kurtosis : ?axis:[`I of int | `None] -> ?fisher:bool -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the kurtosis (Fisher or Pearson) of a dataset.

Kurtosis is the fourth central moment divided by the square of the
variance. If Fisher's definition is used, then 3.0 is subtracted from
the result to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics to
eliminate bias coming from biased moment estimators

Use `kurtosistest` to see if result is close enough to normal.

Parameters
----------
a : array
    Data for which the kurtosis is calculated.
axis : int or None, optional
    Axis along which the kurtosis is calculated. Default is 0.
    If None, compute over the whole array `a`.
fisher : bool, optional
    If True, Fisher's definition is used (normal ==> 0.0). If False,
    Pearson's definition is used (normal ==> 3.0).
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
kurtosis : array
    The kurtosis of values along an axis. If all values are equal,
    return -3 for Fisher's definition and 0 for Pearson's definition.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.

Examples
--------
In Fisher's definiton, the kurtosis of the normal distribution is zero.
In the following example, the kurtosis is close to zero, because it was
calculated from the dataset, not from the continuous distribution.

>>> from scipy.stats import norm, kurtosis
>>> data = norm.rvs(size=1000, random_state=3)
>>> kurtosis(data)
-0.06928694200380558

The distribution with a higher kurtosis has a heavier tail.
The zero valued kurtosis of the normal distribution in Fisher's definition
can serve as a reference point.

>>> import matplotlib.pyplot as plt
>>> import scipy.stats as stats
>>> from scipy.stats import kurtosis

>>> x = np.linspace(-5, 5, 100)
>>> ax = plt.subplot()
>>> distnames = ['laplace', 'norm', 'uniform']

>>> for distname in distnames:
...     if distname == 'uniform':
...         dist = getattr(stats, distname)(loc=-2, scale=4)
...     else:
...         dist = getattr(stats, distname)
...     data = dist.rvs(size=1000)
...     kur = kurtosis(data, fisher=True)
...     y = dist.pdf(x)
...     ax.plot(x, y, label='{}, {}'.format(distname, round(kur, 3)))
...     ax.legend()

The Laplace distribution has a heavier tail than the normal distribution.
The uniform distribution (which has negative kurtosis) has the thinnest
tail.
*)

val kurtosistest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Test whether a dataset has normal kurtosis.

This function tests the null hypothesis that the kurtosis
of the population from which the sample was drawn is that
of the normal distribution: ``kurtosis = 3(n-1)/(n+1)``.

Parameters
----------
a : array
    Array of the sample data.
axis : int or None, optional
   Axis along which to compute test. Default is 0. If None,
   compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    The two-sided p-value for the hypothesis test.

Notes
-----
Valid only for n>20. This function uses the method described in [1]_.

References
----------
.. [1] see e.g. F. J. Anscombe, W. J. Glynn, 'Distribution of the kurtosis
   statistic b2 for normal samples', Biometrika, vol. 70, pp. 227-234, 1983.

Examples
--------
>>> from scipy.stats import kurtosistest
>>> kurtosistest(list(range(20)))
KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.08804338332528348)

>>> np.random.seed(28041990)
>>> s = np.random.normal(0, 1, 1000)
>>> kurtosistest(s)
KurtosistestResult(statistic=1.2317590987707365, pvalue=0.21803908613450895)
*)

val laplace : ?loc:float -> ?scale:float -> unit -> [`Laplace_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Laplace continuous random variable.

As an instance of the `rv_continuous` class, `laplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `laplace` is

.. math::

    f(x) = \frac{1}{2} \exp(-|x|)

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``laplace.pdf(x, loc, scale)`` is identically
equivalent to ``laplace.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import laplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = laplace.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(laplace.ppf(0.01),
...                 laplace.ppf(0.99), 100)
>>> ax.plot(x, laplace.pdf(x),
...        'r-', lw=5, alpha=0.6, label='laplace pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = laplace()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = laplace.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], laplace.cdf(vals))
True

Generate random numbers:

>>> r = laplace.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levene : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float)
(**
Perform Levene test for equal variances.

The Levene test tests the null hypothesis that all input samples
are from populations with equal variances.  Levene's test is an
alternative to Bartlett's test `bartlett` in the case where
there are significant deviations from normality.

Parameters
----------
sample1, sample2, ... : array_like
    The sample data, possibly with different lengths. Only one-dimensional
    samples are accepted.
center : {'mean', 'median', 'trimmed'}, optional
    Which function of the data to use in the test.  The default
    is 'median'.
proportiontocut : float, optional
    When `center` is 'trimmed', this gives the proportion of data points
    to cut from each end. (See `scipy.stats.trim_mean`.)
    Default is 0.05.

Returns
-------
statistic : float
    The test statistic.
pvalue : float
    The p-value for the test.

Notes
-----
Three variations of Levene's test are possible.  The possibilities
and their recommended usages are:

  * 'median' : Recommended for skewed (non-normal) distributions>
  * 'mean' : Recommended for symmetric, moderate-tailed distributions.
  * 'trimmed' : Recommended for heavy-tailed distributions.

The test version using the mean was proposed in the original article
of Levene ([2]_) while the median and trimmed mean have been studied by
Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe
test.

References
----------
.. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm
.. [2]   Levene, H. (1960). In Contributions to Probability and Statistics:
           Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,
           Stanford University Press, pp. 278-292.
.. [3]  Brown, M. B. and Forsythe, A. B. (1974), Journal of the American
          Statistical Association, 69, 364-367
*)

val levy : ?loc:float -> ?scale:float -> unit -> [`Levy_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Levy continuous random variable.

As an instance of the `rv_continuous` class, `levy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy_stable, levy_l

Notes
-----
The probability density function for `levy` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp\left(-\frac{1}{2x}\right)

for :math:`x >= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy.pdf(x, loc, scale)`` is identically
equivalent to ``levy.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import levy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = levy.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy.ppf(0.01),
...                 levy.ppf(0.99), 100)
>>> ax.plot(x, levy.pdf(x),
...        'r-', lw=5, alpha=0.6, label='levy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], levy.cdf(vals))
True

Generate random numbers:

>>> r = levy.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levy_l : ?loc:float -> ?scale:float -> unit -> [`Levy_l_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A left-skewed Levy continuous random variable.

As an instance of the `rv_continuous` class, `levy_l` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy, levy_stable

Notes
-----
The probability density function for `levy_l` is:

.. math::
    f(x) = \frac{1}{ |x| \sqrt{2\pi |x| }} \exp{ \left(-\frac{1}{2|x| } \right)}

for :math:`x <= 0`.

This is the same as the Levy-stable distribution with :math:`a=1/2` and
:math:`b=-1`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy_l.pdf(x, loc, scale)`` is identically
equivalent to ``levy_l.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import levy_l
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = levy_l.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy_l.ppf(0.01),
...                 levy_l.ppf(0.99), 100)
>>> ax.plot(x, levy_l.pdf(x),
...        'r-', lw=5, alpha=0.6, label='levy_l pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy_l()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy_l.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], levy_l.cdf(vals))
True

Generate random numbers:

>>> r = levy_l.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val levy_stable : ?loc:float -> ?scale:float -> alpha:Py.Object.t -> beta:Py.Object.t -> unit -> [`Levy_stable_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Levy-stable continuous random variable.

As an instance of the `rv_continuous` class, `levy_stable` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(alpha, beta, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, alpha, beta, loc=0, scale=1)
    Probability density function.
logpdf(x, alpha, beta, loc=0, scale=1)
    Log of the probability density function.
cdf(x, alpha, beta, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, alpha, beta, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, alpha, beta, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, alpha, beta, loc=0, scale=1)
    Log of the survival function.
ppf(q, alpha, beta, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, alpha, beta, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, alpha, beta, loc=0, scale=1)
    Non-central moment of order n
stats(alpha, beta, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(alpha, beta, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, alpha, beta, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(alpha, beta), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(alpha, beta, loc=0, scale=1)
    Median of the distribution.
mean(alpha, beta, loc=0, scale=1)
    Mean of the distribution.
var(alpha, beta, loc=0, scale=1)
    Variance of the distribution.
std(alpha, beta, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, alpha, beta, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
levy, levy_l

Notes
-----
The distribution for `levy_stable` has characteristic function:

.. math::

    \varphi(t, \alpha, \beta, c, \mu) =
    e^{it\mu -|ct|^{\alpha}(1-i\beta \operatorname{sign}(t)\Phi(\alpha, t))}

where:

.. math::

    \Phi = \begin{cases}
            \tan \left({\frac {\pi \alpha }{2}}\right)&\alpha \neq 1\\
            -{\frac {2}{\pi }}\log |t|&\alpha =1
            \end{cases}

The probability density function for `levy_stable` is:

.. math::

    f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \varphi(t)e^{-ixt}\,dt

where :math:`-\infty < t < \infty`. This integral does not have a known closed form.

For evaluation of pdf we use either Zolotarev :math:`S_0` parameterization with integration,
direct integration of standard parameterization of characteristic function or FFT of
characteristic function. If set to other than None and if number of points is greater than
``levy_stable.pdf_fft_min_points_threshold`` (defaults to None) we use FFT otherwise we use one
of the other methods.

The default method is 'best' which uses Zolotarev's method if alpha = 1 and integration of
characteristic function otherwise. The default method can be changed by setting
``levy_stable.pdf_default_method`` to either 'zolotarev', 'quadrature' or 'best'.

To increase accuracy of FFT calculation one can specify ``levy_stable.pdf_fft_grid_spacing``
(defaults to 0.001) and ``pdf_fft_n_points_two_power`` (defaults to a value that covers the
input range * 4). Setting ``pdf_fft_n_points_two_power`` to 16 should be sufficiently accurate
in most cases at the expense of CPU time.

For evaluation of cdf we use Zolatarev :math:`S_0` parameterization with integration or integral of
the pdf FFT interpolated spline. The settings affecting FFT calculation are the same as
for pdf calculation. Setting the threshold to ``None`` (default) will disable FFT. For cdf
calculations the Zolatarev method is superior in accuracy, so FFT is disabled by default.

Fitting estimate uses quantile estimation method in [MC]. MLE estimation of parameters in
fit method uses this quantile estimate initially. Note that MLE doesn't always converge if
using FFT for pdf calculations; so it's best that ``pdf_fft_min_points_threshold`` is left unset.

.. warning::

    For pdf calculations implementation of Zolatarev is unstable for values where alpha = 1 and
    beta != 0. In this case the quadrature method is recommended. FFT calculation is also
    considered experimental.

    For cdf calculations FFT calculation is considered experimental. Use Zolatarev's method
    instead (default).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``levy_stable.pdf(x, alpha, beta, loc, scale)`` is identically
equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [MC] McCulloch, J., 1986. Simple consistent estimators of stable distribution parameters.
   Communications in Statistics - Simulation and Computation 15, 11091136.
.. [MS] Mittnik, S.T. Rachev, T. Doganoglu, D. Chenyao, 1999. Maximum likelihood estimation
   of stable Paretian models, Mathematical and Computer Modelling, Volume 29, Issue 10,
   1999, Pages 275-293.
.. [BS] Borak, S., Hardle, W., Rafal, W. 2005. Stable distributions, Economic Risk.

Examples
--------
>>> from scipy.stats import levy_stable
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> alpha, beta = 1.8, -0.5
>>> mean, var, skew, kurt = levy_stable.stats(alpha, beta, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(levy_stable.ppf(0.01, alpha, beta),
...                 levy_stable.ppf(0.99, alpha, beta), 100)
>>> ax.plot(x, levy_stable.pdf(x, alpha, beta),
...        'r-', lw=5, alpha=0.6, label='levy_stable pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = levy_stable(alpha, beta)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = levy_stable.ppf([0.001, 0.5, 0.999], alpha, beta)
>>> np.allclose([0.001, 0.5, 0.999], levy_stable.cdf(vals, alpha, beta))
True

Generate random numbers:

>>> r = levy_stable.rvs(alpha, beta, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val linregress : ?y:Py.Object.t -> x:Py.Object.t -> unit -> (float * float * float * float * float)
(**
Calculate a linear least-squares regression for two sets of measurements.

Parameters
----------
x, y : array_like
    Two sets of measurements.  Both arrays should have the same length.  If
    only `x` is given (and ``y=None``), then it must be a two-dimensional
    array where one dimension has length 2.  The two sets of measurements
    are then found by splitting the array along the length-2 dimension.  In
    the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is
    equivalent to ``linregress(x[0], x[1])``.

Returns
-------
slope : float
    Slope of the regression line.
intercept : float
    Intercept of the regression line.
rvalue : float
    Correlation coefficient.
pvalue : float
    Two-sided p-value for a hypothesis test whose null hypothesis is
    that the slope is zero, using Wald Test with t-distribution of
    the test statistic.
stderr : float
    Standard error of the estimated gradient.

See also
--------
:func:`scipy.optimize.curve_fit` : Use non-linear
 least squares to fit a function to data.
:func:`scipy.optimize.leastsq` : Minimize the sum of
 squares of a set of equations.

Notes
-----
Missing values are considered pair-wise: if a value is missing in `x`,
the corresponding value in `y` is masked.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats

Generate some data:

>>> np.random.seed(12345678)
>>> x = np.random.random(10)
>>> y = 1.6*x + np.random.random(10)

Perform the linear regression:

>>> slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
>>> print('slope: %f    intercept: %f' % (slope, intercept))
slope: 1.944864    intercept: 0.268578

To get coefficient of determination (R-squared):

>>> print('R-squared: %f' % r_value**2)
R-squared: 0.735498

Plot the data along with the fitted line:

>>> plt.plot(x, y, 'o', label='original data')
>>> plt.plot(x, intercept + slope*x, 'r', label='fitted line')
>>> plt.legend()
>>> plt.show()

Example for the case where only x is provided as a 2x2 array:

>>> x = np.array([[0, 1], [0, 2]])
>>> r = stats.linregress(x)
>>> r.slope, r.intercept
(2.0, 0.0)
*)

val loggamma : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Loggamma_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A log gamma continuous random variable.

As an instance of the `rv_continuous` class, `loggamma` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `loggamma` is:

.. math::

    f(x, c) = \frac{\exp(c x - \exp(x))}
                   {\Gamma(c)}

for all :math:`x, c > 0`. Here, :math:`\Gamma` is the
gamma function (`scipy.special.gamma`).

`loggamma` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loggamma.pdf(x, c, loc, scale)`` is identically
equivalent to ``loggamma.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import loggamma
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.414
>>> mean, var, skew, kurt = loggamma.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loggamma.ppf(0.01, c),
...                 loggamma.ppf(0.99, c), 100)
>>> ax.plot(x, loggamma.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='loggamma pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loggamma(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loggamma.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], loggamma.cdf(vals, c))
True

Generate random numbers:

>>> r = loggamma.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val logistic : ?loc:float -> ?scale:float -> unit -> [`Logistic_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A logistic (or Sech-squared) continuous random variable.

As an instance of the `rv_continuous` class, `logistic` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `logistic` is:

.. math::

    f(x) = \frac{\exp(-x)}
                {(1+\exp(-x))^2}

`logistic` is a special case of `genlogistic` with ``c=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``logistic.pdf(x, loc, scale)`` is identically
equivalent to ``logistic.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import logistic
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = logistic.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(logistic.ppf(0.01),
...                 logistic.ppf(0.99), 100)
>>> ax.plot(x, logistic.pdf(x),
...        'r-', lw=5, alpha=0.6, label='logistic pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = logistic()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = logistic.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], logistic.cdf(vals))
True

Generate random numbers:

>>> r = logistic.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val loglaplace : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Loglaplace_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A log-Laplace continuous random variable.

As an instance of the `rv_continuous` class, `loglaplace` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `loglaplace` is:

.. math::

    f(x, c) = \begin{cases}\frac{c}{2} x^{ c-1}  &\text{for } 0 < x < 1\\
                           \frac{c}{2} x^{-c-1}  &\text{for } x \ge 1
              \end{cases}

for :math:`c > 0`.

`loglaplace` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loglaplace.pdf(x, c, loc, scale)`` is identically
equivalent to ``loglaplace.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
T.J. Kozubowski and K. Podgorski, 'A log-Laplace growth rate model',
The Mathematical Scientist, vol. 28, pp. 49-60, 2003.

Examples
--------
>>> from scipy.stats import loglaplace
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 3.25
>>> mean, var, skew, kurt = loglaplace.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loglaplace.ppf(0.01, c),
...                 loglaplace.ppf(0.99, c), 100)
>>> ax.plot(x, loglaplace.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='loglaplace pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loglaplace(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loglaplace.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], loglaplace.cdf(vals, c))
True

Generate random numbers:

>>> r = loglaplace.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val lognorm : ?loc:float -> ?scale:float -> s:Py.Object.t -> unit -> [`Lognorm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A lognormal continuous random variable.

As an instance of the `rv_continuous` class, `lognorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, s, loc=0, scale=1)
    Probability density function.
logpdf(x, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, s, loc=0, scale=1)
    Non-central moment of order n
stats(s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(s,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(s, loc=0, scale=1)
    Median of the distribution.
mean(s, loc=0, scale=1)
    Mean of the distribution.
var(s, loc=0, scale=1)
    Variance of the distribution.
std(s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `lognorm` is:

.. math::

    f(x, s) = \frac{1}{s x \sqrt{2\pi}}
              \exp\left(-\frac{\log^2(x)}{2s^2}\right)

for :math:`x > 0`, :math:`s > 0`.

`lognorm` takes ``s`` as a shape parameter for :math:`s`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``lognorm.pdf(x, s, loc, scale)`` is identically
equivalent to ``lognorm.pdf(y, s) / scale`` with
``y = (x - loc) / scale``.

A common parametrization for a lognormal random variable ``Y`` is in
terms of the mean, ``mu``, and standard deviation, ``sigma``, of the
unique normally distributed random variable ``X`` such that exp(X) = Y.
This parametrization corresponds to setting ``s = sigma`` and ``scale =
exp(mu)``.

Examples
--------
>>> from scipy.stats import lognorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> s = 0.954
>>> mean, var, skew, kurt = lognorm.stats(s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(lognorm.ppf(0.01, s),
...                 lognorm.ppf(0.99, s), 100)
>>> ax.plot(x, lognorm.pdf(x, s),
...        'r-', lw=5, alpha=0.6, label='lognorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = lognorm(s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = lognorm.ppf([0.001, 0.5, 0.999], s)
>>> np.allclose([0.001, 0.5, 0.999], lognorm.cdf(vals, s))
True

Generate random numbers:

>>> r = lognorm.rvs(s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val logser : ?loc:float -> p:Py.Object.t -> unit -> [`Logser_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Logarithmic (Log-Series, Series) discrete random variable.

As an instance of the `rv_discrete` class, `logser` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, p, loc=0)
    Probability mass function.
logpmf(k, p, loc=0)
    Log of the probability mass function.
cdf(k, p, loc=0)
    Cumulative distribution function.
logcdf(k, p, loc=0)
    Log of the cumulative distribution function.
sf(k, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, p, loc=0)
    Log of the survival function.
ppf(q, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(p, loc=0)
    Median of the distribution.
mean(p, loc=0)
    Mean of the distribution.
var(p, loc=0)
    Variance of the distribution.
std(p, loc=0)
    Standard deviation of the distribution.
interval(alpha, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `logser` is:

.. math::

    f(k) = - \frac{p^k}{k \log(1-p)}

for :math:`k \ge 1`.

`logser` takes :math:`p` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``logser.pmf(k, p, loc)`` is identically
equivalent to ``logser.pmf(k - loc, p)``.

Examples
--------
>>> from scipy.stats import logser
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> p = 0.6
>>> mean, var, skew, kurt = logser.stats(p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(logser.ppf(0.01, p),
...               logser.ppf(0.99, p))
>>> ax.plot(x, logser.pmf(x, p), 'bo', ms=8, label='logser pmf')
>>> ax.vlines(x, 0, logser.pmf(x, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = logser(p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = logser.cdf(x, p)
>>> np.allclose(x, logser.ppf(prob, p))
True

Generate random numbers:

>>> r = logser.rvs(p, size=1000)
*)

val loguniform : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> Py.Object.t
(**
A loguniform or reciprocal continuous random variable.

As an instance of the `rv_continuous` class, `loguniform` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for this class is:

.. math::

    f(x, a, b) = \frac{1}{x \log(b/a)}

for :math:`a \le x \le b`, :math:`b > a > 0`. This class takes
:math:`a` and :math:`b` as shape parameters. The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``loguniform.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``loguniform.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import loguniform
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.01, 1
>>> mean, var, skew, kurt = loguniform.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(loguniform.ppf(0.01, a, b),
...                 loguniform.ppf(0.99, a, b), 100)
>>> ax.plot(x, loguniform.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='loguniform pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = loguniform(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = loguniform.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], loguniform.cdf(vals, a, b))
True

Generate random numbers:

>>> r = loguniform.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


This doesn't show the equal probability of ``0.01``, ``0.1`` and
``1``. This is best when the x-axis is log-scaled:

>>> import numpy as np
>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log10(r))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$10^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()

This random variable will be log-uniform regardless of the base chosen for
``a`` and ``b``. Let's specify with base ``2`` instead:

>>> rvs = loguniform(2**-2, 2**0).rvs(size=1000)

Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random
variable.  Here's the histogram:

>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log2(rvs))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$2^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()
*)

val lomax : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Lomax_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Lomax (Pareto of the second kind) continuous random variable.

As an instance of the `rv_continuous` class, `lomax` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `lomax` is:

.. math::

    f(x, c) = \frac{c}{(1+x)^{c+1}}

for :math:`x \ge 0`, :math:`c > 0`.

`lomax` takes ``c`` as a shape parameter for :math:`c`.

`lomax` is a special case of `pareto` with ``loc=-1.0``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``lomax.pdf(x, c, loc, scale)`` is identically
equivalent to ``lomax.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import lomax
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.88
>>> mean, var, skew, kurt = lomax.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(lomax.ppf(0.01, c),
...                 lomax.ppf(0.99, c), 100)
>>> ax.plot(x, lomax.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='lomax pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = lomax(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = lomax.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], lomax.cdf(vals, c))
True

Generate random numbers:

>>> r = lomax.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val mannwhitneyu : ?use_continuity:bool -> ?alternative:[`Two_sided | `Less | `Greater] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Mann-Whitney rank test on samples x and y.

Parameters
----------
x, y : array_like
    Array of samples, should be one-dimensional.
use_continuity : bool, optional
        Whether a continuity correction (1/2.) should be taken into
        account. Default is True.
alternative : {None, 'two-sided', 'less', 'greater'}, optional
    Defines the alternative hypothesis.
    The following options are available (default is None):

      * None: computes p-value half the size of the 'two-sided' p-value and
        a different U statistic. The default behavior is not the same as
        using 'less' or 'greater'; it only exists for backward compatibility
        and is deprecated.
      * 'two-sided'
      * 'less': one-sided
      * 'greater': one-sided

    Use of the None option is deprecated.

Returns
-------
statistic : float
    The Mann-Whitney U statistic, equal to min(U for x, U for y) if
    `alternative` is equal to None (deprecated; exists for backward
    compatibility), and U for y otherwise.
pvalue : float
    p-value assuming an asymptotic normal distribution. One-sided or
    two-sided, depending on the choice of `alternative`.

Notes
-----
Use only when the number of observation in each sample is > 20 and
you have 2 independent samples of ranks. Mann-Whitney U is
significant if the u-obtained is LESS THAN or equal to the critical
value of U.

This test corrects for ties and by default uses a continuity correction.

References
----------
.. [1] https://en.wikipedia.org/wiki/Mann-Whitney_U_test

.. [2] H.B. Mann and D.R. Whitney, 'On a Test of Whether one of Two Random
       Variables is Stochastically Larger than the Other,' The Annals of
       Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 1947.
*)

val matrix_normal : ?mean:[>`Ndarray] Np.Obj.t -> ?rowcov:[>`Ndarray] Np.Obj.t -> ?colcov:[>`Ndarray] Np.Obj.t -> ?seed:Py.Object.t -> unit -> Py.Object.t
(**
A matrix normal random variable.

The `mean` keyword specifies the mean. The `rowcov` keyword specifies the
among-row covariance matrix. The 'colcov' keyword specifies the
among-column covariance matrix.

Methods
-------
``pdf(X, mean=None, rowcov=1, colcov=1)``
    Probability density function.
``logpdf(X, mean=None, rowcov=1, colcov=1)``
    Log of the probability density function.
``rvs(mean=None, rowcov=1, colcov=1, size=1, random_state=None)``
    Draw random samples.

Parameters
----------
X : array_like
    Quantiles, with the last two axes of `X` denoting the components.
mean : array_like, optional
    Mean of the distribution (default: `None`)
rowcov : array_like, optional
    Among-row covariance matrix of the distribution (default: `1`)
colcov : array_like, optional
    Among-column covariance matrix of the distribution (default: `1`)
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Alternatively, the object may be called (as a function) to fix the mean
and covariance parameters, returning a 'frozen' matrix normal
random variable:

rv = matrix_normal(mean=None, rowcov=1, colcov=1)
    - Frozen object with the same methods but holding the given
      mean and covariance fixed.

Notes
-----
If `mean` is set to `None` then a matrix of zeros is used for the mean.
    The dimensions of this matrix are inferred from the shape of `rowcov` and
    `colcov`, if these are provided, or set to `1` if ambiguous.

    `rowcov` and `colcov` can be two-dimensional array_likes specifying the
    covariance matrices directly. Alternatively, a one-dimensional array will
    be be interpreted as the entries of a diagonal matrix, and a scalar or
    zero-dimensional array will be interpreted as this value times the
    identity matrix.
    

The covariance matrices specified by `rowcov` and `colcov` must be
(symmetric) positive definite. If the samples in `X` are
:math:`m \times n`, then `rowcov` must be :math:`m \times m` and
`colcov` must be :math:`n \times n`. `mean` must be the same shape as `X`.

The probability density function for `matrix_normal` is

.. math::

    f(X) = (2 \pi)^{-\frac{mn}{2}}|U|^{-\frac{n}{2}} |V|^{-\frac{m}{2}}
           \exp\left( -\frac{1}{2} \mathrm{Tr}\left[ U^{-1} (X-M) V^{-1}
           (X-M)^T \right] \right),

where :math:`M` is the mean, :math:`U` the among-row covariance matrix,
:math:`V` the among-column covariance matrix.

The `allow_singular` behaviour of the `multivariate_normal`
distribution is not currently supported. Covariance matrices must be
full rank.

The `matrix_normal` distribution is closely related to the
`multivariate_normal` distribution. Specifically, :math:`\mathrm{Vec}(X)`
(the vector formed by concatenating the columns  of :math:`X`) has a
multivariate normal distribution with mean :math:`\mathrm{Vec}(M)`
and covariance :math:`V \otimes U` (where :math:`\otimes` is the Kronecker
product). Sampling and pdf evaluation are
:math:`\mathcal{O}(m^3 + n^3 + m^2 n + m n^2)` for the matrix normal, but
:math:`\mathcal{O}(m^3 n^3)` for the equivalent multivariate normal,
making this equivalent form algorithmically inefficient.

.. versionadded:: 0.17.0

Examples
--------

>>> from scipy.stats import matrix_normal

>>> M = np.arange(6).reshape(3,2); M
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> U = np.diag([1,2,3]); U
array([[1, 0, 0],
       [0, 2, 0],
       [0, 0, 3]])
>>> V = 0.3*np.identity(2); V
array([[ 0.3,  0. ],
       [ 0. ,  0.3]])
>>> X = M + 0.1; X
array([[ 0.1,  1.1],
       [ 2.1,  3.1],
       [ 4.1,  5.1]])
>>> matrix_normal.pdf(X, mean=M, rowcov=U, colcov=V)
0.023410202050005054

>>> # Equivalent multivariate normal
>>> from scipy.stats import multivariate_normal
>>> vectorised_X = X.T.flatten()
>>> equiv_mean = M.T.flatten()
>>> equiv_cov = np.kron(V,U)
>>> multivariate_normal.pdf(vectorised_X, mean=equiv_mean, cov=equiv_cov)
0.023410202050005054
*)

val maxwell : ?loc:float -> ?scale:float -> unit -> [`Maxwell_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Maxwell continuous random variable.

As an instance of the `rv_continuous` class, `maxwell` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,
and given ``scale = a``, where ``a`` is the parameter used in the
Mathworld description [1]_.

The probability density function for `maxwell` is:

.. math::

    f(x) = \sqrt{2/\pi}x^2 \exp(-x^2/2)

for :math:`x >= 0`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``maxwell.pdf(x, loc, scale)`` is identically
equivalent to ``maxwell.pdf(y) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] http://mathworld.wolfram.com/MaxwellDistribution.html

Examples
--------
>>> from scipy.stats import maxwell
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = maxwell.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(maxwell.ppf(0.01),
...                 maxwell.ppf(0.99), 100)
>>> ax.plot(x, maxwell.pdf(x),
...        'r-', lw=5, alpha=0.6, label='maxwell pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = maxwell()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = maxwell.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], maxwell.cdf(vals))
True

Generate random numbers:

>>> r = maxwell.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val median_absolute_deviation : ?axis:[`I of int | `None] -> ?center:Py.Object.t -> ?scale:float -> ?nan_policy:[`Propagate | `Raise | `Omit] -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the median absolute deviation of the data along the given axis.

The median absolute deviation (MAD, [1]_) computes the median over the
absolute deviations from the median. It is a measure of dispersion
similar to the standard deviation but more robust to outliers [2]_.

The MAD of an empty array is ``np.nan``.

.. versionadded:: 1.3.0

Parameters
----------
x : array_like
    Input array or object that can be converted to an array.
axis : int or None, optional
    Axis along which the range is computed. Default is 0. If None, compute
    the MAD over the entire array.
center : callable, optional
    A function that will return the central value. The default is to use
    np.median. Any user defined function used will need to have the function
    signature ``func(arr, axis)``.
scale : int, optional
    The scaling factor applied to the MAD. The default scale (1.4826)
    ensures consistency with the standard deviation for normally distributed
    data.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
mad : scalar or ndarray
    If ``axis=None``, a scalar is returned. If the input contains
    integers or floats of smaller precision than ``np.float64``, then the
    output data-type is ``np.float64``. Otherwise, the output data-type is
    the same as that of the input.

See Also
--------
numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,
scipy.stats.tstd, scipy.stats.tvar

Notes
-----
The `center` argument only affects the calculation of the central value
around which the MAD is calculated. That is, passing in ``center=np.mean``
will calculate the MAD around the mean - it will not calculate the *mean*
absolute deviation.

References
----------
.. [1] 'Median absolute deviation' https://en.wikipedia.org/wiki/Median_absolute_deviation
.. [2] 'Robust measures of scale' https://en.wikipedia.org/wiki/Robust_measures_of_scale

Examples
--------
When comparing the behavior of `median_absolute_deviation` with ``np.std``,
the latter is affected when we change a single value of an array to have an
outlier value while the MAD hardly changes:

>>> from scipy import stats
>>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)
>>> x.std()
0.9973906394005013
>>> stats.median_absolute_deviation(x)
1.2280762773108278
>>> x[0] = 345.6
>>> x.std()
34.42304872314415
>>> stats.median_absolute_deviation(x)
1.2340335571164334

Axis handling example:

>>> x = np.array([[10, 7, 4], [3, 2, 1]])
>>> x
array([[10,  7,  4],
       [ 3,  2,  1]])
>>> stats.median_absolute_deviation(x)
array([5.1891, 3.7065, 2.2239])
>>> stats.median_absolute_deviation(x, axis=None)
2.9652
*)

val median_test : ?kwds:(string * Py.Object.t) list -> Py.Object.t list -> (float * float * float * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Perform a Mood's median test.

Test that two or more samples come from populations with the same median.

Let ``n = len(args)`` be the number of samples.  The 'grand median' of
all the data is computed, and a contingency table is formed by
classifying the values in each sample as being above or below the grand
median.  The contingency table, along with `correction` and `lambda_`,
are passed to `scipy.stats.chi2_contingency` to compute the test statistic
and p-value.

Parameters
----------
sample1, sample2, ... : array_like
    The set of samples.  There must be at least two samples.
    Each sample must be a one-dimensional sequence containing at least
    one value.  The samples are not required to have the same length.
ties : str, optional
    Determines how values equal to the grand median are classified in
    the contingency table.  The string must be one of::

        'below':
            Values equal to the grand median are counted as 'below'.
        'above':
            Values equal to the grand median are counted as 'above'.
        'ignore':
            Values equal to the grand median are not counted.

    The default is 'below'.
correction : bool, optional
    If True, *and* there are just two samples, apply Yates' correction
    for continuity when computing the test statistic associated with
    the contingency table.  Default is True.
lambda_ : float or str, optional
    By default, the statistic computed in this test is Pearson's
    chi-squared statistic.  `lambda_` allows a statistic from the
    Cressie-Read power divergence family to be used instead.  See
    `power_divergence` for details.
    Default is 1 (Pearson's chi-squared statistic).
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
stat : float
    The test statistic.  The statistic that is returned is determined by
    `lambda_`.  The default is Pearson's chi-squared statistic.
p : float
    The p-value of the test.
m : float
    The grand median.
table : ndarray
    The contingency table.  The shape of the table is (2, n), where
    n is the number of samples.  The first row holds the counts of the
    values above the grand median, and the second row holds the counts
    of the values below the grand median.  The table allows further
    analysis with, for example, `scipy.stats.chi2_contingency`, or with
    `scipy.stats.fisher_exact` if there are two samples, without having
    to recompute the table.  If ``nan_policy`` is 'propagate' and there
    are nans in the input, the return value for ``table`` is ``None``.

See Also
--------
kruskal : Compute the Kruskal-Wallis H-test for independent samples.
mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.

Notes
-----
.. versionadded:: 0.15.0

References
----------
.. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill
    (1950), pp. 394-399.
.. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).
    See Sections 8.12 and 10.15.

Examples
--------
A biologist runs an experiment in which there are three groups of plants.
Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.
Each plant produces a number of seeds.  The seed counts for each group
are::

    Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49
    Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99
    Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84

The following code applies Mood's median test to these samples.

>>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]
>>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]
>>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]
>>> from scipy.stats import median_test
>>> stat, p, med, tbl = median_test(g1, g2, g3)

The median is

>>> med
34.0

and the contingency table is

>>> tbl
array([[ 5, 10,  7],
       [11,  5, 10]])

`p` is too large to conclude that the medians are not the same:

>>> p
0.12609082774093244

The 'G-test' can be performed by passing ``lambda_='log-likelihood'`` to
`median_test`.

>>> g, p, med, tbl = median_test(g1, g2, g3, lambda_='log-likelihood')
>>> p
0.12224779737117837

The median occurs several times in the data, so we'll get a different
result if, for example, ``ties='above'`` is used:

>>> stat, p, med, tbl = median_test(g1, g2, g3, ties='above')
>>> p
0.063873276069553273

>>> tbl
array([[ 5, 11,  9],
       [11,  4,  8]])

This example demonstrates that if the data set is not large and there
are values equal to the median, the p-value can be sensitive to the
choice of `ties`.
*)

val mielke : ?loc:float -> ?scale:float -> k:Py.Object.t -> s:Py.Object.t -> unit -> [`Mielke_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Mielke Beta-Kappa / Dagum continuous random variable.

As an instance of the `rv_continuous` class, `mielke` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(k, s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, k, s, loc=0, scale=1)
    Probability density function.
logpdf(x, k, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, k, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, k, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, k, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, k, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, k, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, k, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, k, s, loc=0, scale=1)
    Non-central moment of order n
stats(k, s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(k, s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, k, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(k, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(k, s, loc=0, scale=1)
    Median of the distribution.
mean(k, s, loc=0, scale=1)
    Mean of the distribution.
var(k, s, loc=0, scale=1)
    Variance of the distribution.
std(k, s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, k, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `mielke` is:

.. math::

    f(x, k, s) = \frac{k x^{k-1}}{(1+x^s)^{1+k/s}}

for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes
called Dagum distribution ([2]_). It was already defined in [3]_, called
a Burr Type III distribution (`burr` with parameters ``c=s`` and
``d=k/s``).

`mielke` takes ``k`` and ``s`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``mielke.pdf(x, k, s, loc, scale)`` is identically
equivalent to ``mielke.pdf(y, k, s) / scale`` with
``y = (x - loc) / scale``.

References
----------
.. [1] Mielke, P.W., 1973 'Another Family of Distributions for Describing
       and Analyzing Precipitation Data.' J. Appl. Meteor., 12, 275-280
.. [2] Dagum, C., 1977 'A new model for personal income distribution.'
       Economie Appliquee, 33, 327-367.
.. [3] Burr, I. W. 'Cumulative frequency functions', Annals of
       Mathematical Statistics, 13(2), pp 215-232 (1942).

Examples
--------
>>> from scipy.stats import mielke
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> k, s = 10.4, 4.6
>>> mean, var, skew, kurt = mielke.stats(k, s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(mielke.ppf(0.01, k, s),
...                 mielke.ppf(0.99, k, s), 100)
>>> ax.plot(x, mielke.pdf(x, k, s),
...        'r-', lw=5, alpha=0.6, label='mielke pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = mielke(k, s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = mielke.ppf([0.001, 0.5, 0.999], k, s)
>>> np.allclose([0.001, 0.5, 0.999], mielke.cdf(vals, k, s))
True

Generate random numbers:

>>> r = mielke.rvs(k, s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val mode : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Return an array of the modal (most common) value in the passed array.

If there is more than one such value, only the smallest is returned.
The bin-count for the modal bins is also returned.

Parameters
----------
a : array_like
    n-dimensional array of which to find mode(s).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
mode : ndarray
    Array of modal values.
count : ndarray
    Array of counts for each mode.

Examples
--------
>>> a = np.array([[6, 8, 3, 0],
...               [3, 2, 1, 7],
...               [8, 1, 8, 4],
...               [5, 3, 0, 5],
...               [4, 7, 5, 9]])
>>> from scipy import stats
>>> stats.mode(a)
(array([[3, 1, 0, 0]]), array([[1, 1, 1, 1]]))

To get mode of whole array, specify ``axis=None``:

>>> stats.mode(a, axis=None)
(array([3]), array([3]))
*)

val moment : ?moment:[`I of int | `Array_like_of_ints of Py.Object.t] -> ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the nth moment about the mean for a sample.

A moment is a specific quantitative measure of the shape of a set of
points. It is often used to calculate coefficients of skewness and kurtosis
due to its close relationship with them.

Parameters
----------
a : array_like
   Input array.
moment : int or array_like of ints, optional
   Order of central moment that is returned. Default is 1.
axis : int or None, optional
   Axis along which the central moment is computed. Default is 0.
   If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
n-th central moment : ndarray or float
   The appropriate moment along the given axis or over all values if axis
   is None. The denominator for the moment calculation is the number of
   observations, no degrees of freedom correction is done.

See Also
--------
kurtosis, skew, describe

Notes
-----
The k-th central moment of a data sample is:

.. math::

    m_k = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x})^k

Where n is the number of samples and x-bar is the mean. This function uses
exponentiation by squares [1]_ for efficiency.

References
----------
.. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms

Examples
--------
>>> from scipy.stats import moment
>>> moment([1, 2, 3, 4, 5], moment=1)
0.0
>>> moment([1, 2, 3, 4, 5], moment=2)
2.0
*)

val mood : ?axis:int -> x:Py.Object.t -> y:Py.Object.t -> unit -> Py.Object.t
(**
Perform Mood's test for equal scale parameters.

Mood's two-sample test for scale parameters is a non-parametric
test for the null hypothesis that two samples are drawn from the
same distribution with the same scale parameter.

Parameters
----------
x, y : array_like
    Arrays of sample data.
axis : int, optional
    The axis along which the samples are tested.  `x` and `y` can be of
    different length along `axis`.
    If `axis` is None, `x` and `y` are flattened and the test is done on
    all values in the flattened arrays.

Returns
-------
z : scalar or ndarray
    The z-score for the hypothesis test.  For 1-D inputs a scalar is
    returned.
p-value : scalar ndarray
    The p-value for the hypothesis test.

See Also
--------
fligner : A non-parametric test for the equality of k variances
ansari : A non-parametric test for the equality of 2 variances
bartlett : A parametric test for equality of k variances in normal samples
levene : A parametric test for equality of k variances

Notes
-----
The data are assumed to be drawn from probability distributions ``f(x)``
and ``f(x/s) / s`` respectively, for some probability density function f.
The null hypothesis is that ``s == 1``.

For multi-dimensional arrays, if the inputs are of shapes
``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the
resulting z and p values will have shape ``(n0, n2, n3)``.  Note that
``n1`` and ``m1`` don't have to be equal, but the other dimensions do.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(1234)
>>> x2 = np.random.randn(2, 45, 6, 7)
>>> x1 = np.random.randn(2, 30, 6, 7)
>>> z, p = stats.mood(x1, x2, axis=1)
>>> p.shape
(2, 6, 7)

Find the number of points where the difference in scale is not significant:

>>> (p > 0.1).sum()
74

Perform the test with different scales:

>>> x1 = np.random.randn(2, 30)
>>> x2 = np.random.randn(2, 35) * 10.0
>>> stats.mood(x1, x2, axis=1)
(array([-5.7178125 , -5.25342163]), array([  1.07904114e-08,   1.49299218e-07]))
*)

val moyal : ?loc:float -> ?scale:float -> unit -> [`Moyal_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Moyal continuous random variable.

As an instance of the `rv_continuous` class, `moyal` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `moyal` is:

.. math::

    f(x) = \exp(-(x + \exp(-x))/2) / \sqrt{2\pi}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``moyal.pdf(x, loc, scale)`` is identically
equivalent to ``moyal.pdf(y) / scale`` with
``y = (x - loc) / scale``.

This distribution has utility in high-energy physics and radiation
detection. It describes the energy loss of a charged relativistic
particle due to ionization of the medium [1]_. It also provides an
approximation for the Landau distribution. For an in depth description
see [2]_. For additional description, see [3]_.

References
----------
.. [1] J.E. Moyal, 'XXX. Theory of ionization fluctuations',
       The London, Edinburgh, and Dublin Philosophical Magazine
       and Journal of Science, vol 46, 263-280, (1955).
       :doi:`10.1080/14786440308521076` (gated)
.. [2] G. Cordeiro et al., 'The beta Moyal: a useful skew distribution',
       International Journal of Research and Reviews in Applied Sciences,
       vol 10, 171-192, (2012).
       http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf
.. [3] C. Walck, 'Handbook on Statistical Distributions for
       Experimentalists; International Report SUF-PFY/96-01', Chapter 26,
       University of Stockholm: Stockholm, Sweden, (2007).
       http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf

.. versionadded:: 1.1.0

Examples
--------
>>> from scipy.stats import moyal
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = moyal.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(moyal.ppf(0.01),
...                 moyal.ppf(0.99), 100)
>>> ax.plot(x, moyal.pdf(x),
...        'r-', lw=5, alpha=0.6, label='moyal pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = moyal()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = moyal.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], moyal.cdf(vals))
True

Generate random numbers:

>>> r = moyal.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val multinomial : ?seed:Py.Object.t -> n:int -> p:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
A multinomial random variable.

Methods
-------
``pmf(x, n, p)``
    Probability mass function.
``logpmf(x, n, p)``
    Log of the probability mass function.
``rvs(n, p, size=1, random_state=None)``
    Draw random samples from a multinomial distribution.
``entropy(n, p)``
    Compute the entropy of the multinomial distribution.
``cov(n, p)``
    Compute the covariance matrix of the multinomial distribution.

Parameters
----------
x : array_like
    Quantiles, with the last axis of `x` denoting the components.
n : int
    Number of trials
p : array_like
    Probability of a trial falling into each category; should sum to 1
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Notes
-----
`n` should be a positive integer. Each element of `p` should be in the
interval :math:`[0,1]` and the elements should sum to 1. If they do not sum to
1, the last element of the `p` array is not used and is replaced with the
remaining probability left over from the earlier elements.

Alternatively, the object may be called (as a function) to fix the `n` and
`p` parameters, returning a 'frozen' multinomial random variable:

The probability mass function for `multinomial` is

.. math::

    f(x) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k},

supported on :math:`x=(x_1, \ldots, x_k)` where each :math:`x_i` is a
nonnegative integer and their sum is :math:`n`.

.. versionadded:: 0.19.0

Examples
--------

>>> from scipy.stats import multinomial
>>> rv = multinomial(8, [0.3, 0.2, 0.5])
>>> rv.pmf([1, 3, 4])
0.042000000000000072

The multinomial distribution for :math:`k=2` is identical to the
corresponding binomial distribution (tiny numerical differences
notwithstanding):

>>> from scipy.stats import binom
>>> multinomial.pmf([3, 4], n=7, p=[0.4, 0.6])
0.29030399999999973
>>> binom.pmf(3, 7, 0.4)
0.29030400000000012

The functions ``pmf``, ``logpmf``, ``entropy``, and ``cov`` support
broadcasting, under the convention that the vector parameters (``x`` and
``p``) are interpreted as if each row along the last axis is a single
object. For instance:

>>> multinomial.pmf([[3, 4], [3, 5]], n=[7, 8], p=[.3, .7])
array([0.2268945,  0.25412184])

Here, ``x.shape == (2, 2)``, ``n.shape == (2,)``, and ``p.shape == (2,)``,
but following the rules mentioned above they behave as if the rows
``[3, 4]`` and ``[3, 5]`` in ``x`` and ``[.3, .7]`` in ``p`` were a single
object, and as if we had ``x.shape = (2,)``, ``n.shape = (2,)``, and
``p.shape = ()``. To obtain the individual elements without broadcasting,
we would do this:

>>> multinomial.pmf([3, 4], n=7, p=[.3, .7])
0.2268945
>>> multinomial.pmf([3, 5], 8, p=[.3, .7])
0.25412184

This broadcasting also works for ``cov``, where the output objects are
square matrices of size ``p.shape[-1]``. For example:

>>> multinomial.cov([4, 5], [[.3, .7], [.4, .6]])
array([[[ 0.84, -0.84],
        [-0.84,  0.84]],
       [[ 1.2 , -1.2 ],
        [-1.2 ,  1.2 ]]])

In this example, ``n.shape == (2,)`` and ``p.shape == (2, 2)``, and
following the rules above, these broadcast as if ``p.shape == (2,)``.
Thus the result should also be of shape ``(2,)``, but since each output is
a :math:`2 \times 2` matrix, the result in fact has shape ``(2, 2, 2)``,
where ``result[0]`` is equal to ``multinomial.cov(n=4, p=[.3, .7])`` and
``result[1]`` is equal to ``multinomial.cov(n=5, p=[.4, .6])``.

See also
--------
scipy.stats.binom : The binomial distribution.
numpy.random.Generator.multinomial : Sampling from the multinomial distribution.
*)

val multiscale_graphcorr : ?compute_distance:Py.Object.t -> ?reps:int -> ?workers:[`Map_like_callable of Py.Object.t | `I of int] -> ?is_twosamp:bool -> ?random_state:[`I of int | `Np_random_RandomState_instance of Py.Object.t] -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float * Py.Object.t)
(**
Computes the Multiscale Graph Correlation (MGC) test statistic.

Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for
one property (e.g. cloud density), and the :math:`l`-nearest neighbors for
the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is
called the 'scale'. A priori, however, it is not know which scales will be
most informative. So, MGC computes all distance pairs, and then efficiently
computes the distance correlations for all scales. The local correlations
illustrate which scales are relatively informative about the relationship.
The key, therefore, to successfully discover and decipher relationships
between disparate data modalities is to adaptively determine which scales
are the most informative, and the geometric implication for the most
informative scales. Doing so not only provides an estimate of whether the
modalities are related, but also provides insight into how the
determination was made. This is especially important in high-dimensional
data, where simple visualizations do not reveal relationships to the
unaided human eye. Characterizations of this implementation in particular
have been derived from and benchmarked within in [2]_.

Parameters
----------
x, y : ndarray
    If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is
    the number of samples and `p` and `q` are the number of dimensions,
    then the MGC independence test will be run.  Alternatively, ``x`` and
    ``y`` can have shapes ``(n, n)`` if they are distance or similarity
    matrices, and ``compute_distance`` must be sent to ``None``. If ``x``
    and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired
    two-sample MGC test will be run.
compute_distance : callable, optional
    A function that computes the distance or similarity among the samples
    within each data matrix. Set to ``None`` if ``x`` and ``y`` are
    already distance matrices. The default uses the euclidean norm metric.
    If you are calling a custom function, either create the distance
    matrix before-hand or create a function of the form
    ``compute_distance(x)`` where `x` is the data matrix for which
    pairwise distances are calculated.
reps : int, optional
    The number of replications used to estimate the null when using the
    permutation test. The default is ``1000``.
workers : int or map-like callable, optional
    If ``workers`` is an int the population is subdivided into ``workers``
    sections and evaluated in parallel (uses ``multiprocessing.Pool
    <multiprocessing>``). Supply ``-1`` to use all cores available to the
    Process. Alternatively supply a map-like callable, such as
    ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.
    This evaluation is carried out as ``workers(func, iterable)``.
    Requires that `func` be pickleable. The default is ``1``.
is_twosamp : bool, optional
    If `True`, a two sample test will be run. If ``x`` and ``y`` have
    shapes ``(n, p)`` and ``(m, p)``, this optional will be overriden and
    set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes
    ``(n, p)`` and a two sample test is desired. The default is ``False``.
random_state : int or np.random.RandomState instance, optional
    If already a RandomState instance, use it.
    If seed is an int, return a new RandomState instance seeded with seed.
    If None, use np.random.RandomState. Default is None.

Returns
-------
stat : float
    The sample MGC test statistic within `[-1, 1]`.
pvalue : float
    The p-value obtained via permutation.
mgc_dict : dict
    Contains additional useful additional returns containing the following
    keys:

        - mgc_map : ndarray
            A 2D representation of the latent geometry of the relationship.
            of the relationship.
        - opt_scale : (int, int)
            The estimated optimal scale as a `(x, y)` pair.
        - null_dist : list
            The null distribution derived from the permuted matrices

See Also
--------
pearsonr : Pearson correlation coefficient and p-value for testing
           non-correlation.
kendalltau : Calculates Kendall's tau.
spearmanr : Calculates a Spearman rank-order correlation coefficient.

Notes
-----
A description of the process of MGC and applications on neuroscience data
can be found in [1]_. It is performed using the following steps:

#. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and
   modified to be mean zero columnwise. This results in two
   :math:`n \times n` distance matrices :math:`A` and :math:`B` (the
   centering and unbiased modification) [3]_.

#. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,

   * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs
     are calculated for each property. Here, :math:`G_k (i, j)` indicates
     the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`
     and :math:`H_l (i, j)` indicates the :math:`l` smallested values of
     the :math:`i`-th row of :math:`B`

   * Let :math:`\circ` denotes the entry-wise matrix product, then local
     correlations are summed and normalized using the following statistic:

.. math::

    c^{kl} = \frac{\sum_{ij} A G_k B H_l}
                  {\sqrt{\sum_{ij} A^2 G_k \times \sum_{ij} B^2 H_l}}

#. The MGC test statistic is the smoothed optimal local correlation of
   :math:`\{ c^{kl} \}`. Denote the smoothing operation as :math:`R(\cdot)`
   (which essentially set all isolated large correlations) as 0 and
   connected large correlations the same as before, see [3]_.) MGC is,

.. math::

    MGC_n (x, y) = \max_{(k, l)} R \left(c^{kl} \left( x_n, y_n \right)
                                                \right)

The test statistic returns a value between :math:`(-1, 1)` since it is
normalized.

The p-value returned is calculated using a permutation test. This process
is completed by first randomly permuting :math:`y` to estimate the null
distribution and then calculating the probability of observing a test
statistic, under the null, at least as extreme as the observed test
statistic.

MGC requires at least 5 samples to run with reliable results. It can also
handle high-dimensional data sets.

In addition, by manipulating the input data matrices, the two-sample
testing problem can be reduced to the independence testing problem [4]_.
Given sample data :math:`U` and :math:`V` of sizes :math:`p \times n`
:math:`p \times m`, data matrix :math:`X` and :math:`Y` can be created as
follows:

.. math::

    X = [U | V] \in \mathcal{R}^{p \times (n + m)}

    Y = [0_{1 \times n} | 1_{1 \times m}] \in \mathcal{R}^{(n + m)}

Then, the MGC statistic can be calculated as normal. This methodology can
be extended to similar tests such as distance correlation [4]_.

.. versionadded:: 1.4.0

References
----------
.. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,
       Maggioni, M., & Shen, C. (2019). Discovering and deciphering
       relationships across disparate data modalities. ELife.
.. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,
       Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).
       mgcpy: A Comprehensive High Dimensional Independence Testing Python
       Package. ArXiv:1907.02088 [Cs, Stat].
.. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance
       correlation to multiscale graph correlation. Journal of the American
       Statistical Association.
.. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of
       Distance and Kernel Methods for Hypothesis Testing. ArXiv:1806.05514
       [Cs, Stat].

Examples
--------
>>> from scipy.stats import multiscale_graphcorr
>>> x = np.arange(100)
>>> y = x
>>> stat, pvalue, _ = multiscale_graphcorr(x, y, workers=-1)
>>> '%.1f, %.3f' % (stat, pvalue)
'1.0, 0.001'

Alternatively,

>>> x = np.arange(100)
>>> y = x
>>> mgc = multiscale_graphcorr(x, y)
>>> '%.1f, %.3f' % (mgc.stat, mgc.pvalue)
'1.0, 0.001'

To run an unpaired two-sample test,

>>> x = np.arange(100)
>>> y = np.arange(79)
>>> mgc = multiscale_graphcorr(x, y, random_state=1)
>>> '%.3f, %.2f' % (mgc.stat, mgc.pvalue)
'0.033, 0.02'

or, if shape of the inputs are the same,

>>> x = np.arange(100)
>>> y = x
>>> mgc = multiscale_graphcorr(x, y, is_twosamp=True)
>>> '%.3f, %.1f' % (mgc.stat, mgc.pvalue)
'-0.008, 1.0'
*)

val multivariate_normal : ?mean:[>`Ndarray] Np.Obj.t -> ?cov:[>`Ndarray] Np.Obj.t -> ?allow_singular:bool -> ?seed:Py.Object.t -> unit -> Py.Object.t
(**
A multivariate normal random variable.

The `mean` keyword specifies the mean. The `cov` keyword specifies the
covariance matrix.

Methods
-------
``pdf(x, mean=None, cov=1, allow_singular=False)``
    Probability density function.
``logpdf(x, mean=None, cov=1, allow_singular=False)``
    Log of the probability density function.
``cdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)``
    Cumulative distribution function.
``logcdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)``
    Log of the cumulative distribution function.
``rvs(mean=None, cov=1, size=1, random_state=None)``
    Draw random samples from a multivariate normal distribution.
``entropy()``
    Compute the differential entropy of the multivariate normal.

Parameters
----------
x : array_like
    Quantiles, with the last axis of `x` denoting the components.
mean : array_like, optional
    Mean of the distribution (default zero)
cov : array_like, optional
    Covariance matrix of the distribution (default one)
allow_singular : bool, optional
    Whether to allow a singular covariance matrix.  (Default: False)
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Alternatively, the object may be called (as a function) to fix the mean
and covariance parameters, returning a 'frozen' multivariate normal
random variable:

rv = multivariate_normal(mean=None, cov=1, allow_singular=False)
    - Frozen object with the same methods but holding the given
      mean and covariance fixed.

Notes
-----
Setting the parameter `mean` to `None` is equivalent to having `mean`
    be the zero-vector. The parameter `cov` can be a scalar, in which case
    the covariance matrix is the identity times that value, a vector of
    diagonal entries for the covariance matrix, or a two-dimensional
    array_like.
    

The covariance matrix `cov` must be a (symmetric) positive
semi-definite matrix. The determinant and inverse of `cov` are computed
as the pseudo-determinant and pseudo-inverse, respectively, so
that `cov` does not need to have full rank.

The probability density function for `multivariate_normal` is

.. math::

    f(x) = \frac{1}{\sqrt{(2 \pi)^k \det \Sigma}}
           \exp\left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right),

where :math:`\mu` is the mean, :math:`\Sigma` the covariance matrix,
and :math:`k` is the dimension of the space where :math:`x` takes values.

.. versionadded:: 0.14.0

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy.stats import multivariate_normal

>>> x = np.linspace(0, 5, 10, endpoint=False)
>>> y = multivariate_normal.pdf(x, mean=2.5, cov=0.5); y
array([ 0.00108914,  0.01033349,  0.05946514,  0.20755375,  0.43939129,
        0.56418958,  0.43939129,  0.20755375,  0.05946514,  0.01033349])
>>> fig1 = plt.figure()
>>> ax = fig1.add_subplot(111)
>>> ax.plot(x, y)

The input quantiles can be any shape of array, as long as the last
axis labels the components.  This allows us for instance to
display the frozen pdf for a non-isotropic random variable in 2D as
follows:

>>> x, y = np.mgrid[-1:1:.01, -1:1:.01]
>>> pos = np.dstack((x, y))
>>> rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])
>>> fig2 = plt.figure()
>>> ax2 = fig2.add_subplot(111)
>>> ax2.contourf(x, y, rv.pdf(pos))
*)

val mvsdist : [>`Ndarray] Np.Obj.t -> (Py.Object.t * Py.Object.t * Py.Object.t)
(**
'Frozen' distributions for mean, variance, and standard deviation of data.

Parameters
----------
data : array_like
    Input array. Converted to 1-D using ravel.
    Requires 2 or more data-points.

Returns
-------
mdist : 'frozen' distribution object
    Distribution object representing the mean of the data.
vdist : 'frozen' distribution object
    Distribution object representing the variance of the data.
sdist : 'frozen' distribution object
    Distribution object representing the standard deviation of the data.

See Also
--------
bayes_mvs

Notes
-----
The return values from ``bayes_mvs(data)`` is equivalent to
``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.

In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``
on the three distribution objects returned from this function will give
the same results that are returned from `bayes_mvs`.

References
----------
T.E. Oliphant, 'A Bayesian perspective on estimating mean, variance, and
standard-deviation from data', https://scholarsarchive.byu.edu/facpub/278,
2006.

Examples
--------
>>> from scipy import stats
>>> data = [6, 9, 12, 7, 8, 8, 13]
>>> mean, var, std = stats.mvsdist(data)

We now have frozen distribution objects 'mean', 'var' and 'std' that we can
examine:

>>> mean.mean()
9.0
>>> mean.interval(0.95)
(6.6120585482655692, 11.387941451734431)
>>> mean.std()
1.1952286093343936
*)

val nakagami : ?loc:float -> ?scale:float -> nu:Py.Object.t -> unit -> [`Nakagami_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Nakagami continuous random variable.

As an instance of the `rv_continuous` class, `nakagami` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(nu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, nu, loc=0, scale=1)
    Probability density function.
logpdf(x, nu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, nu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, nu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, nu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, nu, loc=0, scale=1)
    Log of the survival function.
ppf(q, nu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, nu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, nu, loc=0, scale=1)
    Non-central moment of order n
stats(nu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(nu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, nu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(nu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(nu, loc=0, scale=1)
    Median of the distribution.
mean(nu, loc=0, scale=1)
    Mean of the distribution.
var(nu, loc=0, scale=1)
    Variance of the distribution.
std(nu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, nu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `nakagami` is:

.. math::

    f(x, \nu) = \frac{2 \nu^\nu}{\Gamma(\nu)} x^{2\nu-1} \exp(-\nu x^2)

for :math:`x >= 0`, :math:`\nu > 0`.

`nakagami` takes ``nu`` as a shape parameter for :math:`\nu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``nakagami.pdf(x, nu, loc, scale)`` is identically
equivalent to ``nakagami.pdf(y, nu) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import nakagami
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> nu = 4.97
>>> mean, var, skew, kurt = nakagami.stats(nu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(nakagami.ppf(0.01, nu),
...                 nakagami.ppf(0.99, nu), 100)
>>> ax.plot(x, nakagami.pdf(x, nu),
...        'r-', lw=5, alpha=0.6, label='nakagami pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = nakagami(nu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = nakagami.ppf([0.001, 0.5, 0.999], nu)
>>> np.allclose([0.001, 0.5, 0.999], nakagami.cdf(vals, nu))
True

Generate random numbers:

>>> r = nakagami.rvs(nu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val nbinom : ?loc:float -> n:Py.Object.t -> p:Py.Object.t -> unit -> [`Nbinom_gen|`Object|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A negative binomial discrete random variable.

As an instance of the `rv_discrete` class, `nbinom` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(n, p, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, n, p, loc=0)
    Probability mass function.
logpmf(k, n, p, loc=0)
    Log of the probability mass function.
cdf(k, n, p, loc=0)
    Cumulative distribution function.
logcdf(k, n, p, loc=0)
    Log of the cumulative distribution function.
sf(k, n, p, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, n, p, loc=0)
    Log of the survival function.
ppf(q, n, p, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, n, p, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(n, p, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(n, p, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(n, p, loc=0)
    Median of the distribution.
mean(n, p, loc=0)
    Mean of the distribution.
var(n, p, loc=0)
    Variance of the distribution.
std(n, p, loc=0)
    Standard deviation of the distribution.
interval(alpha, n, p, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
Negative binomial distribution describes a sequence of i.i.d. Bernoulli
trials, repeated until a predefined, non-random number of successes occurs.

The probability mass function of the number of failures for `nbinom` is:

.. math::

   f(k) = \binom{k+n-1}{n-1} p^n (1-p)^k

for :math:`k \ge 0`.

`nbinom` takes :math:`n` and :math:`p` as shape parameters where n is the
number of successes, whereas p is the probability of a single success.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``nbinom.pmf(k, n, p, loc)`` is identically
equivalent to ``nbinom.pmf(k - loc, n, p)``.

Examples
--------
>>> from scipy.stats import nbinom
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> n, p = 0.4, 0.4
>>> mean, var, skew, kurt = nbinom.stats(n, p, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(nbinom.ppf(0.01, n, p),
...               nbinom.ppf(0.99, n, p))
>>> ax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')
>>> ax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = nbinom(n, p)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = nbinom.cdf(x, n, p)
>>> np.allclose(x, nbinom.ppf(prob, n, p))
True

Generate random numbers:

>>> r = nbinom.rvs(n, p, size=1000)
*)

val ncf : ?loc:float -> ?scale:float -> dfn:Py.Object.t -> dfd:Py.Object.t -> nc:Py.Object.t -> unit -> [`Ncf_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central F distribution continuous random variable.

As an instance of the `rv_continuous` class, `ncf` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(dfn, dfd, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, dfn, dfd, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, dfn, dfd, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, dfn, dfd, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, dfn, dfd, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, dfn, dfd, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, dfn, dfd, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, dfn, dfd, nc, loc=0, scale=1)
    Non-central moment of order n
stats(dfn, dfd, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(dfn, dfd, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, dfn, dfd, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(dfn, dfd, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(dfn, dfd, nc, loc=0, scale=1)
    Median of the distribution.
mean(dfn, dfd, nc, loc=0, scale=1)
    Mean of the distribution.
var(dfn, dfd, nc, loc=0, scale=1)
    Variance of the distribution.
std(dfn, dfd, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, dfn, dfd, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `ncf` is:

.. math::

    f(x, n_1, n_2, \lambda) =
                      \exp(\frac{\lambda}{2} + \lambda n_1 \frac{x}{2(n_1 x+n_2)})
                      n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\
                      (n_2+n_1 x)^{-(n_1+n_2)/2}
                      \gamma(n_1/2) \gamma(1+n_2/2) \\
                     \frac{L^{\frac{v_1}{2}-1}_{v_2/2}
                           (-\lambda v_1 \frac{x}{2(v_1 x+v_2)})}
                          {B(v_1/2, v_2/2)  \gamma(\frac{v_1+v_2}{2})}

for :math:`n_1 > 1`, :math:`n_2, \lambda > 0`.  Here :math:`n_1` is the
degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in
the denominator, :math:`\lambda` the non-centrality parameter,
:math:`\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a
generalized Laguerre polynomial and :math:`B` is the beta function.

`ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ncf.pdf(x, dfn, dfd, nc, loc, scale)`` is identically
equivalent to ``ncf.pdf(y, dfn, dfd, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import ncf
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> dfn, dfd, nc = 27, 27, 0.416
>>> mean, var, skew, kurt = ncf.stats(dfn, dfd, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ncf.ppf(0.01, dfn, dfd, nc),
...                 ncf.ppf(0.99, dfn, dfd, nc), 100)
>>> ax.plot(x, ncf.pdf(x, dfn, dfd, nc),
...        'r-', lw=5, alpha=0.6, label='ncf pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ncf(dfn, dfd, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ncf.ppf([0.001, 0.5, 0.999], dfn, dfd, nc)
>>> np.allclose([0.001, 0.5, 0.999], ncf.cdf(vals, dfn, dfd, nc))
True

Generate random numbers:

>>> r = ncf.rvs(dfn, dfd, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val nct : ?loc:float -> ?scale:float -> df:Py.Object.t -> nc:Py.Object.t -> unit -> [`Nct_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central Student's t continuous random variable.

As an instance of the `rv_continuous` class, `nct` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, df, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, nc, loc=0, scale=1)
    Non-central moment of order n
stats(df, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, nc, loc=0, scale=1)
    Median of the distribution.
mean(df, nc, loc=0, scale=1)
    Mean of the distribution.
var(df, nc, loc=0, scale=1)
    Variance of the distribution.
std(df, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
If :math:`Y` is a standard normal random variable and :math:`V` is
an independent chi-square random variable (`chi2`) with :math:`k` degrees
of freedom, then

.. math::

    X = \frac{Y + c}{\sqrt{V/k}}

has a non-central Student's t distribution on the real line.
The degrees of freedom parameter :math:`k` (denoted ``df`` in the
implementation) satisfies :math:`k > 0` and the noncentrality parameter
:math:`c` (denoted ``nc`` in the implementation) is a real number.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``nct.pdf(x, df, nc, loc, scale)`` is identically
equivalent to ``nct.pdf(y, df, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import nct
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df, nc = 14, 0.24
>>> mean, var, skew, kurt = nct.stats(df, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(nct.ppf(0.01, df, nc),
...                 nct.ppf(0.99, df, nc), 100)
>>> ax.plot(x, nct.pdf(x, df, nc),
...        'r-', lw=5, alpha=0.6, label='nct pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = nct(df, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = nct.ppf([0.001, 0.5, 0.999], df, nc)
>>> np.allclose([0.001, 0.5, 0.999], nct.cdf(vals, df, nc))
True

Generate random numbers:

>>> r = nct.rvs(df, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val ncx2 : ?loc:float -> ?scale:float -> df:Py.Object.t -> nc:Py.Object.t -> unit -> [`Ncx2_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A non-central chi-squared continuous random variable.

As an instance of the `rv_continuous` class, `ncx2` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, nc, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, nc, loc=0, scale=1)
    Probability density function.
logpdf(x, df, nc, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, nc, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, nc, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, nc, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, nc, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, nc, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, nc, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, nc, loc=0, scale=1)
    Non-central moment of order n
stats(df, nc, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, nc, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, nc, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, nc, loc=0, scale=1)
    Median of the distribution.
mean(df, nc, loc=0, scale=1)
    Mean of the distribution.
var(df, nc, loc=0, scale=1)
    Variance of the distribution.
std(df, nc, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, nc, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `ncx2` is:

.. math::

    f(x, k, \lambda) = \frac{1}{2} \exp(-(\lambda+x)/2)
        (x/\lambda)^{(k-2)/4}  I_{(k-2)/2}(\sqrt{\lambda x})

for :math:`x >= 0` and :math:`k, \lambda > 0`. :math:`k` specifies the
degrees of freedom (denoted ``df`` in the implementation) and
:math:`\lambda` is the non-centrality parameter (denoted ``nc`` in the
implementation). :math:`I_\nu` denotes the modified Bessel function of
first order of degree :math:`\nu` (`scipy.special.iv`).

`ncx2` takes ``df`` and ``nc`` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``ncx2.pdf(x, df, nc, loc, scale)`` is identically
equivalent to ``ncx2.pdf(y, df, nc) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import ncx2
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df, nc = 21, 1.06
>>> mean, var, skew, kurt = ncx2.stats(df, nc, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(ncx2.ppf(0.01, df, nc),
...                 ncx2.ppf(0.99, df, nc), 100)
>>> ax.plot(x, ncx2.pdf(x, df, nc),
...        'r-', lw=5, alpha=0.6, label='ncx2 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = ncx2(df, nc)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = ncx2.ppf([0.001, 0.5, 0.999], df, nc)
>>> np.allclose([0.001, 0.5, 0.999], ncx2.cdf(vals, df, nc))
True

Generate random numbers:

>>> r = ncx2.rvs(df, nc, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val norm : ?loc:float -> ?scale:float -> unit -> [`Norm_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A normal continuous random variable.

The location (``loc``) keyword specifies the mean.
The scale (``scale``) keyword specifies the standard deviation.

As an instance of the `rv_continuous` class, `norm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `norm` is:

.. math::

    f(x) = \frac{\exp(-x^2/2)}{\sqrt{2\pi}}

for a real number :math:`x`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``norm.pdf(x, loc, scale)`` is identically
equivalent to ``norm.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import norm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = norm.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(norm.ppf(0.01),
...                 norm.ppf(0.99), 100)
>>> ax.plot(x, norm.pdf(x),
...        'r-', lw=5, alpha=0.6, label='norm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = norm()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = norm.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))
True

Generate random numbers:

>>> r = norm.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val normaltest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Test whether a sample differs from a normal distribution.

This function tests the null hypothesis that a sample comes
from a normal distribution.  It is based on D'Agostino and
Pearson's [1]_, [2]_ test that combines skew and kurtosis to
produce an omnibus test of normality.

Parameters
----------
a : array_like
    The array containing the sample to be tested.
axis : int or None, optional
    Axis along which to compute test. Default is 0. If None,
    compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and
    ``k`` is the z-score returned by `kurtosistest`.
pvalue : float or array
   A 2-sided chi squared probability for the hypothesis test.

References
----------
.. [1] D'Agostino, R. B. (1971), 'An omnibus test of normality for
       moderate and large sample size', Biometrika, 58, 341-348

.. [2] D'Agostino, R. and Pearson, E. S. (1973), 'Tests for departure from
       normality', Biometrika, 60, 613-622

Examples
--------
>>> from scipy import stats
>>> pts = 1000
>>> np.random.seed(28041990)
>>> a = np.random.normal(0, 1, size=pts)
>>> b = np.random.normal(2, 1, size=pts)
>>> x = np.concatenate((a, b))
>>> k2, p = stats.normaltest(x)
>>> alpha = 1e-3
>>> print('p = {:g}'.format(p))
p = 3.27207e-11
>>> if p < alpha:  # null hypothesis: x comes from a normal distribution
...     print('The null hypothesis can be rejected')
... else:
...     print('The null hypothesis cannot be rejected')
The null hypothesis can be rejected
*)

val norminvgauss : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Norminvgauss_gen|`Object|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Normal Inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `norminvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `norminvgauss` is:

.. math::

    f(x, a, b) = (a \exp(\sqrt{a^2 - b^2} + b x)) /
                 (\pi \sqrt{1 + x^2} \, K_1(a \sqrt{1 + x^2}))

where :math:`x` is a real number, the parameter :math:`a` is the tail
heaviness and :math:`b` is the asymmetry parameter satisfying
:math:`a > 0` and :math:`|b| <= a`.
:math:`K_1` is the modified Bessel function of second kind
(`scipy.special.k1`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``norminvgauss.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``norminvgauss.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

A normal inverse Gaussian random variable `Y` with parameters `a` and `b`
can be expressed as a normal mean-variance mixture:
`Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is
`invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used
to generate random variates.

References
----------
O. Barndorff-Nielsen, 'Hyperbolic Distributions and Distributions on
Hyperbolae', Scandinavian Journal of Statistics, Vol. 5(3),
pp. 151-157, 1978.

O. Barndorff-Nielsen, 'Normal Inverse Gaussian Distributions and Stochastic
Volatility Modelling', Scandinavian Journal of Statistics, Vol. 24,
pp. 1-13, 1997.

Examples
--------
>>> from scipy.stats import norminvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 1, 0.5
>>> mean, var, skew, kurt = norminvgauss.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(norminvgauss.ppf(0.01, a, b),
...                 norminvgauss.ppf(0.99, a, b), 100)
>>> ax.plot(x, norminvgauss.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='norminvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = norminvgauss(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = norminvgauss.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], norminvgauss.cdf(vals, a, b))
True

Generate random numbers:

>>> r = norminvgauss.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val obrientransform : Py.Object.t list -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the O'Brien transform on input data (any number of arrays).

Used to test for homogeneity of variance prior to running one-way stats.
Each array in ``*args`` is one level of a factor.
If `f_oneway` is run on the transformed data and found significant,
the variances are unequal.  From Maxwell and Delaney [1]_, p.112.

Parameters
----------
args : tuple of array_like
    Any number of arrays.

Returns
-------
obrientransform : ndarray
    Transformed data for use in an ANOVA.  The first dimension
    of the result corresponds to the sequence of transformed
    arrays.  If the arrays given are all 1-D of the same length,
    the return value is a 2-D array; otherwise it is a 1-D array
    of type object, with each element being an ndarray.

References
----------
.. [1] S. E. Maxwell and H. D. Delaney, 'Designing Experiments and
       Analyzing Data: A Model Comparison Perspective', Wadsworth, 1990.

Examples
--------
We'll test the following data sets for differences in their variance.

>>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]
>>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]

Apply the O'Brien transform to the data.

>>> from scipy.stats import obrientransform
>>> tx, ty = obrientransform(x, y)

Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the
transformed data.

>>> from scipy.stats import f_oneway
>>> F, p = f_oneway(tx, ty)
>>> p
0.1314139477040335

If we require that ``p < 0.05`` for significance, we cannot conclude
that the variances are different.
*)

val pareto : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Pareto_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Pareto continuous random variable.

As an instance of the `rv_continuous` class, `pareto` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `pareto` is:

.. math::

    f(x, b) = \frac{b}{x^{b+1}}

for :math:`x \ge 1`, :math:`b > 0`.

`pareto` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``pareto.pdf(x, b, loc, scale)`` is identically
equivalent to ``pareto.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import pareto
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 2.62
>>> mean, var, skew, kurt = pareto.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(pareto.ppf(0.01, b),
...                 pareto.ppf(0.99, b), 100)
>>> ax.plot(x, pareto.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='pareto pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = pareto(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = pareto.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], pareto.cdf(vals, b))
True

Generate random numbers:

>>> r = pareto.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val pearson3 : ?loc:float -> ?scale:float -> skew:Py.Object.t -> unit -> [`Object|`Pearson3_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A pearson type III continuous random variable.

As an instance of the `rv_continuous` class, `pearson3` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(skew, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, skew, loc=0, scale=1)
    Probability density function.
logpdf(x, skew, loc=0, scale=1)
    Log of the probability density function.
cdf(x, skew, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, skew, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, skew, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, skew, loc=0, scale=1)
    Log of the survival function.
ppf(q, skew, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, skew, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, skew, loc=0, scale=1)
    Non-central moment of order n
stats(skew, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(skew, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, skew, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(skew,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(skew, loc=0, scale=1)
    Median of the distribution.
mean(skew, loc=0, scale=1)
    Mean of the distribution.
var(skew, loc=0, scale=1)
    Variance of the distribution.
std(skew, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, skew, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `pearson3` is:

.. math::

    f(x, skew) = \frac{ |\beta| }{\Gamma(\alpha)}
                 (\beta (x - \zeta))^{\alpha - 1}
                 \exp(-\beta (x - \zeta))

where:

.. math::

        \beta = \frac{2}{skew  stddev}
        \alpha = (stddev \beta)^2
        \zeta = loc - \frac{\alpha}{\beta}

:math:`\Gamma` is the gamma function (`scipy.special.gamma`).
`pearson3` takes ``skew`` as a shape parameter for :math:`skew`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``pearson3.pdf(x, skew, loc, scale)`` is identically
equivalent to ``pearson3.pdf(y, skew) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import pearson3
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> skew = 0.1
>>> mean, var, skew, kurt = pearson3.stats(skew, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(pearson3.ppf(0.01, skew),
...                 pearson3.ppf(0.99, skew), 100)
>>> ax.plot(x, pearson3.pdf(x, skew),
...        'r-', lw=5, alpha=0.6, label='pearson3 pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = pearson3(skew)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = pearson3.ppf([0.001, 0.5, 0.999], skew)
>>> np.allclose([0.001, 0.5, 0.999], pearson3.cdf(vals, skew))
True

Generate random numbers:

>>> r = pearson3.rvs(skew, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


References
----------
R.W. Vogel and D.E. McMartin, 'Probability Plot Goodness-of-Fit and
Skewness Estimation Procedures for the Pearson Type 3 Distribution', Water
Resources Research, Vol.27, 3149-3158 (1991).

L.R. Salvosa, 'Tables of Pearson's Type III Function', Ann. Math. Statist.,
Vol.1, 191-198 (1930).

'Using Modern Computing Tools to Fit the Pearson Type III Distribution to
Aviation Loads Data', Office of Aviation Research (2003).
*)

val pearsonr : x:[>`Ndarray] Np.Obj.t -> y:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Pearson correlation coefficient and p-value for testing non-correlation.

The Pearson correlation coefficient [1]_ measures the linear relationship
between two datasets.  The calculation of the p-value relies on the
assumption that each dataset is normally distributed.  (See Kowalski [3]_
for a discussion of the effects of non-normality of the input on the
distribution of the correlation coefficient.)  Like other correlation
coefficients, this one varies between -1 and +1 with 0 implying no
correlation. Correlations of -1 or +1 imply an exact linear relationship.
Positive correlations imply that as x increases, so does y. Negative
correlations imply that as x increases, y decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Pearson correlation at least as extreme
as the one computed from these datasets.

Parameters
----------
x : (N,) array_like
    Input array.
y : (N,) array_like
    Input array.

Returns
-------
r : float
    Pearson's correlation coefficient.
p-value : float
    Two-tailed p-value.

Warns
-----
PearsonRConstantInputWarning
    Raised if an input is a constant array.  The correlation coefficient
    is not defined in this case, so ``np.nan`` is returned.

PearsonRNearConstantInputWarning
    Raised if an input is 'nearly' constant.  The array ``x`` is considered
    nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.
    Numerical errors in the calculation ``x - mean(x)`` in this case might
    result in an inaccurate calculation of r.

See Also
--------
spearmanr : Spearman rank-order correlation coefficient.
kendalltau : Kendall's tau, a correlation measure for ordinal data.

Notes
-----
The correlation coefficient is calculated as follows:

.. math::

    r = \frac{\sum (x - m_x) (y - m_y)}
             {\sqrt{\sum (x - m_x)^2 \sum (y - m_y)^2}}

where :math:`m_x` is the mean of the vector :math:`x` and :math:`m_y` is
the mean of the vector :math:`y`.

Under the assumption that x and y are drawn from independent normal
distributions (so the population correlation coefficient is 0), the
probability density function of the sample correlation coefficient r
is ([1]_, [2]_)::

           (1 - r**2)**(n/2 - 2)
    f(r) = ---------------------
              B(1/2, n/2 - 1)

where n is the number of samples, and B is the beta function.  This
is sometimes referred to as the exact distribution of r.  This is
the distribution that is used in `pearsonr` to compute the p-value.
The distribution is a beta distribution on the interval [-1, 1],
with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's
implementation of the beta distribution, the distribution of r is::

    dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)

The p-value returned by `pearsonr` is a two-sided p-value.  For a
given sample with correlation coefficient r, the p-value is
the probability that abs(r') of a random sample x' and y' drawn from
the population with zero correlation would be greater than or equal
to abs(r).  In terms of the object ``dist`` shown above, the p-value
for a given r and length n can be computed as::

    p = 2*dist.cdf(-abs(r))

When n is 2, the above continuous distribution is not well-defined.
One can interpret the limit of the beta distribution as the shape
parameters a and b approach a = b = 0 as a discrete distribution with
equal probability masses at r = 1 and r = -1.  More directly, one
can observe that, given the data x = [x1, x2] and y = [y1, y2], and
assuming x1 != x2 and y1 != y2, the only possible values for r are 1
and -1.  Because abs(r') for any sample x' and y' with length 2 will
be 1, the two-sided p-value for a sample of length 2 is always 1.

References
----------
.. [1] 'Pearson correlation coefficient', Wikipedia,
       https://en.wikipedia.org/wiki/Pearson_correlation_coefficient
.. [2] Student, 'Probable error of a correlation coefficient',
       Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.
.. [3] C. J. Kowalski, 'On the Effects of Non-Normality on the Distribution
       of the Sample Product-Moment Correlation Coefficient'
       Journal of the Royal Statistical Society. Series C (Applied
       Statistics), Vol. 21, No. 1 (1972), pp. 1-12.

Examples
--------
>>> from scipy import stats
>>> a = np.array([0, 0, 0, 1, 1, 1, 1])
>>> b = np.arange(7)
>>> stats.pearsonr(a, b)
(0.8660254037844386, 0.011724811003954649)

>>> stats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])
(-0.7426106572325057, 0.1505558088534455)
*)

val percentileofscore : ?kind:[`Rank | `Weak | `Strict | `Mean] -> a:[>`Ndarray] Np.Obj.t -> score:[`I of int | `F of float] -> unit -> float
(**
Compute the percentile rank of a score relative to a list of scores.

A `percentileofscore` of, for example, 80% means that 80% of the
scores in `a` are below the given score. In the case of gaps or
ties, the exact definition depends on the optional keyword, `kind`.

Parameters
----------
a : array_like
    Array of scores to which `score` is compared.
score : int or float
    Score that is compared to the elements in `a`.
kind : {'rank', 'weak', 'strict', 'mean'}, optional
    Specifies the interpretation of the resulting score.
    The following options are available (default is 'rank'):

      * 'rank': Average percentage ranking of score.  In case of multiple
        matches, average the percentage rankings of all matching scores.
      * 'weak': This kind corresponds to the definition of a cumulative
        distribution function.  A percentileofscore of 80% means that 80%
        of values are less than or equal to the provided score.
      * 'strict': Similar to 'weak', except that only values that are
        strictly less than the given score are counted.
      * 'mean': The average of the 'weak' and 'strict' scores, often used
        in testing.  See https://en.wikipedia.org/wiki/Percentile_rank

Returns
-------
pcos : float
    Percentile-position of score (0-100) relative to `a`.

See Also
--------
numpy.percentile

Examples
--------
Three-quarters of the given values lie below a given score:

>>> from scipy import stats
>>> stats.percentileofscore([1, 2, 3, 4], 3)
75.0

With multiple matches, note how the scores of the two matches, 0.6
and 0.8 respectively, are averaged:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3)
70.0

Only 2/5 values are strictly less than 3:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')
40.0

But 4/5 values are less than or equal to 3:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')
80.0

The average between the weak and the strict scores is:

>>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')
60.0
*)

val planck : ?loc:float -> lambda_:Py.Object.t -> unit -> [`Object|`Planck_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Planck discrete exponential random variable.

As an instance of the `rv_discrete` class, `planck` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lambda_, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, lambda_, loc=0)
    Probability mass function.
logpmf(k, lambda_, loc=0)
    Log of the probability mass function.
cdf(k, lambda_, loc=0)
    Cumulative distribution function.
logcdf(k, lambda_, loc=0)
    Log of the cumulative distribution function.
sf(k, lambda_, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, lambda_, loc=0)
    Log of the survival function.
ppf(q, lambda_, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lambda_, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(lambda_, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lambda_, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(lambda_,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(lambda_, loc=0)
    Median of the distribution.
mean(lambda_, loc=0)
    Mean of the distribution.
var(lambda_, loc=0)
    Variance of the distribution.
std(lambda_, loc=0)
    Standard deviation of the distribution.
interval(alpha, lambda_, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `planck` is:

.. math::

    f(k) = (1-\exp(-\lambda)) \exp(-\lambda k)

for :math:`k \ge 0` and :math:`\lambda > 0`.

`planck` takes :math:`\lambda` as shape parameter. The Planck distribution
can be written as a geometric distribution (`geom`) with
:math:`p = 1 - \exp(-\lambda)` shifted by `loc = -1`.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``planck.pmf(k, lambda_, loc)`` is identically
equivalent to ``planck.pmf(k - loc, lambda_)``.

See Also
--------
geom

Examples
--------
>>> from scipy.stats import planck
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lambda_ = 0.51
>>> mean, var, skew, kurt = planck.stats(lambda_, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(planck.ppf(0.01, lambda_),
...               planck.ppf(0.99, lambda_))
>>> ax.plot(x, planck.pmf(x, lambda_), 'bo', ms=8, label='planck pmf')
>>> ax.vlines(x, 0, planck.pmf(x, lambda_), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = planck(lambda_)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = planck.cdf(x, lambda_)
>>> np.allclose(x, planck.ppf(prob, lambda_))
True

Generate random numbers:

>>> r = planck.rvs(lambda_, size=1000)
*)

val pointbiserialr : x:Py.Object.t -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculate a point biserial correlation coefficient and its p-value.

The point biserial correlation is used to measure the relationship
between a binary variable, x, and a continuous variable, y. Like other
correlation coefficients, this one varies between -1 and +1 with 0
implying no correlation. Correlations of -1 or +1 imply a determinative
relationship.

This function uses a shortcut formula but produces the same result as
`pearsonr`.

Parameters
----------
x : array_like of bools
    Input array.
y : array_like
    Input array.

Returns
-------
correlation : float
    R value.
pvalue : float
    Two-sided p-value.

Notes
-----
`pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.
It is equivalent to `pearsonr.`

The value of the point-biserial correlation can be calculated from:

.. math::

    r_{pb} = \frac{\overline{Y_{1}} -
             \overline{Y_{0}}}{s_{y}}\sqrt{\frac{N_{1} N_{2}}{N (N - 1))}}

Where :math:`Y_{0}` and :math:`Y_{1}` are means of the metric
observations coded 0 and 1 respectively; :math:`N_{0}` and :math:`N_{1}`
are number of observations coded 0 and 1 respectively; :math:`N` is the
total number of observations and :math:`s_{y}` is the standard
deviation of all the metric observations.

A value of :math:`r_{pb}` that is significantly different from zero is
completely equivalent to a significant difference in means between the two
groups. Thus, an independent groups t Test with :math:`N-2` degrees of
freedom may be used to test whether :math:`r_{pb}` is nonzero. The
relation between the t-statistic for comparing two independent groups and
:math:`r_{pb}` is given by:

.. math::

    t = \sqrt{N - 2}\frac{r_{pb}}{\sqrt{1 - r^{2}_{pb}}}

References
----------
.. [1] J. Lev, 'The Point Biserial Coefficient of Correlation', Ann. Math.
       Statist., Vol. 20, no.1, pp. 125-126, 1949.

.. [2] R.F. Tate, 'Correlation Between a Discrete and a Continuous
       Variable. Point-Biserial Correlation.', Ann. Math. Statist., Vol. 25,
       np. 3, pp. 603-607, 1954.

.. [3] D. Kornbrot 'Point Biserial Correlation', In Wiley StatsRef:
       Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.
       https://doi.org/10.1002/9781118445112.stat06227

Examples
--------
>>> from scipy import stats
>>> a = np.array([0, 0, 0, 1, 1, 1, 1])
>>> b = np.arange(7)
>>> stats.pointbiserialr(a, b)
(0.8660254037844386, 0.011724811003954652)
>>> stats.pearsonr(a, b)
(0.86602540378443871, 0.011724811003954626)
>>> np.corrcoef(a, b)
array([[ 1.       ,  0.8660254],
       [ 0.8660254,  1.       ]])
*)

val poisson : ?loc:float -> mu:Py.Object.t -> unit -> [`Object|`Poisson_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A Poisson discrete random variable.

As an instance of the `rv_discrete` class, `poisson` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, mu, loc=0)
    Probability mass function.
logpmf(k, mu, loc=0)
    Log of the probability mass function.
cdf(k, mu, loc=0)
    Cumulative distribution function.
logcdf(k, mu, loc=0)
    Log of the cumulative distribution function.
sf(k, mu, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, mu, loc=0)
    Log of the survival function.
ppf(q, mu, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(mu, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(mu,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0)
    Median of the distribution.
mean(mu, loc=0)
    Mean of the distribution.
var(mu, loc=0)
    Variance of the distribution.
std(mu, loc=0)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `poisson` is:

.. math::

    f(k) = \exp(-\mu) \frac{\mu^k}{k!}

for :math:`k \ge 0`.

`poisson` takes :math:`\mu` as shape parameter.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``poisson.pmf(k, mu, loc)`` is identically
equivalent to ``poisson.pmf(k - loc, mu)``.

Examples
--------
>>> from scipy.stats import poisson
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.6
>>> mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(poisson.ppf(0.01, mu),
...               poisson.ppf(0.99, mu))
>>> ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')
>>> ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = poisson(mu)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = poisson.cdf(x, mu)
>>> np.allclose(x, poisson.ppf(prob, mu))
True

Generate random numbers:

>>> r = poisson.rvs(mu, size=1000)
*)

val power_divergence : ?f_exp:[>`Ndarray] Np.Obj.t -> ?ddof:int -> ?axis:[`I of int | `None] -> ?lambda_:[`S of string | `F of float] -> f_obs:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Cressie-Read power divergence statistic and goodness of fit test.

This function tests the null hypothesis that the categorical data
has the given frequencies, using the Cressie-Read power divergence
statistic.

Parameters
----------
f_obs : array_like
    Observed frequencies in each category.
f_exp : array_like, optional
    Expected frequencies in each category.  By default the categories are
    assumed to be equally likely.
ddof : int, optional
    'Delta degrees of freedom': adjustment to the degrees of freedom
    for the p-value.  The p-value is computed using a chi-squared
    distribution with ``k - 1 - ddof`` degrees of freedom, where `k`
    is the number of observed frequencies.  The default value of `ddof`
    is 0.
axis : int or None, optional
    The axis of the broadcast result of `f_obs` and `f_exp` along which to
    apply the test.  If axis is None, all values in `f_obs` are treated
    as a single data set.  Default is 0.
lambda_ : float or str, optional
    The power in the Cressie-Read power divergence statistic.  The default
    is 1.  For convenience, `lambda_` may be assigned one of the following
    strings, in which case the corresponding numerical value is used::

        String              Value   Description
        'pearson'             1     Pearson's chi-squared statistic.
                                    In this case, the function is
                                    equivalent to `stats.chisquare`.
        'log-likelihood'      0     Log-likelihood ratio. Also known as
                                    the G-test [3]_.
        'freeman-tukey'      -1/2   Freeman-Tukey statistic.
        'mod-log-likelihood' -1     Modified log-likelihood ratio.
        'neyman'             -2     Neyman's statistic.
        'cressie-read'        2/3   The power recommended in [5]_.

Returns
-------
statistic : float or ndarray
    The Cressie-Read power divergence test statistic.  The value is
    a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.
pvalue : float or ndarray
    The p-value of the test.  The value is a float if `ddof` and the
    return value `stat` are scalars.

See Also
--------
chisquare

Notes
-----
This test is invalid when the observed or expected frequencies in each
category are too small.  A typical rule is that all of the observed
and expected frequencies should be at least 5.

When `lambda_` is less than zero, the formula for the statistic involves
dividing by `f_obs`, so a warning or error may be generated if any value
in `f_obs` is 0.

Similarly, a warning or error may be generated if any value in `f_exp` is
zero when `lambda_` >= 0.

The default degrees of freedom, k-1, are for the case when no parameters
of the distribution are estimated. If p parameters are estimated by
efficient maximum likelihood then the correct degrees of freedom are
k-1-p. If the parameters are estimated in a different way, then the
dof can be between k-1-p and k-1. However, it is also possible that
the asymptotic distribution is not a chisquare, in which case this
test is not appropriate.

This function handles masked arrays.  If an element of `f_obs` or `f_exp`
is masked, then data at that position is ignored, and does not count
towards the size of the data set.

.. versionadded:: 0.13.0

References
----------
.. [1] Lowry, Richard.  'Concepts and Applications of Inferential
       Statistics'. Chapter 8.
       https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html
.. [2] 'Chi-squared test', https://en.wikipedia.org/wiki/Chi-squared_test
.. [3] 'G-test', https://en.wikipedia.org/wiki/G-test
.. [4] Sokal, R. R. and Rohlf, F. J. 'Biometry: the principles and
       practice of statistics in biological research', New York: Freeman
       (1981)
.. [5] Cressie, N. and Read, T. R. C., 'Multinomial Goodness-of-Fit
       Tests', J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),
       pp. 440-464.

Examples
--------
(See `chisquare` for more examples.)

When just `f_obs` is given, it is assumed that the expected frequencies
are uniform and given by the mean of the observed frequencies.  Here we
perform a G-test (i.e. use the log-likelihood ratio statistic):

>>> from scipy.stats import power_divergence
>>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')
(2.006573162632538, 0.84823476779463769)

The expected frequencies can be given with the `f_exp` argument:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[16, 16, 16, 16, 16, 8],
...                  lambda_='log-likelihood')
(3.3281031458963746, 0.6495419288047497)

When `f_obs` is 2-D, by default the test is applied to each column.

>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T
>>> obs.shape
(6, 2)
>>> power_divergence(obs, lambda_='log-likelihood')
(array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))

By setting ``axis=None``, the test is applied to all data in the array,
which is equivalent to applying the test to the flattened array.

>>> power_divergence(obs, axis=None)
(23.31034482758621, 0.015975692534127565)
>>> power_divergence(obs.ravel())
(23.31034482758621, 0.015975692534127565)

`ddof` is the change to make to the default degrees of freedom.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)
(2.0, 0.73575888234288467)

The calculation of the p-values is done by broadcasting the
test statistic with `ddof`.

>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])
(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))

`f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has
shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting
`f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared
statistics, we must use ``axis=1``:

>>> power_divergence([16, 18, 16, 14, 12, 12],
...                  f_exp=[[16, 16, 16, 16, 16, 8],
...                         [8, 20, 20, 16, 12, 12]],
...                  axis=1)
(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))
*)

val powerlaw : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> [`Object|`Powerlaw_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power-function continuous random variable.

As an instance of the `rv_continuous` class, `powerlaw` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powerlaw` is:

.. math::

    f(x, a) = a x^{a-1}

for :math:`0 \le x \le 1`, :math:`a > 0`.

`powerlaw` takes ``a`` as a shape parameter for :math:`a`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powerlaw.pdf(x, a, loc, scale)`` is identically
equivalent to ``powerlaw.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

`powerlaw` is a special case of `beta` with ``b=1``.

Examples
--------
>>> from scipy.stats import powerlaw
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 1.66
>>> mean, var, skew, kurt = powerlaw.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powerlaw.ppf(0.01, a),
...                 powerlaw.ppf(0.99, a), 100)
>>> ax.plot(x, powerlaw.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='powerlaw pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powerlaw(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powerlaw.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], powerlaw.cdf(vals, a))
True

Generate random numbers:

>>> r = powerlaw.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val powerlognorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> s:Py.Object.t -> unit -> [`Object|`Powerlognorm_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power log-normal continuous random variable.

As an instance of the `rv_continuous` class, `powerlognorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, s, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, s, loc=0, scale=1)
    Probability density function.
logpdf(x, c, s, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, s, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, s, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, s, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, s, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, s, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, s, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, s, loc=0, scale=1)
    Non-central moment of order n
stats(c, s, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, s, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, s, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, s, loc=0, scale=1)
    Median of the distribution.
mean(c, s, loc=0, scale=1)
    Mean of the distribution.
var(c, s, loc=0, scale=1)
    Variance of the distribution.
std(c, s, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, s, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powerlognorm` is:

.. math::

    f(x, c, s) = \frac{c}{x s} \phi(\log(x)/s)
                 (\Phi(-\log(x)/s))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x > 0`, :math:`s, c > 0`.

`powerlognorm` takes :math:`c` and :math:`s` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powerlognorm.pdf(x, c, s, loc, scale)`` is identically
equivalent to ``powerlognorm.pdf(y, c, s) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import powerlognorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, s = 2.14, 0.446
>>> mean, var, skew, kurt = powerlognorm.stats(c, s, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powerlognorm.ppf(0.01, c, s),
...                 powerlognorm.ppf(0.99, c, s), 100)
>>> ax.plot(x, powerlognorm.pdf(x, c, s),
...        'r-', lw=5, alpha=0.6, label='powerlognorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powerlognorm(c, s)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powerlognorm.ppf([0.001, 0.5, 0.999], c, s)
>>> np.allclose([0.001, 0.5, 0.999], powerlognorm.cdf(vals, c, s))
True

Generate random numbers:

>>> r = powerlognorm.rvs(c, s, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val powernorm : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Powernorm_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A power normal continuous random variable.

As an instance of the `rv_continuous` class, `powernorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `powernorm` is:

.. math::

    f(x, c) = c \phi(x) (\Phi(-x))^{c-1}

where :math:`\phi` is the normal pdf, and :math:`\Phi` is the normal cdf,
and :math:`x >= 0`, :math:`c > 0`.

`powernorm` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``powernorm.pdf(x, c, loc, scale)`` is identically
equivalent to ``powernorm.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import powernorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 4.45
>>> mean, var, skew, kurt = powernorm.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(powernorm.ppf(0.01, c),
...                 powernorm.ppf(0.99, c), 100)
>>> ax.plot(x, powernorm.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='powernorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = powernorm(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = powernorm.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], powernorm.cdf(vals, c))
True

Generate random numbers:

>>> r = powernorm.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val ppcc_max : ?brack:Py.Object.t -> ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> x:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Calculate the shape parameter that maximizes the PPCC.

The probability plot correlation coefficient (PPCC) plot can be used to
determine the optimal shape parameter for a one-parameter family of
distributions.  ppcc_max returns the shape parameter that would maximize the
probability plot correlation coefficient for the given data to a
one-parameter family of distributions.

Parameters
----------
x : array_like
    Input array.
brack : tuple, optional
    Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)
    then they are assumed to be a starting interval for a downhill bracket
    search (see `scipy.optimize.brent`).
dist : str or stats.distributions instance, optional
    Distribution or distribution function name.  Objects that look enough
    like a stats.distributions instance (i.e. they have a ``ppf`` method)
    are also accepted.  The default is ``'tukeylambda'``.

Returns
-------
shape_value : float
    The shape parameter at which the probability plot correlation
    coefficient reaches its max value.

See Also
--------
ppcc_plot, probplot, boxcox

Notes
-----
The brack keyword serves as a starting point which is useful in corner
cases. One can use a plot to obtain a rough visual estimate of the location
for the maximum to start the search near it.

References
----------
.. [1] J.J. Filliben, 'The Probability Plot Correlation Coefficient Test for
       Normality', Technometrics, Vol. 17, pp. 111-117, 1975.

.. [2] https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm

Examples
--------
First we generate some random data from a Tukey-Lambda distribution,
with shape parameter -0.7:

>>> from scipy import stats
>>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000,
...                           random_state=1234567) + 1e4

Now we explore this data with a PPCC plot as well as the related
probability plot and Box-Cox normplot.  A red line is drawn where we
expect the PPCC value to be maximal (at the shape parameter -0.7 used
above):

>>> import matplotlib.pyplot as plt
>>> fig = plt.figure(figsize=(8, 6))
>>> ax = fig.add_subplot(111)
>>> res = stats.ppcc_plot(x, -5, 5, plot=ax)

We calculate the value where the shape should reach its maximum and a red
line is drawn there. The line should coincide with the highest point in the
ppcc_plot.

>>> max = stats.ppcc_max(x)
>>> ax.vlines(max, 0, 1, colors='r', label='Expected shape value')

>>> plt.show()
*)

val ppcc_plot : ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> a:Py.Object.t -> b:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Calculate and optionally plot probability plot correlation coefficient.

The probability plot correlation coefficient (PPCC) plot can be used to
determine the optimal shape parameter for a one-parameter family of
distributions.  It cannot be used for distributions without shape parameters
(like the normal distribution) or with multiple shape parameters.

By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A
Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed
distributions via an approximately normal one, and is therefore particularly
useful in practice.

Parameters
----------
x : array_like
    Input array.
a, b : scalar
    Lower and upper bounds of the shape parameter to use.
dist : str or stats.distributions instance, optional
    Distribution or distribution function name.  Objects that look enough
    like a stats.distributions instance (i.e. they have a ``ppf`` method)
    are also accepted.  The default is ``'tukeylambda'``.
plot : object, optional
    If given, plots PPCC against the shape parameter.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `a` to `b`).

Returns
-------
svals : ndarray
    The shape values for which `ppcc` was calculated.
ppcc : ndarray
    The calculated probability plot correlation coefficient values.

See Also
--------
ppcc_max, probplot, boxcox_normplot, tukeylambda

References
----------
J.J. Filliben, 'The Probability Plot Correlation Coefficient Test for
Normality', Technometrics, Vol. 17, pp. 111-117, 1975.

Examples
--------
First we generate some random data from a Tukey-Lambda distribution,
with shape parameter -0.7:

>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234567)
>>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000) + 1e4

Now we explore this data with a PPCC plot as well as the related
probability plot and Box-Cox normplot.  A red line is drawn where we
expect the PPCC value to be maximal (at the shape parameter -0.7 used
above):

>>> fig = plt.figure(figsize=(12, 4))
>>> ax1 = fig.add_subplot(131)
>>> ax2 = fig.add_subplot(132)
>>> ax3 = fig.add_subplot(133)
>>> res = stats.probplot(x, plot=ax1)
>>> res = stats.boxcox_normplot(x, -5, 5, plot=ax2)
>>> res = stats.ppcc_plot(x, -5, 5, plot=ax3)
>>> ax3.vlines(-0.7, 0, 1, colors='r', label='Expected shape value')
>>> plt.show()
*)

val probplot : ?sparams:Py.Object.t -> ?dist:[`Stats_distributions_instance of Py.Object.t | `S of string] -> ?fit:bool -> ?plot:Py.Object.t -> ?rvalue:Py.Object.t -> x:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate quantiles for a probability plot, and optionally show the plot.

Generates a probability plot of sample data against the quantiles of a
specified theoretical distribution (the normal distribution by default).
`probplot` optionally calculates a best-fit line for the data and plots the
results using Matplotlib or a given plot function.

Parameters
----------
x : array_like
    Sample/response data from which `probplot` creates the plot.
sparams : tuple, optional
    Distribution-specific shape parameters (shape parameters plus location
    and scale).
dist : str or stats.distributions instance, optional
    Distribution or distribution function name. The default is 'norm' for a
    normal probability plot.  Objects that look enough like a
    stats.distributions instance (i.e. they have a ``ppf`` method) are also
    accepted.
fit : bool, optional
    Fit a least-squares regression (best-fit) line to the sample data if
    True (default).
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.

Returns
-------
(osm, osr) : tuple of ndarrays
    Tuple of theoretical quantiles (osm, or order statistic medians) and
    ordered responses (osr).  `osr` is simply sorted input `x`.
    For details on how `osm` is calculated see the Notes section.
(slope, intercept, r) : tuple of floats, optional
    Tuple  containing the result of the least-squares fit, if that is
    performed by `probplot`. `r` is the square root of the coefficient of
    determination.  If ``fit=False`` and ``plot=None``, this tuple is not
    returned.

Notes
-----
Even if `plot` is given, the figure is not shown or saved by `probplot`;
``plt.show()`` or ``plt.savefig('figname.png')`` should be used after
calling `probplot`.

`probplot` generates a probability plot, which should not be confused with
a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this
type, see ``statsmodels.api.ProbPlot``.

The formula used for the theoretical quantiles (horizontal axis of the
probability plot) is Filliben's estimate::

    quantiles = dist.ppf(val), for

            0.5**(1/n),                  for i = n
      val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1
            1 - 0.5**(1/n),              for i = 1

where ``i`` indicates the i-th ordered value and ``n`` is the total number
of values.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> nsample = 100
>>> np.random.seed(7654321)

A t distribution with small degrees of freedom:

>>> ax1 = plt.subplot(221)
>>> x = stats.t.rvs(3, size=nsample)
>>> res = stats.probplot(x, plot=plt)

A t distribution with larger degrees of freedom:

>>> ax2 = plt.subplot(222)
>>> x = stats.t.rvs(25, size=nsample)
>>> res = stats.probplot(x, plot=plt)

A mixture of two normal distributions with broadcasting:

>>> ax3 = plt.subplot(223)
>>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],
...                    size=(nsample//2,2)).ravel()
>>> res = stats.probplot(x, plot=plt)

A standard normal distribution:

>>> ax4 = plt.subplot(224)
>>> x = stats.norm.rvs(loc=0, scale=1, size=nsample)
>>> res = stats.probplot(x, plot=plt)

Produce a new figure with a loggamma distribution, using the ``dist`` and
``sparams`` keywords:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> x = stats.loggamma.rvs(c=2.5, size=500)
>>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)
>>> ax.set_title('Probplot for loggamma dist with shape parameter 2.5')

Show the results with Matplotlib:

>>> plt.show()
*)

val randint : ?loc:float -> low:Py.Object.t -> high:Py.Object.t -> unit -> [`Object|`Randint_gen|`Rv_discrete|`Rv_generic] Np.Obj.t
(**
A uniform discrete random variable.

As an instance of the `rv_discrete` class, `randint` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(low, high, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, low, high, loc=0)
    Probability mass function.
logpmf(k, low, high, loc=0)
    Log of the probability mass function.
cdf(k, low, high, loc=0)
    Cumulative distribution function.
logcdf(k, low, high, loc=0)
    Log of the cumulative distribution function.
sf(k, low, high, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, low, high, loc=0)
    Log of the survival function.
ppf(q, low, high, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, low, high, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(low, high, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(low, high, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(low, high), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(low, high, loc=0)
    Median of the distribution.
mean(low, high, loc=0)
    Mean of the distribution.
var(low, high, loc=0)
    Variance of the distribution.
std(low, high, loc=0)
    Standard deviation of the distribution.
interval(alpha, low, high, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `randint` is:

.. math::

    f(k) = \frac{1}{high - low}

for ``k = low, ..., high - 1``.

`randint` takes ``low`` and ``high`` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``randint.pmf(k, low, high, loc)`` is identically
equivalent to ``randint.pmf(k - loc, low, high)``.

Examples
--------
>>> from scipy.stats import randint
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> low, high = 7, 31
>>> mean, var, skew, kurt = randint.stats(low, high, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(randint.ppf(0.01, low, high),
...               randint.ppf(0.99, low, high))
>>> ax.plot(x, randint.pmf(x, low, high), 'bo', ms=8, label='randint pmf')
>>> ax.vlines(x, 0, randint.pmf(x, low, high), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = randint(low, high)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = randint.cdf(x, low, high)
>>> np.allclose(x, randint.ppf(prob, low, high))
True

Generate random numbers:

>>> r = randint.rvs(low, high, size=1000)
*)

val rankdata : ?method_:[`Average | `Min | `Max | `Dense | `Ordinal] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Assign ranks to data, dealing with ties appropriately.

Ranks begin at 1.  The `method` argument controls how ranks are assigned
to equal values.  See [1]_ for further discussion of ranking methods.

Parameters
----------
a : array_like
    The array of values to be ranked.  The array is first flattened.
method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional
    The method used to assign ranks to tied elements.
    The following methods are available (default is 'average'):

      * 'average': The average of the ranks that would have been assigned to
        all the tied values is assigned to each value.
      * 'min': The minimum of the ranks that would have been assigned to all
        the tied values is assigned to each value.  (This is also
        referred to as 'competition' ranking.)
      * 'max': The maximum of the ranks that would have been assigned to all
        the tied values is assigned to each value.
      * 'dense': Like 'min', but the rank of the next highest element is
        assigned the rank immediately after those assigned to the tied
        elements.
      * 'ordinal': All values are given a distinct rank, corresponding to
        the order that the values occur in `a`.

Returns
-------
ranks : ndarray
     An array of length equal to the size of `a`, containing rank
     scores.

References
----------
.. [1] 'Ranking', https://en.wikipedia.org/wiki/Ranking

Examples
--------
>>> from scipy.stats import rankdata
>>> rankdata([0, 2, 3, 2])
array([ 1. ,  2.5,  4. ,  2.5])
>>> rankdata([0, 2, 3, 2], method='min')
array([ 1,  2,  4,  2])
>>> rankdata([0, 2, 3, 2], method='max')
array([ 1,  3,  4,  3])
>>> rankdata([0, 2, 3, 2], method='dense')
array([ 1,  2,  3,  2])
>>> rankdata([0, 2, 3, 2], method='ordinal')
array([ 1,  2,  4,  3])
*)

val ranksums : x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute the Wilcoxon rank-sum statistic for two samples.

The Wilcoxon rank-sum test tests the null hypothesis that two sets
of measurements are drawn from the same distribution.  The alternative
hypothesis is that values in one sample are more likely to be
larger than the values in the other sample.

This test should be used to compare two samples from continuous
distributions.  It does not handle ties between measurements
in x and y.  For tie-handling and an optional continuity correction
see `scipy.stats.mannwhitneyu`.

Parameters
----------
x,y : array_like
    The data from the two samples.

Returns
-------
statistic : float
    The test statistic under the large-sample approximation that the
    rank sum statistic is normally distributed.
pvalue : float
    The two-sided p-value of the test.

References
----------
.. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test
*)

val rayleigh : ?loc:float -> ?scale:float -> unit -> [`Object|`Rayleigh_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Rayleigh continuous random variable.

As an instance of the `rv_continuous` class, `rayleigh` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rayleigh` is:

.. math::

    f(x) = x \exp(-x^2/2)

for :math:`x \ge 0`.

`rayleigh` is a special case of `chi` with ``df=2``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rayleigh.pdf(x, loc, scale)`` is identically
equivalent to ``rayleigh.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import rayleigh
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = rayleigh.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rayleigh.ppf(0.01),
...                 rayleigh.ppf(0.99), 100)
>>> ax.plot(x, rayleigh.pdf(x),
...        'r-', lw=5, alpha=0.6, label='rayleigh pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rayleigh()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rayleigh.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], rayleigh.cdf(vals))
True

Generate random numbers:

>>> r = rayleigh.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val rdist : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rdist_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
An R-distributed (symmetric beta) continuous random variable.

As an instance of the `rv_continuous` class, `rdist` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rdist` is:

.. math::

    f(x, c) = \frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}

for :math:`-1 \le x \le 1`, :math:`c > 0`. `rdist` is also called the
symmetric beta distribution: if B has a `beta` distribution with
parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with
parameter c.

`rdist` takes ``c`` as a shape parameter for :math:`c`.

This distribution includes the following distribution kernels as
special cases::

    c = 2:  uniform
    c = 3:  `semicircular`
    c = 4:  Epanechnikov (parabolic)
    c = 6:  quartic (biweight)
    c = 8:  triweight

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rdist.pdf(x, c, loc, scale)`` is identically
equivalent to ``rdist.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import rdist
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.6
>>> mean, var, skew, kurt = rdist.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rdist.ppf(0.01, c),
...                 rdist.ppf(0.99, c), 100)
>>> ax.plot(x, rdist.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='rdist pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rdist(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rdist.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], rdist.cdf(vals, c))
True

Generate random numbers:

>>> r = rdist.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val recipinvgauss : ?loc:float -> ?scale:float -> mu:Py.Object.t -> unit -> [`Object|`Recipinvgauss_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A reciprocal inverse Gaussian continuous random variable.

As an instance of the `rv_continuous` class, `recipinvgauss` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, mu, loc=0, scale=1)
    Probability density function.
logpdf(x, mu, loc=0, scale=1)
    Log of the probability density function.
cdf(x, mu, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, mu, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, mu, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, mu, loc=0, scale=1)
    Log of the survival function.
ppf(q, mu, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, mu, loc=0, scale=1)
    Non-central moment of order n
stats(mu, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, mu, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu, loc=0, scale=1)
    Median of the distribution.
mean(mu, loc=0, scale=1)
    Mean of the distribution.
var(mu, loc=0, scale=1)
    Variance of the distribution.
std(mu, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, mu, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `recipinvgauss` is:

.. math::

    f(x, \mu) = \frac{1}{\sqrt{2\pi x}}
                \exp\left(\frac{-(1-\mu x)^2}{2\mu^2x}\right)

for :math:`x \ge 0`.

`recipinvgauss` takes ``mu`` as a shape parameter for :math:`\mu`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``recipinvgauss.pdf(x, mu, loc, scale)`` is identically
equivalent to ``recipinvgauss.pdf(y, mu) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import recipinvgauss
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu = 0.63
>>> mean, var, skew, kurt = recipinvgauss.stats(mu, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(recipinvgauss.ppf(0.01, mu),
...                 recipinvgauss.ppf(0.99, mu), 100)
>>> ax.plot(x, recipinvgauss.pdf(x, mu),
...        'r-', lw=5, alpha=0.6, label='recipinvgauss pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = recipinvgauss(mu)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = recipinvgauss.ppf([0.001, 0.5, 0.999], mu)
>>> np.allclose([0.001, 0.5, 0.999], recipinvgauss.cdf(vals, mu))
True

Generate random numbers:

>>> r = recipinvgauss.rvs(mu, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val reciprocal : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Object|`Reciprocal_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A loguniform or reciprocal continuous random variable.

As an instance of the `rv_continuous` class, `reciprocal` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for this class is:

.. math::

    f(x, a, b) = \frac{1}{x \log(b/a)}

for :math:`a \le x \le b`, :math:`b > a > 0`. This class takes
:math:`a` and :math:`b` as shape parameters. The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``reciprocal.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``reciprocal.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import reciprocal
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.01, 1
>>> mean, var, skew, kurt = reciprocal.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(reciprocal.ppf(0.01, a, b),
...                 reciprocal.ppf(0.99, a, b), 100)
>>> ax.plot(x, reciprocal.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='reciprocal pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = reciprocal(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = reciprocal.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], reciprocal.cdf(vals, a, b))
True

Generate random numbers:

>>> r = reciprocal.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


This doesn't show the equal probability of ``0.01``, ``0.1`` and
``1``. This is best when the x-axis is log-scaled:

>>> import numpy as np
>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log10(r))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$10^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()

This random variable will be log-uniform regardless of the base chosen for
``a`` and ``b``. Let's specify with base ``2`` instead:

>>> rvs = reciprocal(2**-2, 2**0).rvs(size=1000)

Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random
variable.  Here's the histogram:

>>> fig, ax = plt.subplots(1, 1)
>>> ax.hist(np.log2(rvs))
>>> ax.set_ylabel('Frequency')
>>> ax.set_xlabel('Value of random variable')
>>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))
>>> ticks = ['$2^{{ {} }}$'.format(i) for i in [-2, -1, 0]]
>>> ax.set_xticklabels(ticks)  # doctest: +SKIP
>>> plt.show()
*)

val relfreq : ?numbins:int -> ?defaultreallimits:Py.Object.t -> ?weights:[>`Ndarray] Np.Obj.t -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float * int)
(**
Return a relative frequency histogram, using the histogram function.

A relative frequency  histogram is a mapping of the number of
observations in each of the bins relative to the total of observations.

Parameters
----------
a : array_like
    Input array.
numbins : int, optional
    The number of bins to use for the histogram. Default is 10.
defaultreallimits : tuple (lower, upper), optional
    The lower and upper values for the range of the histogram.
    If no value is given, a range slightly larger than the range of the
    values in a is used. Specifically ``(a.min() - s, a.max() + s)``,
    where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.
weights : array_like, optional
    The weights for each value in `a`. Default is None, which gives each
    value a weight of 1.0

Returns
-------
frequency : ndarray
    Binned values of relative frequency.
lowerlimit : float
    Lower real limit.
binsize : float
    Width of each bin.
extrapoints : int
    Extra points.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy import stats
>>> a = np.array([2, 4, 1, 2, 3, 2])
>>> res = stats.relfreq(a, numbins=4)
>>> res.frequency
array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])
>>> np.sum(res.frequency)  # relative frequencies should add up to 1
1.0

Create a normal distribution with 1000 random values

>>> rng = np.random.RandomState(seed=12345)
>>> samples = stats.norm.rvs(size=1000, random_state=rng)

Calculate relative frequencies

>>> res = stats.relfreq(samples, numbins=25)

Calculate space of values for x

>>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,
...                                  res.frequency.size)

Plot relative frequency histogram

>>> fig = plt.figure(figsize=(5, 4))
>>> ax = fig.add_subplot(1, 1, 1)
>>> ax.bar(x, res.frequency, width=res.binsize)
>>> ax.set_title('Relative frequency histogram')
>>> ax.set_xlim([x.min(), x.max()])

>>> plt.show()
*)

val rice : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Rice_gen|`Rv_continuous|`Rv_generic] Np.Obj.t
(**
A Rice continuous random variable.

As an instance of the `rv_continuous` class, `rice` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `rice` is:

.. math::

    f(x, b) = x \exp(- \frac{x^2 + b^2}{2}) I_0(x b)

for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel
function of order zero (`scipy.special.i0`).

`rice` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``rice.pdf(x, b, loc, scale)`` is identically
equivalent to ``rice.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

The Rice distribution describes the length, :math:`r`, of a 2-D vector with
components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,
v` are independent Gaussian random variables with standard deviation
:math:`s`.  Let :math:`R = \sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is
``rice.pdf(x, R/s, scale=s)``.

Examples
--------
>>> from scipy.stats import rice
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 0.775
>>> mean, var, skew, kurt = rice.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(rice.ppf(0.01, b),
...                 rice.ppf(0.99, b), 100)
>>> ax.plot(x, rice.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='rice pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = rice(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = rice.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], rice.cdf(vals, b))
True

Generate random numbers:

>>> r = rice.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val rvs_ratio_uniforms : ?size:int list -> ?c:float -> ?random_state:[`I of int | `Np_random_RandomState_instance of Py.Object.t] -> pdf:Py.Object.t -> umax:float -> vmin:float -> vmax:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Generate random samples from a probability density function using the
ratio-of-uniforms method.

Parameters
----------
pdf : callable
    A function with signature `pdf(x)` that is the probability
    density function of the distribution.
umax : float
    The upper bound of the bounding rectangle in the u-direction.
vmin : float
    The lower bound of the bounding rectangle in the v-direction.
vmax : float
    The upper bound of the bounding rectangle in the v-direction.
size : int or tuple of ints, optional
    Defining number of random variates (default is 1).
c : float, optional.
    Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.
random_state : int or np.random.RandomState instance, optional
    If already a RandomState instance, use it.
    If seed is an int, return a new RandomState instance seeded with seed.
    If None, use np.random.RandomState. Default is None.

Returns
-------
rvs : ndarray
    The random variates distributed according to the probability
    distribution defined by the pdf.

Notes
-----
Given a univariate probability density function `pdf` and a constant `c`,
define the set ``A = {(u, v) : 0 < u <= sqrt(pdf(v/u + c))}``.
If `(U, V)` is a random vector uniformly distributed over `A`,
then `V/U + c` follows a distribution according to `pdf`.

The above result (see [1]_, [2]_) can be used to sample random variables
using only the pdf, i.e. no inversion of the cdf is required. Typical
choices of `c` are zero or the mode of `pdf`. The set `A` is a subset of
the rectangle ``R = [0, umax] x [vmin, vmax]`` where

- ``umax = sup sqrt(pdf(x))``
- ``vmin = inf (x - c) sqrt(pdf(x))``
- ``vmax = sup (x - c) sqrt(pdf(x))``

In particular, these values are finite if `pdf` is bounded and
``x**2 * pdf(x)`` is bounded (i.e. subquadratic tails).
One can generate `(U, V)` uniformly on `R` and return
`V/U + c` if `(U, V)` are also in `A` which can be directly
verified.

Intuitively, the method works well if `A` fills up most of the
enclosing rectangle such that the probability is high that `(U, V)`
lies in `A` whenever it lies in `R` as the number of required
iterations becomes too large otherwise. To be more precise, note that
the expected number of iterations to draw `(U, V)` uniformly
distributed on `R` such that `(U, V)` is also in `A` is given by
the ratio ``area(R) / area(A) = 2 * umax * (vmax - vmin)``, using the fact
that the area of `A` is equal to 1/2 (Theorem 7.1 in [1]_). A warning
is displayed if this ratio is larger than 20. Moreover, if the sampling
fails to generate a single random variate after 50000 iterations (i.e.
not a single draw is in `A`), an exception is raised.

If the bounding rectangle is not correctly specified (i.e. if it does not
contain `A`), the algorithm samples from a distribution different from
the one given by `pdf`. It is therefore recommended to perform a
test such as `~scipy.stats.kstest` as a check.

References
----------
.. [1] L. Devroye, 'Non-Uniform Random Variate Generation',
   Springer-Verlag, 1986.

.. [2] W. Hoermann and J. Leydold, 'Generating generalized inverse Gaussian
   random variates', Statistics and Computing, 24(4), p. 547--557, 2014.

.. [3] A.J. Kinderman and J.F. Monahan, 'Computer Generation of Random
   Variables Using the Ratio of Uniform Deviates',
   ACM Transactions on Mathematical Software, 3(3), p. 257--260, 1977.

Examples
--------
>>> from scipy import stats

Simulate normally distributed random variables. It is easy to compute the
bounding rectangle explicitly in that case.

>>> f = stats.norm.pdf
>>> v_bound = np.sqrt(f(np.sqrt(2))) * np.sqrt(2)
>>> umax, vmin, vmax = np.sqrt(f(0)), -v_bound, v_bound
>>> np.random.seed(12345)
>>> rvs = stats.rvs_ratio_uniforms(f, umax, vmin, vmax, size=2500)

The K-S test confirms that the random variates are indeed normally
distributed (normality is not rejected at 5% significance level):

>>> stats.kstest(rvs, 'norm')[1]
0.3420173467307603

The exponential distribution provides another example where the bounding
rectangle can be determined explicitly.

>>> np.random.seed(12345)
>>> rvs = stats.rvs_ratio_uniforms(lambda x: np.exp(-x), umax=1,
...                                vmin=0, vmax=2*np.exp(-1), size=1000)
>>> stats.kstest(rvs, 'expon')[1]
0.928454552559516

Sometimes it can be helpful to use a non-zero shift parameter `c`, see e.g.
[2]_ above in the case of the generalized inverse Gaussian distribution.
*)

val scoreatpercentile : ?limit:Py.Object.t -> ?interpolation_method:[`Fraction | `Lower | `Higher] -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> per:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Calculate the score at a given percentile of the input sequence.

For example, the score at `per=50` is the median. If the desired quantile
lies between two data points, we interpolate between them, according to
the value of `interpolation`. If the parameter `limit` is provided, it
should be a tuple (lower, upper) of two values.

Parameters
----------
a : array_like
    A 1-D array of values from which to extract score.
per : array_like
    Percentile(s) at which to extract score.  Values should be in range
    [0,100].
limit : tuple, optional
    Tuple of two scalars, the lower and upper limits within which to
    compute the percentile. Values of `a` outside
    this (closed) interval will be ignored.
interpolation_method : {'fraction', 'lower', 'higher'}, optional
    Specifies the interpolation method to use,
    when the desired quantile lies between two data points `i` and `j`
    The following options are available (default is 'fraction'):

      * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the
        fractional part of the index surrounded by ``i`` and ``j``
      * 'lower': ``i``
      * 'higher': ``j``

axis : int, optional
    Axis along which the percentiles are computed. Default is None. If
    None, compute over the whole array `a`.

Returns
-------
score : float or ndarray
    Score at percentile(s).

See Also
--------
percentileofscore, numpy.percentile

Notes
-----
This function will become obsolete in the future.
For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality
that `scoreatpercentile` provides.  And it's significantly faster.
Therefore it's recommended to use `numpy.percentile` for users that have
numpy >= 1.9.

Examples
--------
>>> from scipy import stats
>>> a = np.arange(100)
>>> stats.scoreatpercentile(a, 50)
49.5
*)

val sem : ?axis:[`I of int | `None] -> ?ddof:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute standard error of the mean.

Calculate the standard error of the mean (or standard error of
measurement) of the values in the input array.

Parameters
----------
a : array_like
    An array containing the values for which the standard error is
    returned.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Delta degrees-of-freedom. How many degrees of freedom to adjust
    for bias in limited samples relative to the population estimate
    of variance. Defaults to 1.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
s : ndarray or float
    The standard error of the mean in the sample(s), along the input axis.

Notes
-----
The default value for `ddof` is different to the default (0) used by other
ddof containing routines, such as np.std and np.nanstd.

Examples
--------
Find standard error along the first axis:

>>> from scipy import stats
>>> a = np.arange(20).reshape(5,4)
>>> stats.sem(a)
array([ 2.8284,  2.8284,  2.8284,  2.8284])

Find standard error across the whole array, using n degrees of freedom:

>>> stats.sem(a, axis=None, ddof=0)
1.2893796958227628
*)

val semicircular : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Semicircular_gen] Np.Obj.t
(**
A semicircular continuous random variable.

As an instance of the `rv_continuous` class, `semicircular` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `semicircular` is:

.. math::

    f(x) = \frac{2}{\pi} \sqrt{1-x^2}

for :math:`-1 \le x \le 1`.

The distribution is a special case of `rdist` with `c = 3`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``semicircular.pdf(x, loc, scale)`` is identically
equivalent to ``semicircular.pdf(y) / scale`` with
``y = (x - loc) / scale``.

See Also
--------
rdist

References
----------
.. [1] 'Wigner semicircle distribution',
       https://en.wikipedia.org/wiki/Wigner_semicircle_distribution

Examples
--------
>>> from scipy.stats import semicircular
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = semicircular.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(semicircular.ppf(0.01),
...                 semicircular.ppf(0.99), 100)
>>> ax.plot(x, semicircular.pdf(x),
...        'r-', lw=5, alpha=0.6, label='semicircular pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = semicircular()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = semicircular.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], semicircular.cdf(vals))
True

Generate random numbers:

>>> r = semicircular.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val shapiro : [>`Ndarray] Np.Obj.t -> float
(**
Perform the Shapiro-Wilk test for normality.

The Shapiro-Wilk test tests the null hypothesis that the
data was drawn from a normal distribution.

Parameters
----------
x : array_like
    Array of sample data.

Returns
-------
W : float
    The test statistic.
p-value : float
    The p-value for the hypothesis test.

See Also
--------
anderson : The Anderson-Darling test for normality
kstest : The Kolmogorov-Smirnov test for goodness of fit.

Notes
-----
The algorithm used is described in [4]_ but censoring parameters as
described are not implemented. For N > 5000 the W test statistic is accurate
but the p-value may not be.

The chance of rejecting the null hypothesis when it is true is close to 5%
regardless of sample size.

References
----------
.. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm
.. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for
       normality (complete samples), Biometrika, Vol. 52, pp. 591-611.
.. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,
       Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of
       Statistical Modeling and Analytics, Vol. 2, pp. 21-33.
.. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)
>>> x = stats.norm.rvs(loc=5, scale=3, size=100)
>>> stats.shapiro(x)
(0.9772805571556091, 0.08144091814756393)
*)

val siegelslopes : ?x:[>`Ndarray] Np.Obj.t -> ?method_:[`Hierarchical | `Separate] -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Computes the Siegel estimator for a set of points (x, y).

`siegelslopes` implements a method for robust linear regression
using repeated medians (see [1]_) to fit a line to the points (x, y).
The method is robust to outliers with an asymptotic breakdown point
of 50%.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
method : {'hierarchical', 'separate'}
    If 'hierarchical', estimate the intercept using the estimated
    slope ``medslope`` (default option).
    If 'separate', estimate the intercept independent of the estimated
    slope. See Notes for details.

Returns
-------
medslope : float
    Estimate of the slope of the regression line.
medintercept : float
    Estimate of the intercept of the regression line.

See also
--------
theilslopes : a similar technique without repeated medians

Notes
-----
With ``n = len(y)``, compute ``m_j`` as the median of
the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.
``medslope`` is then the median of all slopes ``m_j``.
Two ways are given to estimate the intercept in [1]_ which can be chosen
via the parameter ``method``.
The hierarchical approach uses the estimated slope ``medslope``
and computes ``medintercept`` as the median of ``y - medslope*x``.
The other approach estimates the intercept separately as follows: for
each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`
lines through the remaining points and take the median ``i_j``.
``medintercept`` is the median of the ``i_j``.

The implementation computes `n` times the median of a vector of size `n`
which can be slow for large vectors. There are more efficient algorithms
(see [2]_) which are not implemented here.

References
----------
.. [1] A. Siegel, 'Robust Regression Using Repeated Medians',
       Biometrika, Vol. 69, pp. 242-244, 1982.

.. [2] A. Stein and M. Werman, 'Finding the repeated median regression
       line', Proceedings of the Third Annual ACM-SIAM Symposium on
       Discrete Algorithms, pp. 409-413, 1992.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope and intercept.  For comparison, also compute the
least-squares fit with `linregress`:

>>> res = stats.siegelslopes(y, x)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Siegel regression line is shown in red. The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val sigmaclip : ?low:float -> ?high:float -> a:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float * float)
(**
Perform iterative sigma-clipping of array elements.

Starting from the full sample, all elements outside the critical range are
removed, i.e. all elements of the input array `c` that satisfy either of
the following conditions::

    c < mean(c) - std(c)*low
    c > mean(c) + std(c)*high

The iteration continues with the updated sample until no
elements are outside the (updated) range.

Parameters
----------
a : array_like
    Data array, will be raveled if not 1-D.
low : float, optional
    Lower bound factor of sigma clipping. Default is 4.
high : float, optional
    Upper bound factor of sigma clipping. Default is 4.

Returns
-------
clipped : ndarray
    Input array with clipped elements removed.
lower : float
    Lower threshold value use for clipping.
upper : float
    Upper threshold value use for clipping.

Examples
--------
>>> from scipy.stats import sigmaclip
>>> a = np.concatenate((np.linspace(9.5, 10.5, 31),
...                     np.linspace(0, 20, 5)))
>>> fact = 1.5
>>> c, low, upp = sigmaclip(a, fact, fact)
>>> c
array([  9.96666667,  10.        ,  10.03333333,  10.        ])
>>> c.var(), c.std()
(0.00055555555555555165, 0.023570226039551501)
>>> low, c.mean() - fact*c.std(), c.min()
(9.9646446609406727, 9.9646446609406727, 9.9666666666666668)
>>> upp, c.mean() + fact*c.std(), c.max()
(10.035355339059327, 10.035355339059327, 10.033333333333333)

>>> a = np.concatenate((np.linspace(9.5, 10.5, 11),
...                     np.linspace(-100, -50, 3)))
>>> c, low, upp = sigmaclip(a, 1.8, 1.8)
>>> (c == np.linspace(9.5, 10.5, 11)).all()
True
*)

val skellam : ?loc:float -> mu1:Py.Object.t -> mu2:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Skellam_gen] Np.Obj.t
(**
A  Skellam discrete random variable.

As an instance of the `rv_discrete` class, `skellam` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(mu1, mu2, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, mu1, mu2, loc=0)
    Probability mass function.
logpmf(k, mu1, mu2, loc=0)
    Log of the probability mass function.
cdf(k, mu1, mu2, loc=0)
    Cumulative distribution function.
logcdf(k, mu1, mu2, loc=0)
    Log of the cumulative distribution function.
sf(k, mu1, mu2, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, mu1, mu2, loc=0)
    Log of the survival function.
ppf(q, mu1, mu2, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, mu1, mu2, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(mu1, mu2, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(mu1, mu2, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(mu1, mu2), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(mu1, mu2, loc=0)
    Median of the distribution.
mean(mu1, mu2, loc=0)
    Mean of the distribution.
var(mu1, mu2, loc=0)
    Variance of the distribution.
std(mu1, mu2, loc=0)
    Standard deviation of the distribution.
interval(alpha, mu1, mu2, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
Probability distribution of the difference of two correlated or
uncorrelated Poisson random variables.

Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with
expected values :math:`\lambda_1` and :math:`\lambda_2`. Then,
:math:`k_1 - k_2` follows a Skellam distribution with parameters
:math:`\mu_1 = \lambda_1 - \rho \sqrt{\lambda_1 \lambda_2}` and
:math:`\mu_2 = \lambda_2 - \rho \sqrt{\lambda_1 \lambda_2}`, where
:math:`\rho` is the correlation coefficient between :math:`k_1` and
:math:`k_2`. If the two Poisson-distributed r.v. are independent then
:math:`\rho = 0`.

Parameters :math:`\mu_1` and :math:`\mu_2` must be strictly positive.

For details see: https://en.wikipedia.org/wiki/Skellam_distribution

`skellam` takes :math:`\mu_1` and :math:`\mu_2` as shape parameters.

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``skellam.pmf(k, mu1, mu2, loc)`` is identically
equivalent to ``skellam.pmf(k - loc, mu1, mu2)``.

Examples
--------
>>> from scipy.stats import skellam
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> mu1, mu2 = 15, 8
>>> mean, var, skew, kurt = skellam.stats(mu1, mu2, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(skellam.ppf(0.01, mu1, mu2),
...               skellam.ppf(0.99, mu1, mu2))
>>> ax.plot(x, skellam.pmf(x, mu1, mu2), 'bo', ms=8, label='skellam pmf')
>>> ax.vlines(x, 0, skellam.pmf(x, mu1, mu2), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = skellam(mu1, mu2)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = skellam.cdf(x, mu1, mu2)
>>> np.allclose(x, skellam.ppf(prob, mu1, mu2))
True

Generate random numbers:

>>> r = skellam.rvs(mu1, mu2, size=1000)
*)

val skew : ?axis:[`I of int | `None] -> ?bias:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the sample skewness of a data set.

For normally distributed data, the skewness should be about zero. For
unimodal continuous distributions, a skewness value greater than zero means
that there is more weight in the right tail of the distribution. The
function `skewtest` can be used to determine if the skewness value
is close enough to zero, statistically speaking.

Parameters
----------
a : ndarray
    Input array.
axis : int or None, optional
    Axis along which skewness is calculated. Default is 0.
    If None, compute over the whole array `a`.
bias : bool, optional
    If False, then the calculations are corrected for statistical bias.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
skewness : ndarray
    The skewness of values along an axis, returning 0 where all values are
    equal.

Notes
-----
The sample skewness is computed as the Fisher-Pearson coefficient
of skewness, i.e.

.. math:: 

    g_1=\frac{m_3}{m_2^{3/2}}

where

.. math:: 

    m_i=\frac{1}{N}\sum_{n=1}^N(x[n]-\bar{x})^i

is the biased sample :math:`i\texttt{th}` central moment, and :math:`\bar{x}` is
the sample mean.  If ``bias`` is False, the calculations are
corrected for bias and the value computed is the adjusted
Fisher-Pearson standardized moment coefficient, i.e.

.. math:: 

    G_1=\frac{k_3}{k_2^{3/2}}=
        \frac{\sqrt{N(N-1)}}{N-2}\frac{m_3}{m_2^{3/2}}.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.
   Section 2.2.24.1

Examples
--------
>>> from scipy.stats import skew
>>> skew([1, 2, 3, 4, 5])
0.0
>>> skew([2, 8, 0, 4, 1, 9, 9, 0])
0.2650554122698573
*)

val skewnorm : ?loc:float -> ?scale:float -> a:Py.Object.t -> unit -> Py.Object.t
(**
A skew-normal random variable.

As an instance of the `rv_continuous` class, `skewnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, loc=0, scale=1)
    Probability density function.
logpdf(x, a, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, loc=0, scale=1)
    Non-central moment of order n
stats(a, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0, scale=1)
    Median of the distribution.
mean(a, loc=0, scale=1)
    Mean of the distribution.
var(a, loc=0, scale=1)
    Variance of the distribution.
std(a, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The pdf is::

    skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)

`skewnorm` takes a real number :math:`a` as a skewness parameter
When ``a = 0`` the distribution is identical to a normal distribution
(`norm`). `rvs` implements the method of [1]_.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``skewnorm.pdf(x, a, loc, scale)`` is identically
equivalent to ``skewnorm.pdf(y, a) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import skewnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 4
>>> mean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(skewnorm.ppf(0.01, a),
...                 skewnorm.ppf(0.99, a), 100)
>>> ax.plot(x, skewnorm.pdf(x, a),
...        'r-', lw=5, alpha=0.6, label='skewnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = skewnorm(a)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = skewnorm.ppf([0.001, 0.5, 0.999], a)
>>> np.allclose([0.001, 0.5, 0.999], skewnorm.cdf(vals, a))
True

Generate random numbers:

>>> r = skewnorm.rvs(a, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()


References
----------
.. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of the
    multivariate skew-normal distribution. J. Roy. Statist. Soc., B 61, 579-602.
    https://arxiv.org/abs/0911.2093
*)

val skewtest : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Test whether the skew is different from the normal distribution.

This function tests the null hypothesis that the skewness of
the population that the sample was drawn from is the same
as that of a corresponding normal distribution.

Parameters
----------
a : array
    The data to be tested.
axis : int or None, optional
   Axis along which statistics are calculated. Default is 0.
   If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float
    The computed z-score for this test.
pvalue : float
    Two-sided p-value for the hypothesis test.

Notes
-----
The sample size must be at least 8.

References
----------
.. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,
        'A suggestion for using powerful and informative tests of
        normality', American Statistician 44, pp. 316-321, 1990.

Examples
--------
>>> from scipy.stats import skewtest
>>> skewtest([1, 2, 3, 4, 5, 6, 7, 8])
SkewtestResult(statistic=1.0108048609177787, pvalue=0.3121098361421897)
>>> skewtest([2, 8, 0, 4, 1, 9, 9, 0])
SkewtestResult(statistic=0.44626385374196975, pvalue=0.6554066631275459)
>>> skewtest([1, 2, 3, 4, 5, 6, 7, 8000])
SkewtestResult(statistic=3.571773510360407, pvalue=0.0003545719905823133)
>>> skewtest([100, 100, 100, 100, 100, 100, 100, 101])
SkewtestResult(statistic=3.5717766638478072, pvalue=0.000354567720281634)
*)

val spearmanr : ?b:Py.Object.t -> ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> unit -> (Py.Object.t * float)
(**
Calculate a Spearman correlation coefficient with associated p-value.

The Spearman rank-order correlation coefficient is a nonparametric measure
of the monotonicity of the relationship between two datasets. Unlike the
Pearson correlation, the Spearman correlation does not assume that both
datasets are normally distributed. Like other correlation coefficients,
this one varies between -1 and +1 with 0 implying no correlation.
Correlations of -1 or +1 imply an exact monotonic relationship. Positive
correlations imply that as x increases, so does y. Negative correlations
imply that as x increases, y decreases.

The p-value roughly indicates the probability of an uncorrelated system
producing datasets that have a Spearman correlation at least as extreme
as the one computed from these datasets. The p-values are not entirely
reliable but are probably reasonable for datasets larger than 500 or so.

Parameters
----------
a, b : 1D or 2D array_like, b is optional
    One or two 1-D or 2-D arrays containing multiple variables and
    observations. When these are 1-D, each represents a vector of
    observations of a single variable. For the behavior in the 2-D case,
    see under ``axis``, below.
    Both arrays need to have the same length in the ``axis`` dimension.
axis : int or None, optional
    If axis=0 (default), then each column represents a variable, with
    observations in the rows. If axis=1, the relationship is transposed:
    each row represents a variable, while the columns contain observations.
    If axis=None, then both arrays will be raveled.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
correlation : float or ndarray (2-D square)
    Spearman correlation matrix or correlation coefficient (if only 2
    variables are given as parameters. Correlation matrix is square with
    length equal to total number of variables (columns or rows) in ``a``
    and ``b`` combined.
pvalue : float
    The two-sided p-value for a hypothesis test whose null hypothesis is
    that two sets of data are uncorrelated, has same dimension as rho.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.
   Section  14.7

Examples
--------
>>> from scipy import stats
>>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])
(0.82078268166812329, 0.088587005313543798)
>>> np.random.seed(1234321)
>>> x2n = np.random.randn(100, 2)
>>> y2n = np.random.randn(100, 2)
>>> stats.spearmanr(x2n)
(0.059969996999699973, 0.55338590803773591)
>>> stats.spearmanr(x2n[:,0], x2n[:,1])
(0.059969996999699973, 0.55338590803773591)
>>> rho, pval = stats.spearmanr(x2n, y2n)
>>> rho
array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],
       [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],
       [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],
       [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])
>>> pval
array([[ 0.        ,  0.55338591,  0.06435364,  0.53617935],
       [ 0.55338591,  0.        ,  0.27592895,  0.80234077],
       [ 0.06435364,  0.27592895,  0.        ,  0.73039992],
       [ 0.53617935,  0.80234077,  0.73039992,  0.        ]])
>>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1)
>>> rho
array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],
       [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],
       [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],
       [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])
>>> stats.spearmanr(x2n, y2n, axis=None)
(0.10816770419260482, 0.1273562188027364)
>>> stats.spearmanr(x2n.ravel(), y2n.ravel())
(0.10816770419260482, 0.1273562188027364)

>>> xint = np.random.randint(10, size=(100, 2))
>>> stats.spearmanr(xint)
(0.052760927029710199, 0.60213045837062351)
*)

val special_ortho_group : ?dim:[`Bool of bool | `S of string | `I of int | `F of float] -> ?seed:Py.Object.t -> unit -> Py.Object.t
(**
A matrix-valued SO(N) random variable.

Return a random rotation matrix, drawn from the Haar distribution
(the only uniform distribution on SO(n)).

The `dim` keyword specifies the dimension N.

Methods
-------
``rvs(dim=None, size=1, random_state=None)``
    Draw random samples from SO(N).

Parameters
----------
dim : scalar
    Dimension of matrices

Notes
----------
This class is wrapping the random_rot code from the MDP Toolkit,
https://github.com/mdp-toolkit/mdp-toolkit

Return a random rotation matrix, drawn from the Haar distribution
(the only uniform distribution on SO(n)).
The algorithm is described in the paper
Stewart, G.W., 'The efficient generation of random orthogonal
matrices with an application to condition estimators', SIAM Journal
on Numerical Analysis, 17(3), pp. 403-409, 1980.
For more information see
https://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization

See also the similar `ortho_group`.

Examples
--------
>>> from scipy.stats import special_ortho_group
>>> x = special_ortho_group.rvs(3)

>>> np.dot(x, x.T)
array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16],
       [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16],
       [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]])

>>> import scipy.linalg
>>> scipy.linalg.det(x)
1.0

This generates one random matrix from SO(3). It is orthogonal and
has a determinant of 1.
*)

val t : ?loc:float -> ?scale:float -> df:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`T_gen] Np.Obj.t
(**
A Student's t continuous random variable.

As an instance of the `rv_continuous` class, `t` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(df, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, df, loc=0, scale=1)
    Probability density function.
logpdf(x, df, loc=0, scale=1)
    Log of the probability density function.
cdf(x, df, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, df, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, df, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, df, loc=0, scale=1)
    Log of the survival function.
ppf(q, df, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, df, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, df, loc=0, scale=1)
    Non-central moment of order n
stats(df, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(df, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, df, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(df, loc=0, scale=1)
    Median of the distribution.
mean(df, loc=0, scale=1)
    Mean of the distribution.
var(df, loc=0, scale=1)
    Variance of the distribution.
std(df, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, df, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `t` is:

.. math::

    f(x, \nu) = \frac{\Gamma((\nu+1)/2)}
                    {\sqrt{\pi \nu} \Gamma(\nu/2)}
                (1+x^2/\nu)^{-(\nu+1)/2}

where :math:`x` is a real number and the degrees of freedom parameter
:math:`\nu` (denoted ``df`` in the implementation) satisfies
:math:`\nu > 0`. :math:`\Gamma` is the gamma function
(`scipy.special.gamma`).

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``t.pdf(x, df, loc, scale)`` is identically
equivalent to ``t.pdf(y, df) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import t
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> df = 2.74
>>> mean, var, skew, kurt = t.stats(df, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(t.ppf(0.01, df),
...                 t.ppf(0.99, df), 100)
>>> ax.plot(x, t.pdf(x, df),
...        'r-', lw=5, alpha=0.6, label='t pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = t(df)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = t.ppf([0.001, 0.5, 0.999], df)
>>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))
True

Generate random numbers:

>>> r = t.rvs(df, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val theilslopes : ?x:[>`Ndarray] Np.Obj.t -> ?alpha:float -> y:[>`Ndarray] Np.Obj.t -> unit -> (float * float * float * float)
(**
Computes the Theil-Sen estimator for a set of points (x, y).

`theilslopes` implements a method for robust linear regression.  It
computes the slope as the median of all slopes between paired values.

Parameters
----------
y : array_like
    Dependent variable.
x : array_like or None, optional
    Independent variable. If None, use ``arange(len(y))`` instead.
alpha : float, optional
    Confidence degree between 0 and 1. Default is 95% confidence.
    Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are
    interpreted as 'find the 90% confidence interval'.

Returns
-------
medslope : float
    Theil slope.
medintercept : float
    Intercept of the Theil line, as ``median(y) - medslope*median(x)``.
lo_slope : float
    Lower bound of the confidence interval on `medslope`.
up_slope : float
    Upper bound of the confidence interval on `medslope`.

See also
--------
siegelslopes : a similar technique using repeated medians

Notes
-----
The implementation of `theilslopes` follows [1]_. The intercept is
not defined in [1]_, and here it is defined as ``median(y) -
medslope*median(x)``, which is given in [3]_. Other definitions of
the intercept exist in the literature. A confidence interval for
the intercept is not given as this question is not addressed in
[1]_.

References
----------
.. [1] P.K. Sen, 'Estimates of the regression coefficient based on Kendall's tau',
       J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.
.. [2] H. Theil, 'A rank-invariant method of linear and polynomial
       regression analysis I, II and III',  Nederl. Akad. Wetensch., Proc.
       53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.
.. [3] W.L. Conover, 'Practical nonparametric statistics', 2nd ed.,
       John Wiley and Sons, New York, pp. 493.

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

>>> x = np.linspace(-5, 5, num=150)
>>> y = x + np.random.normal(size=x.size)
>>> y[11:15] += 10  # add outliers
>>> y[-5:] -= 7

Compute the slope, intercept and 90% confidence interval.  For comparison,
also compute the least-squares fit with `linregress`:

>>> res = stats.theilslopes(y, x, 0.90)
>>> lsq_res = stats.linregress(x, y)

Plot the results. The Theil-Sen regression line is shown in red, with the
dashed red lines illustrating the confidence interval of the slope (note
that the dashed red lines are not the confidence interval of the regression
as the confidence interval of the intercept is not included). The green
line shows the least-squares fit for comparison.

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(x, y, 'b.')
>>> ax.plot(x, res[1] + res[0] * x, 'r-')
>>> ax.plot(x, res[1] + res[2] * x, 'r--')
>>> ax.plot(x, res[1] + res[3] * x, 'r--')
>>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')
>>> plt.show()
*)

val tiecorrect : [>`Ndarray] Np.Obj.t -> float
(**
Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.

Parameters
----------
rankvals : array_like
    A 1-D sequence of ranks.  Typically this will be the array
    returned by `~scipy.stats.rankdata`.

Returns
-------
factor : float
    Correction factor for U or H.

See Also
--------
rankdata : Assign ranks to the data
mannwhitneyu : Mann-Whitney rank test
kruskal : Kruskal-Wallis H test

References
----------
.. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral
       Sciences.  New York: McGraw-Hill.

Examples
--------
>>> from scipy.stats import tiecorrect, rankdata
>>> tiecorrect([1, 2.5, 2.5, 4])
0.9
>>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])
>>> ranks
array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])
>>> tiecorrect(ranks)
0.9833333333333333
*)

val tmax : ?upperlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed maximum.

This function computes the maximum value of an array along a given axis,
while ignoring values larger than a specified upper limit.

Parameters
----------
a : array_like
    Array of values.
upperlimit : None or float, optional
    Values in the input array greater than the given limit will be ignored.
    When upperlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the upper limit
    are included.  The default value is True.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
tmax : float, int or ndarray
    Trimmed maximum.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmax(x)
19

>>> stats.tmax(x, 13)
13

>>> stats.tmax(x, 13, inclusive=False)
12
*)

val tmean : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed mean.

This function finds the arithmetic mean of given values, ignoring values
outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored.  When limits is None (default), then all
    values are used.  Either of the limit values in the tuple can also be
    None representing a half-open interval.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to compute test. Default is None.

Returns
-------
tmean : float
    Trimmed mean.

See Also
--------
trim_mean : Returns mean after trimming a proportion from both tails.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmean(x)
9.5
>>> stats.tmean(x, (3,17))
10.0
*)

val tmin : ?lowerlimit:float -> ?axis:[`I of int | `None] -> ?inclusive:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> Py.Object.t
(**
Compute the trimmed minimum.

This function finds the miminum value of an array `a` along the
specified axis, but only considering values greater than a specified
lower limit.

Parameters
----------
a : array_like
    Array of values.
lowerlimit : None or float, optional
    Values in the input array less than the given limit will be ignored.
    When lowerlimit is None, then all values are used. The default value
    is None.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
inclusive : {True, False}, optional
    This flag determines whether values exactly equal to the lower limit
    are included.  The default value is True.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
tmin : float, int or ndarray
    Trimmed minimum.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tmin(x)
0

>>> stats.tmin(x, 13)
13

>>> stats.tmin(x, 13, inclusive=False)
14
*)

val trapz : ?loc:float -> ?scale:float -> c:Py.Object.t -> d:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Trapz_gen] Np.Obj.t
(**
A trapezoidal continuous random variable.

As an instance of the `rv_continuous` class, `trapz` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, d, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, d, loc=0, scale=1)
    Probability density function.
logpdf(x, c, d, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, d, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, d, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, d, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, d, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, d, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, d, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, d, loc=0, scale=1)
    Non-central moment of order n
stats(c, d, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, d, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, d, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, d, loc=0, scale=1)
    Median of the distribution.
mean(c, d, loc=0, scale=1)
    Mean of the distribution.
var(c, d, loc=0, scale=1)
    Variance of the distribution.
std(c, d, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, d, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The trapezoidal distribution can be represented with an up-sloping line
from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``
and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.

`trapz` takes :math:`c` and :math:`d` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``trapz.pdf(x, c, d, loc, scale)`` is identically
equivalent to ``trapz.pdf(y, c, d) / scale`` with
``y = (x - loc) / scale``.

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

Examples
--------
>>> from scipy.stats import trapz
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c, d = 0.2, 0.8
>>> mean, var, skew, kurt = trapz.stats(c, d, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(trapz.ppf(0.01, c, d),
...                 trapz.ppf(0.99, c, d), 100)
>>> ax.plot(x, trapz.pdf(x, c, d),
...        'r-', lw=5, alpha=0.6, label='trapz pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = trapz(c, d)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = trapz.ppf([0.001, 0.5, 0.999], c, d)
>>> np.allclose([0.001, 0.5, 0.999], trapz.cdf(vals, c, d))
True

Generate random numbers:

>>> r = trapz.rvs(c, d, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val triang : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Triang_gen] Np.Obj.t
(**
A triangular continuous random variable.

As an instance of the `rv_continuous` class, `triang` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The triangular distribution can be represented with an up-sloping line from
``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``
to ``(loc + scale)``.

`triang` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``triang.pdf(x, c, loc, scale)`` is identically
equivalent to ``triang.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

The standard form is in the range [0, 1] with c the mode.
The location parameter shifts the start to `loc`.
The scale parameter changes the width from 1 to `scale`.

Examples
--------
>>> from scipy.stats import triang
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.158
>>> mean, var, skew, kurt = triang.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(triang.ppf(0.01, c),
...                 triang.ppf(0.99, c), 100)
>>> ax.plot(x, triang.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='triang pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = triang(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = triang.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], triang.cdf(vals, c))
True

Generate random numbers:

>>> r = triang.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val trim1 : ?tail:[`Left | `Right] -> ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Slice off a proportion from ONE end of the passed array distribution.

If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'
10% of scores. The lowest or highest values are trimmed (depending on
the tail).
Slice off less if proportion results in a non-integer slice index
(i.e. conservatively slices off `proportiontocut` ).

Parameters
----------
a : array_like
    Input array.
proportiontocut : float
    Fraction to cut off of 'left' or 'right' of distribution.
tail : {'left', 'right'}, optional
    Defaults to 'right'.
axis : int or None, optional
    Axis along which to trim data. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
trim1 : ndarray
    Trimmed version of array `a`. The order of the trimmed content is
    undefined.
*)

val trim_mean : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Return mean of array after trimming distribution from both tails.

If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of
scores. The input is sorted before slicing. Slices off less if proportion
results in a non-integer slice index (i.e., conservatively slices off
`proportiontocut` ).

Parameters
----------
a : array_like
    Input array.
proportiontocut : float
    Fraction to cut off of both tails of the distribution.
axis : int or None, optional
    Axis along which the trimmed means are computed. Default is 0.
    If None, compute over the whole array `a`.

Returns
-------
trim_mean : ndarray
    Mean of trimmed array.

See Also
--------
trimboth
tmean : Compute the trimmed mean ignoring values outside given `limits`.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.trim_mean(x, 0.1)
9.5
>>> x2 = x.reshape(5, 4)
>>> x2
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15],
       [16, 17, 18, 19]])
>>> stats.trim_mean(x2, 0.25)
array([  8.,   9.,  10.,  11.])
>>> stats.trim_mean(x2, 0.25, axis=1)
array([  1.5,   5.5,   9.5,  13.5,  17.5])
*)

val trimboth : ?axis:[`I of int | `None] -> a:[>`Ndarray] Np.Obj.t -> proportiontocut:float -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Slice off a proportion of items from both ends of an array.

Slice off the passed proportion of items from both ends of the passed
array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**
rightmost 10% of scores). The trimmed values are the lowest and
highest ones.
Slice off less if proportion results in a non-integer slice index (i.e.
conservatively slices off `proportiontocut`).

Parameters
----------
a : array_like
    Data to trim.
proportiontocut : float
    Proportion (in range 0-1) of total data set to trim of each end.
axis : int or None, optional
    Axis along which to trim data. Default is 0. If None, compute over
    the whole array `a`.

Returns
-------
out : ndarray
    Trimmed version of array `a`. The order of the trimmed content
    is undefined.

See Also
--------
trim_mean

Examples
--------
>>> from scipy import stats
>>> a = np.arange(20)
>>> b = stats.trimboth(a, 0.1)
>>> b.shape
(16,)
*)

val truncexpon : ?loc:float -> ?scale:float -> b:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Truncexpon_gen] Np.Obj.t
(**
A truncated exponential continuous random variable.

As an instance of the `rv_continuous` class, `truncexpon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, b, loc=0, scale=1)
    Probability density function.
logpdf(x, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, b, loc=0, scale=1)
    Non-central moment of order n
stats(b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(b, loc=0, scale=1)
    Median of the distribution.
mean(b, loc=0, scale=1)
    Mean of the distribution.
var(b, loc=0, scale=1)
    Variance of the distribution.
std(b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `truncexpon` is:

.. math::

    f(x, b) = \frac{\exp(-x)}{1 - \exp(-b)}

for :math:`0 <= x <= b`.

`truncexpon` takes ``b`` as a shape parameter for :math:`b`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``truncexpon.pdf(x, b, loc, scale)`` is identically
equivalent to ``truncexpon.pdf(y, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import truncexpon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> b = 4.69
>>> mean, var, skew, kurt = truncexpon.stats(b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(truncexpon.ppf(0.01, b),
...                 truncexpon.ppf(0.99, b), 100)
>>> ax.plot(x, truncexpon.pdf(x, b),
...        'r-', lw=5, alpha=0.6, label='truncexpon pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = truncexpon(b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = truncexpon.ppf([0.001, 0.5, 0.999], b)
>>> np.allclose([0.001, 0.5, 0.999], truncexpon.cdf(vals, b))
True

Generate random numbers:

>>> r = truncexpon.rvs(b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val truncnorm : ?loc:float -> ?scale:float -> a:Py.Object.t -> b:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Truncnorm_gen] Np.Obj.t
(**
A truncated normal continuous random variable.

As an instance of the `rv_continuous` class, `truncnorm` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, b, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, a, b, loc=0, scale=1)
    Probability density function.
logpdf(x, a, b, loc=0, scale=1)
    Log of the probability density function.
cdf(x, a, b, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, a, b, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, a, b, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, a, b, loc=0, scale=1)
    Log of the survival function.
ppf(q, a, b, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, b, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, a, b, loc=0, scale=1)
    Non-central moment of order n
stats(a, b, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, b, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, a, b, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, b, loc=0, scale=1)
    Median of the distribution.
mean(a, b, loc=0, scale=1)
    Mean of the distribution.
var(a, b, loc=0, scale=1)
    Variance of the distribution.
std(a, b, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, a, b, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The standard form of this distribution is a standard normal truncated to
the range [a, b] --- notice that a and b are defined over the domain of the
standard normal.  To convert clip values for a specific mean and standard
deviation, use::

    a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std

`truncnorm` takes :math:`a` and :math:`b` as shape parameters.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``truncnorm.pdf(x, a, b, loc, scale)`` is identically
equivalent to ``truncnorm.pdf(y, a, b) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import truncnorm
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a, b = 0.1, 2
>>> mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(truncnorm.ppf(0.01, a, b),
...                 truncnorm.ppf(0.99, a, b), 100)
>>> ax.plot(x, truncnorm.pdf(x, a, b),
...        'r-', lw=5, alpha=0.6, label='truncnorm pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = truncnorm(a, b)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = truncnorm.ppf([0.001, 0.5, 0.999], a, b)
>>> np.allclose([0.001, 0.5, 0.999], truncnorm.cdf(vals, a, b))
True

Generate random numbers:

>>> r = truncnorm.rvs(a, b, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val tsem : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed standard error of the mean.

This function finds the standard error of the mean for given
values, ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tsem : float
    Trimmed standard error of the mean.

Notes
-----
`tsem` uses unbiased sample standard deviation, i.e. it uses a
correction factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tsem(x)
1.3228756555322954
>>> stats.tsem(x, (3,17))
1.1547005383792515
*)

val tstd : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed sample standard deviation.

This function finds the sample standard deviation of given values,
ignoring values outside the given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tstd : float
    Trimmed sample standard deviation.

Notes
-----
`tstd` computes the unbiased sample standard deviation, i.e. it uses a
correction factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tstd(x)
5.9160797830996161
>>> stats.tstd(x, (3,17))
4.4721359549995796
*)

val ttest_1samp : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> popmean:[`Ndarray of [>`Ndarray] Np.Obj.t | `F of float] -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the T-test for the mean of ONE group of scores.

This is a two-sided test for the null hypothesis that the expected value
(mean) of a sample of independent observations `a` is equal to the given
population mean, `popmean`.

Parameters
----------
a : array_like
    Sample observation.
popmean : float or array_like
    Expected value in null hypothesis. If array_like, then it must have the
    same shape as `a` excluding the axis dimension.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    t-statistic.
pvalue : float or array
    Two-sided p-value.

Examples
--------
>>> from scipy import stats

>>> np.random.seed(7654567)  # fix seed to get the same result
>>> rvs = stats.norm.rvs(loc=5, scale=10, size=(50,2))

Test if mean of random sample is equal to true mean, and different mean.
We reject the null hypothesis in the second case and don't reject it in
the first case.

>>> stats.ttest_1samp(rvs,5.0)
(array([-0.68014479, -0.04323899]), array([ 0.49961383,  0.96568674]))
>>> stats.ttest_1samp(rvs,0.0)
(array([ 2.77025808,  4.11038784]), array([ 0.00789095,  0.00014999]))

Examples using axis and non-scalar dimension for population mean.

>>> stats.ttest_1samp(rvs,[5.0,0.0])
(array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))
>>> stats.ttest_1samp(rvs.T,[5.0,0.0],axis=1)
(array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))
>>> stats.ttest_1samp(rvs,[[5.0],[0.0]])
(array([[-0.68014479, -0.04323899],
       [ 2.77025808,  4.11038784]]), array([[  4.99613833e-01,   9.65686743e-01],
       [  7.89094663e-03,   1.49986458e-04]]))
*)

val ttest_ind : ?axis:[`I of int | `None] -> ?equal_var:bool -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the T-test for the means of *two independent* samples of scores.

This is a two-sided test for the null hypothesis that 2 independent samples
have identical average (expected) values. This test assumes that the
populations have identical variances by default.

Parameters
----------
a, b : array_like
    The arrays must have the same shape, except in the dimension
    corresponding to `axis` (the first, by default).
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
equal_var : bool, optional
    If True (default), perform a standard independent 2 sample test
    that assumes equal population variances [1]_.
    If False, perform Welch's t-test, which does not assume equal
    population variance [2]_.

    .. versionadded:: 0.11.0
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values


Returns
-------
statistic : float or array
    The calculated t-statistic.
pvalue : float or array
    The two-tailed p-value.

Notes
-----
We can use this test, if we observe two independent samples from
the same or different population, e.g. exam scores of boys and
girls or of two ethnic groups. The test measures whether the
average (expected) value differs significantly across samples. If
we observe a large p-value, for example larger than 0.05 or 0.1,
then we cannot reject the null hypothesis of identical average scores.
If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,
then we reject the null hypothesis of equal averages.

References
----------
.. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test

.. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678)

Test with sample with identical means:

>>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> stats.ttest_ind(rvs1,rvs2)
(0.26833823296239279, 0.78849443369564776)
>>> stats.ttest_ind(rvs1,rvs2, equal_var = False)
(0.26833823296239279, 0.78849452749500748)

`ttest_ind` underestimates p for unequal variances:

>>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)
>>> stats.ttest_ind(rvs1, rvs3)
(-0.46580283298287162, 0.64145827413436174)
>>> stats.ttest_ind(rvs1, rvs3, equal_var = False)
(-0.46580283298287162, 0.64149646246569292)

When n1 != n2, the equal variance t-statistic is no longer equal to the
unequal variance t-statistic:

>>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs4)
(-0.99882539442782481, 0.3182832709103896)
>>> stats.ttest_ind(rvs1, rvs4, equal_var = False)
(-0.69712570584654099, 0.48716927725402048)

T-test with different means, variance, and n:

>>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)
>>> stats.ttest_ind(rvs1, rvs5)
(-1.4679669854490653, 0.14263895620529152)
>>> stats.ttest_ind(rvs1, rvs5, equal_var = False)
(-0.94365973617132992, 0.34744170334794122)
*)

val ttest_ind_from_stats : ?equal_var:bool -> mean1:[>`Ndarray] Np.Obj.t -> std1:[>`Ndarray] Np.Obj.t -> nobs1:[>`Ndarray] Np.Obj.t -> mean2:[>`Ndarray] Np.Obj.t -> std2:[>`Ndarray] Np.Obj.t -> nobs2:[>`Ndarray] Np.Obj.t -> unit -> (Py.Object.t * Py.Object.t)
(**
T-test for means of two independent samples from descriptive statistics.

This is a two-sided test for the null hypothesis that two independent
samples have identical average (expected) values.

Parameters
----------
mean1 : array_like
    The mean(s) of sample 1.
std1 : array_like
    The standard deviation(s) of sample 1.
nobs1 : array_like
    The number(s) of observations of sample 1.
mean2 : array_like
    The mean(s) of sample 2.
std2 : array_like
    The standard deviations(s) of sample 2.
nobs2 : array_like
    The number(s) of observations of sample 2.
equal_var : bool, optional
    If True (default), perform a standard independent 2 sample test
    that assumes equal population variances [1]_.
    If False, perform Welch's t-test, which does not assume equal
    population variance [2]_.

Returns
-------
statistic : float or array
    The calculated t-statistics.
pvalue : float or array
    The two-tailed p-value.

See Also
--------
scipy.stats.ttest_ind

Notes
-----
.. versionadded:: 0.16.0

References
----------
.. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test

.. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test

Examples
--------
Suppose we have the summary data for two samples, as follows::

                     Sample   Sample
               Size   Mean   Variance
    Sample 1    13    15.0     87.5
    Sample 2    11    12.0     39.0

Apply the t-test to this data (with the assumption that the population
variances are equal):

>>> from scipy.stats import ttest_ind_from_stats
>>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,
...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)
Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)

For comparison, here is the data from which those summary statistics
were taken.  With this data, we can compute the same result using
`scipy.stats.ttest_ind`:

>>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])
>>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])
>>> from scipy.stats import ttest_ind
>>> ttest_ind(a, b)
Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)

Suppose we instead have binary data and would like to apply a t-test to
compare the proportion of 1s in two independent groups::

                      Number of    Sample     Sample
                Size    ones        Mean     Variance
    Sample 1    150      30         0.2        0.16
    Sample 2    200      45         0.225      0.174375

The sample mean :math:`\hat{p}` is the proportion of ones in the sample
and the variance for a binary observation is estimated by
:math:`\hat{p}(1-\hat{p})`.

>>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,
...                      mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)
Ttest_indResult(statistic=-0.564327545549774, pvalue=0.5728947691244874)

For comparison, we could compute the t statistic and p-value using
arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.

>>> group1 = np.array([1]*30 + [0]*(150-30))
>>> group2 = np.array([1]*45 + [0]*(200-45))
>>> ttest_ind(group1, group2)
Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)
*)

val ttest_rel : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:Py.Object.t -> b:Py.Object.t -> unit -> (Py.Object.t * Py.Object.t)
(**
Calculate the t-test on TWO RELATED samples of scores, a and b.

This is a two-sided test for the null hypothesis that 2 related or
repeated samples have identical average (expected) values.

Parameters
----------
a, b : array_like
    The arrays must have the same shape.
axis : int or None, optional
    Axis along which to compute test. If None, compute over the whole
    arrays, `a`, and `b`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
statistic : float or array
    t-statistic.
pvalue : float or array
    Two-sided p-value.

Notes
-----
Examples for use are scores of the same set of student in
different exams, or repeated sampling from the same units. The
test measures whether the average score differs significantly
across samples (e.g. exams). If we observe a large p-value, for
example greater than 0.05 or 0.1 then we cannot reject the null
hypothesis of identical average scores. If the p-value is smaller
than the threshold, e.g. 1%, 5% or 10%, then we reject the null
hypothesis of equal averages. Small p-values are associated with
large t-statistics.

References
----------
https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples

Examples
--------
>>> from scipy import stats
>>> np.random.seed(12345678) # fix random seed to get same numbers

>>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)
>>> rvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) +
...         stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs2)
(0.24101764965300962, 0.80964043445811562)
>>> rvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) +
...         stats.norm.rvs(scale=0.2,size=500))
>>> stats.ttest_rel(rvs1,rvs3)
(-3.9995108708727933, 7.3082402191726459e-005)
*)

val tukeylambda : ?loc:float -> ?scale:float -> lam:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Tukeylambda_gen] Np.Obj.t
(**
A Tukey-Lamdba continuous random variable.

As an instance of the `rv_continuous` class, `tukeylambda` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(lam, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, lam, loc=0, scale=1)
    Probability density function.
logpdf(x, lam, loc=0, scale=1)
    Log of the probability density function.
cdf(x, lam, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, lam, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, lam, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, lam, loc=0, scale=1)
    Log of the survival function.
ppf(q, lam, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, lam, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, lam, loc=0, scale=1)
    Non-central moment of order n
stats(lam, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(lam, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, lam, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(lam,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(lam, loc=0, scale=1)
    Median of the distribution.
mean(lam, loc=0, scale=1)
    Mean of the distribution.
var(lam, loc=0, scale=1)
    Variance of the distribution.
std(lam, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, lam, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
A flexible distribution, able to represent and interpolate between the
following distributions:

- Cauchy                (:math:`lambda = -1`)
- logistic              (:math:`lambda = 0`)
- approx Normal         (:math:`lambda = 0.14`)
- uniform from -1 to 1  (:math:`lambda = 1`)

`tukeylambda` takes a real number :math:`lambda` (denoted ``lam``
in the implementation) as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``tukeylambda.pdf(x, lam, loc, scale)`` is identically
equivalent to ``tukeylambda.pdf(y, lam) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import tukeylambda
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> lam = 3.13
>>> mean, var, skew, kurt = tukeylambda.stats(lam, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(tukeylambda.ppf(0.01, lam),
...                 tukeylambda.ppf(0.99, lam), 100)
>>> ax.plot(x, tukeylambda.pdf(x, lam),
...        'r-', lw=5, alpha=0.6, label='tukeylambda pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = tukeylambda(lam)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = tukeylambda.ppf([0.001, 0.5, 0.999], lam)
>>> np.allclose([0.001, 0.5, 0.999], tukeylambda.cdf(vals, lam))
True

Generate random numbers:

>>> r = tukeylambda.rvs(lam, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val tvar : ?limits:Py.Object.t -> ?inclusive:Py.Object.t -> ?axis:[`I of int | `None] -> ?ddof:int -> a:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute the trimmed variance.

This function computes the sample variance of an array of values,
while ignoring values which are outside of given `limits`.

Parameters
----------
a : array_like
    Array of values.
limits : None or (lower limit, upper limit), optional
    Values in the input array less than the lower limit or greater than the
    upper limit will be ignored. When limits is None, then all values are
    used. Either of the limit values in the tuple can also be None
    representing a half-open interval.  The default value is None.
inclusive : (bool, bool), optional
    A tuple consisting of the (lower flag, upper flag).  These flags
    determine whether values exactly equal to the lower or upper limits
    are included.  The default value is (True, True).
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over the
    whole array `a`.
ddof : int, optional
    Delta degrees of freedom.  Default is 1.

Returns
-------
tvar : float
    Trimmed variance.

Notes
-----
`tvar` computes the unbiased sample variance, i.e. it uses a correction
factor ``n / (n - 1)``.

Examples
--------
>>> from scipy import stats
>>> x = np.arange(20)
>>> stats.tvar(x)
35.0
>>> stats.tvar(x, (3,17))
20.0
*)

val uniform : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Uniform_gen] Np.Obj.t
(**
A uniform continuous random variable.

In the standard form, the distribution is uniform on ``[0, 1]``. Using
the parameters ``loc`` and ``scale``, one obtains the uniform distribution
on ``[loc, loc + scale]``.

As an instance of the `rv_continuous` class, `uniform` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Examples
--------
>>> from scipy.stats import uniform
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = uniform.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(uniform.ppf(0.01),
...                 uniform.ppf(0.99), 100)
>>> ax.plot(x, uniform.pdf(x),
...        'r-', lw=5, alpha=0.6, label='uniform pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = uniform()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = uniform.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], uniform.cdf(vals))
True

Generate random numbers:

>>> r = uniform.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val variation : ?axis:[`I of int | `None] -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the coefficient of variation.

The coefficient of variation is the ratio of the biased standard
deviation to the mean.

Parameters
----------
a : array_like
    Input array.
axis : int or None, optional
    Axis along which to calculate the coefficient of variation. Default
    is 0. If None, compute over the whole array `a`.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan.
    The following options are available (default is 'propagate'):

      * 'propagate': returns nan
      * 'raise': throws an error
      * 'omit': performs the calculations ignoring nan values

Returns
-------
variation : ndarray
    The calculated variation along the requested axis.

References
----------
.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard
   Probability and Statistics Tables and Formulae. Chapman & Hall: New
   York. 2000.

Examples
--------
>>> from scipy.stats import variation
>>> variation([1, 2, 3, 4, 5])
0.47140452079103173
*)

val vonmises : ?loc:float -> ?scale:float -> kappa:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Vonmises_gen] Np.Obj.t
(**
A Von Mises continuous random variable.

As an instance of the `rv_continuous` class, `vonmises` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(kappa, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, kappa, loc=0, scale=1)
    Probability density function.
logpdf(x, kappa, loc=0, scale=1)
    Log of the probability density function.
cdf(x, kappa, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, kappa, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, kappa, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, kappa, loc=0, scale=1)
    Log of the survival function.
ppf(q, kappa, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, kappa, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, kappa, loc=0, scale=1)
    Non-central moment of order n
stats(kappa, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(kappa, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, kappa, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(kappa, loc=0, scale=1)
    Median of the distribution.
mean(kappa, loc=0, scale=1)
    Mean of the distribution.
var(kappa, loc=0, scale=1)
    Variance of the distribution.
std(kappa, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, kappa, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `vonmises` and `vonmises_line` is:

.. math::

    f(x, \kappa) = \frac{ \exp(\kappa \cos(x)) }{ 2 \pi I_0(\kappa) }

for :math:`-\pi \le x \le \pi`, :math:`\kappa > 0`. :math:`I_0` is the
modified Bessel function of order zero (`scipy.special.i0`).

`vonmises` is a circular distribution which does not restrict the
distribution to a fixed interval. Currently, there is no circular
distribution framework in scipy. The ``cdf`` is implemented such that
``cdf(x + 2*np.pi) == cdf(x) + 1``.

`vonmises_line` is the same distribution, defined on :math:`[-\pi, \pi]`
on the real line. This is a regular (i.e. non-circular) distribution.

`vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``vonmises.pdf(x, kappa, loc, scale)`` is identically
equivalent to ``vonmises.pdf(y, kappa) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import vonmises
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> kappa = 3.99
>>> mean, var, skew, kurt = vonmises.stats(kappa, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(vonmises.ppf(0.01, kappa),
...                 vonmises.ppf(0.99, kappa), 100)
>>> ax.plot(x, vonmises.pdf(x, kappa),
...        'r-', lw=5, alpha=0.6, label='vonmises pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = vonmises(kappa)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = vonmises.ppf([0.001, 0.5, 0.999], kappa)
>>> np.allclose([0.001, 0.5, 0.999], vonmises.cdf(vals, kappa))
True

Generate random numbers:

>>> r = vonmises.rvs(kappa, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val vonmises_line : ?loc:float -> ?scale:float -> kappa:Py.Object.t -> unit -> Py.Object.t
(**
A Von Mises continuous random variable.

As an instance of the `rv_continuous` class, `vonmises_line` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(kappa, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, kappa, loc=0, scale=1)
    Probability density function.
logpdf(x, kappa, loc=0, scale=1)
    Log of the probability density function.
cdf(x, kappa, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, kappa, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, kappa, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, kappa, loc=0, scale=1)
    Log of the survival function.
ppf(q, kappa, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, kappa, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, kappa, loc=0, scale=1)
    Non-central moment of order n
stats(kappa, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(kappa, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, kappa, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(kappa, loc=0, scale=1)
    Median of the distribution.
mean(kappa, loc=0, scale=1)
    Mean of the distribution.
var(kappa, loc=0, scale=1)
    Variance of the distribution.
std(kappa, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, kappa, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `vonmises` and `vonmises_line` is:

.. math::

    f(x, \kappa) = \frac{ \exp(\kappa \cos(x)) }{ 2 \pi I_0(\kappa) }

for :math:`-\pi \le x \le \pi`, :math:`\kappa > 0`. :math:`I_0` is the
modified Bessel function of order zero (`scipy.special.i0`).

`vonmises` is a circular distribution which does not restrict the
distribution to a fixed interval. Currently, there is no circular
distribution framework in scipy. The ``cdf`` is implemented such that
``cdf(x + 2*np.pi) == cdf(x) + 1``.

`vonmises_line` is the same distribution, defined on :math:`[-\pi, \pi]`
on the real line. This is a regular (i.e. non-circular) distribution.

`vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``vonmises_line.pdf(x, kappa, loc, scale)`` is identically
equivalent to ``vonmises_line.pdf(y, kappa) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import vonmises_line
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> kappa = 3.99
>>> mean, var, skew, kurt = vonmises_line.stats(kappa, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(vonmises_line.ppf(0.01, kappa),
...                 vonmises_line.ppf(0.99, kappa), 100)
>>> ax.plot(x, vonmises_line.pdf(x, kappa),
...        'r-', lw=5, alpha=0.6, label='vonmises_line pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = vonmises_line(kappa)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = vonmises_line.ppf([0.001, 0.5, 0.999], kappa)
>>> np.allclose([0.001, 0.5, 0.999], vonmises_line.cdf(vals, kappa))
True

Generate random numbers:

>>> r = vonmises_line.rvs(kappa, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val wald : ?loc:float -> ?scale:float -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Wald_gen] Np.Obj.t
(**
A Wald continuous random variable.

As an instance of the `rv_continuous` class, `wald` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, loc=0, scale=1)
    Probability density function.
logpdf(x, loc=0, scale=1)
    Log of the probability density function.
cdf(x, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, loc=0, scale=1)
    Log of the survival function.
ppf(q, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, loc=0, scale=1)
    Non-central moment of order n
stats(loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(loc=0, scale=1)
    Median of the distribution.
mean(loc=0, scale=1)
    Mean of the distribution.
var(loc=0, scale=1)
    Variance of the distribution.
std(loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `wald` is:

.. math::

    f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp(- \frac{ (x-1)^2 }{ 2x })

for :math:`x >= 0`.

`wald` is a special case of `invgauss` with ``mu=1``.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``wald.pdf(x, loc, scale)`` is identically
equivalent to ``wald.pdf(y) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import wald
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:


>>> mean, var, skew, kurt = wald.stats(moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(wald.ppf(0.01),
...                 wald.ppf(0.99), 100)
>>> ax.plot(x, wald.pdf(x),
...        'r-', lw=5, alpha=0.6, label='wald pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = wald()
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = wald.ppf([0.001, 0.5, 0.999])
>>> np.allclose([0.001, 0.5, 0.999], wald.cdf(vals))
True

Generate random numbers:

>>> r = wald.rvs(size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val wasserstein_distance : ?u_weights:Py.Object.t -> ?v_weights:Py.Object.t -> u_values:Py.Object.t -> v_values:Py.Object.t -> unit -> float
(**
Compute the first Wasserstein distance between two 1D distributions.

This distance is also known as the earth mover's distance, since it can be
seen as the minimum amount of 'work' required to transform :math:`u` into
:math:`v`, where 'work' is measured as the amount of distribution weight
that must be moved, multiplied by the distance it has to be moved.

.. versionadded:: 1.0.0

Parameters
----------
u_values, v_values : array_like
    Values observed in the (empirical) distribution.
u_weights, v_weights : array_like, optional
    Weight for each value. If unspecified, each value is assigned the same
    weight.
    `u_weights` (resp. `v_weights`) must have the same length as
    `u_values` (resp. `v_values`). If the weight sum differs from 1, it
    must still be positive and finite so that the weights can be normalized
    to sum to 1.

Returns
-------
distance : float
    The computed distance between the distributions.

Notes
-----
The first Wasserstein distance between the distributions :math:`u` and
:math:`v` is:

.. math::

    l_1 (u, v) = \inf_{\pi \in \Gamma (u, v)} \int_{\mathbb{R} \times
    \mathbb{R}} |x-y| \mathrm{d} \pi (x, y)

where :math:`\Gamma (u, v)` is the set of (probability) distributions on
:math:`\mathbb{R} \times \mathbb{R}` whose marginals are :math:`u` and
:math:`v` on the first and second factors respectively.

If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and
:math:`v`, this distance also equals to:

.. math::

    l_1(u, v) = \int_{-\infty}^{+\infty} |U-V|

See [2]_ for a proof of the equivalence of both definitions.

The input distributions can be empirical, therefore coming from samples
whose values are effectively inputs of the function, or they can be seen as
generalized functions, in which case they are weighted sums of Dirac delta
functions located at the specified values.

References
----------
.. [1] 'Wasserstein metric', https://en.wikipedia.org/wiki/Wasserstein_metric
.. [2] Ramdas, Garcia, Cuturi 'On Wasserstein Two Sample Testing and Related
       Families of Nonparametric Tests' (2015). :arXiv:`1509.02237`.

Examples
--------
>>> from scipy.stats import wasserstein_distance
>>> wasserstein_distance([0, 1, 3], [5, 6, 8])
5.0
>>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])
0.25
>>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],
...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])
4.0781331438047861
*)

val weibull_max : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Weibull_max_gen] Np.Obj.t
(**
Weibull maximum continuous random variable.

As an instance of the `rv_continuous` class, `weibull_max` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_min

Notes
-----
The probability density function for `weibull_max` is:

.. math::

    f(x, c) = c (-x)^{c-1} \exp(-(-x)^c)

for :math:`x < 0`, :math:`c > 0`.

`weibull_max` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``weibull_max.pdf(x, c, loc, scale)`` is identically
equivalent to ``weibull_max.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import weibull_max
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 2.87
>>> mean, var, skew, kurt = weibull_max.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(weibull_max.ppf(0.01, c),
...                 weibull_max.ppf(0.99, c), 100)
>>> ax.plot(x, weibull_max.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='weibull_max pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = weibull_max(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = weibull_max.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], weibull_max.cdf(vals, c))
True

Generate random numbers:

>>> r = weibull_max.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val weibull_min : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Weibull_min_gen] Np.Obj.t
(**
Weibull minimum continuous random variable.

The Weibull Minimum Extreme Value distribution, from extreme value theory,
is also often simply called the Weibull distribution.

As an instance of the `rv_continuous` class, `weibull_min` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

See Also
--------
weibull_max, numpy.random.mtrand.RandomState.weibull, exponweib

Notes
-----
The probability density function for `weibull_min` is:

.. math::

    f(x, c) = c x^{c-1} \exp(-x^c)

for :math:`x >= 0`, :math:`c > 0`.

`weibull_min` takes ``c`` as a shape parameter for :math:`c`.
(named :math:`k` in Wikipedia article and :math:`a` in
``numpy.random.weibull``).  Special shape values are :math:`c=1` and
:math:`c=2` where Weibull distribution reduces to the `expon` and
`rayleigh` distributions respectively.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``weibull_min.pdf(x, c, loc, scale)`` is identically
equivalent to ``weibull_min.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

References
----------
https://en.wikipedia.org/wiki/Weibull_distribution

Examples
--------
>>> from scipy.stats import weibull_min
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 1.79
>>> mean, var, skew, kurt = weibull_min.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(weibull_min.ppf(0.01, c),
...                 weibull_min.ppf(0.99, c), 100)
>>> ax.plot(x, weibull_min.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='weibull_min pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = weibull_min(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = weibull_min.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], weibull_min.cdf(vals, c))
True

Generate random numbers:

>>> r = weibull_min.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val weightedtau : ?rank:[`Bool of bool | `Array_like_of_ints of Py.Object.t] -> ?weigher:Py.Object.t -> ?additive:bool -> x:Py.Object.t -> y:Py.Object.t -> unit -> (float * float)
(**
Compute a weighted version of Kendall's :math:`\tau`.

The weighted :math:`\tau` is a weighted version of Kendall's
:math:`\tau` in which exchanges of high weight are more influential than
exchanges of low weight. The default parameters compute the additive
hyperbolic version of the index, :math:`\tau_\mathrm h`, which has
been shown to provide the best balance between important and
unimportant elements [1]_.

The weighting is defined by means of a rank array, which assigns a
nonnegative rank to each element, and a weigher function, which
assigns a weight based from the rank to each element. The weight of an
exchange is then the sum or the product of the weights of the ranks of
the exchanged elements. The default parameters compute
:math:`\tau_\mathrm h`: an exchange between elements with rank
:math:`r` and :math:`s` (starting from zero) has weight
:math:`1/(r+1) + 1/(s+1)`.

Specifying a rank array is meaningful only if you have in mind an
external criterion of importance. If, as it usually happens, you do
not have in mind a specific rank, the weighted :math:`\tau` is
defined by averaging the values obtained using the decreasing
lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the
behavior with default parameters.

Note that if you are computing the weighted :math:`\tau` on arrays of
ranks, rather than of scores (i.e., a larger value implies a lower
rank) you must negate the ranks, so that elements of higher rank are
associated with a larger value.

Parameters
----------
x, y : array_like
    Arrays of scores, of the same shape. If arrays are not 1-D, they will
    be flattened to 1-D.
rank : array_like of ints or bool, optional
    A nonnegative rank assigned to each element. If it is None, the
    decreasing lexicographical rank by (`x`, `y`) will be used: elements of
    higher rank will be those with larger `x`-values, using `y`-values to
    break ties (in particular, swapping `x` and `y` will give a different
    result). If it is False, the element indices will be used
    directly as ranks. The default is True, in which case this
    function returns the average of the values obtained using the
    decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).
weigher : callable, optional
    The weigher function. Must map nonnegative integers (zero
    representing the most important element) to a nonnegative weight.
    The default, None, provides hyperbolic weighing, that is,
    rank :math:`r` is mapped to weight :math:`1/(r+1)`.
additive : bool, optional
    If True, the weight of an exchange is computed by adding the
    weights of the ranks of the exchanged elements; otherwise, the weights
    are multiplied. The default is True.

Returns
-------
correlation : float
   The weighted :math:`\tau` correlation index.
pvalue : float
   Presently ``np.nan``, as the null statistics is unknown (even in the
   additive hyperbolic case).

See Also
--------
kendalltau : Calculates Kendall's tau.
spearmanr : Calculates a Spearman rank-order correlation coefficient.
theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).

Notes
-----
This function uses an :math:`O(n \log n)`, mergesort-based algorithm
[1]_ that is a weighted extension of Knight's algorithm for Kendall's
:math:`\tau` [2]_. It can compute Shieh's weighted :math:`\tau` [3]_
between rankings without ties (i.e., permutations) by setting
`additive` and `rank` to False, as the definition given in [1]_ is a
generalization of Shieh's.

NaNs are considered the smallest possible score.

.. versionadded:: 0.19.0

References
----------
.. [1] Sebastiano Vigna, 'A weighted correlation index for rankings with
       ties', Proceedings of the 24th international conference on World
       Wide Web, pp. 1166-1176, ACM, 2015.
.. [2] W.R. Knight, 'A Computer Method for Calculating Kendall's Tau with
       Ungrouped Data', Journal of the American Statistical Association,
       Vol. 61, No. 314, Part 1, pp. 436-439, 1966.
.. [3] Grace S. Shieh. 'A weighted Kendall's tau statistic', Statistics &
       Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.

Examples
--------
>>> from scipy import stats
>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> tau, p_value = stats.weightedtau(x, y)
>>> tau
-0.56694968153682723
>>> p_value
nan
>>> tau, p_value = stats.weightedtau(x, y, additive=False)
>>> tau
-0.62205716951801038

NaNs are considered the smallest possible score:

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, np.nan]
>>> tau, _ = stats.weightedtau(x, y)
>>> tau
-0.56694968153682723

This is exactly Kendall's tau:

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)
>>> tau
-0.47140452079103173

>>> x = [12, 2, 1, 12, 2]
>>> y = [1, 4, 7, 1, 0]
>>> stats.weightedtau(x, y, rank=None)
WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)
>>> stats.weightedtau(y, x, rank=None)
WeightedTauResult(correlation=-0.7181341329699028, pvalue=nan)
*)

val wilcoxon : ?y:[>`Ndarray] Np.Obj.t -> ?zero_method:[`Pratt | `Wilcox | `Zsplit] -> ?correction:bool -> ?alternative:[`Two_sided | `Greater | `Less] -> x:[>`Ndarray] Np.Obj.t -> unit -> (float * float)
(**
Calculate the Wilcoxon signed-rank test.

The Wilcoxon signed-rank test tests the null hypothesis that two
related paired samples come from the same distribution. In particular,
it tests whether the distribution of the differences x - y is symmetric
about zero. It is a non-parametric version of the paired T-test.

Parameters
----------
x : array_like
    Either the first set of measurements (in which case `y` is the second
    set of measurements), or the differences between two sets of
    measurements (in which case `y` is not to be specified.)  Must be
    one-dimensional.
y : array_like, optional
    Either the second set of measurements (if `x` is the first set of
    measurements), or not specified (if `x` is the differences between
    two sets of measurements.)  Must be one-dimensional.
zero_method : {'pratt', 'wilcox', 'zsplit'}, optional
    The following options are available (default is 'wilcox'):
 
      * 'pratt': Includes zero-differences in the ranking process,
        but drops the ranks of the zeros, see [4]_, (more conservative).
      * 'wilcox': Discards all zero-differences, the default.
      * 'zsplit': Includes zero-differences in the ranking process and 
        split the zero rank between positive and negative ones.
correction : bool, optional
    If True, apply continuity correction by adjusting the Wilcoxon rank
    statistic by 0.5 towards the mean value when computing the
    z-statistic.  Default is False.
alternative : {'two-sided', 'greater', 'less'}, optional
    The alternative hypothesis to be tested, see Notes. Default is
    'two-sided'.

Returns
-------
statistic : float
    If `alternative` is 'two-sided', the sum of the ranks of the
    differences above or below zero, whichever is smaller.
    Otherwise the sum of the ranks of the differences above zero.
pvalue : float
    The p-value for the test depending on `alternative`.

See Also
--------
kruskal, mannwhitneyu

Notes
-----
The test has been introduced in [4]_. Given n independent samples
(xi, yi) from a bivariate distribution (i.e. paired samples),
it computes the differences di = xi - yi. One assumption of the test
is that the differences are symmetric, see [2]_.
The two-sided test has the null hypothesis that the median of the
differences is zero against the alternative that it is different from
zero. The one-sided test has the null hypothesis that the median is 
positive against the alternative that it is negative 
(``alternative == 'less'``), or vice versa (``alternative == 'greater.'``).

The test uses a normal approximation to derive the p-value (if
``zero_method == 'pratt'``, the approximation is adjusted as in [5]_).
A typical rule is to require that n > 20 ([2]_, p. 383). For smaller n,
exact tables can be used to find critical values.

References
----------
.. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
.. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.
.. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed
   Rank Procedures, Journal of the American Statistical Association,
   Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`
.. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,
   Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`
.. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank
   Sampling Distribution When Zero Differences are Present,
   Journal of the American Statistical Association, Vol. 62, 1967,
   pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`

Examples
--------
In [4]_, the differences in height between cross- and self-fertilized
corn plants is given as follows:

>>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]

Cross-fertilized plants appear to be be higher. To test the null
hypothesis that there is no height difference, we can apply the
two-sided test:

>>> from scipy.stats import wilcoxon
>>> w, p = wilcoxon(d)
>>> w, p
(24.0, 0.04088813291185591)

Hence, we would reject the null hypothesis at a confidence level of 5%,
concluding that there is a difference in height between the groups.
To confirm that the median of the differences can be assumed to be
positive, we use:

>>> w, p = wilcoxon(d, alternative='greater')
>>> w, p
(96.0, 0.020444066455927955)

This shows that the null hypothesis that the median is negative can be
rejected at a confidence level of 5% in favor of the alternative that
the median is greater than zero. The p-value based on the approximation
is within the range of 0.019 and 0.054 given in [2]_.
Note that the statistic changed to 96 in the one-sided case (the sum
of ranks of positive differences) whereas it is 24 in the two-sided
case (the minimum of sum of ranks above and below zero).
*)

val wishart : ?df:int -> ?scale:float -> ?seed:Py.Object.t -> unit -> Py.Object.t
(**
A Wishart random variable.

The `df` keyword specifies the degrees of freedom. The `scale` keyword
specifies the scale matrix, which must be symmetric and positive definite.
In this context, the scale matrix is often interpreted in terms of a
multivariate normal precision matrix (the inverse of the covariance
matrix).

Methods
-------
``pdf(x, df, scale)``
    Probability density function.
``logpdf(x, df, scale)``
    Log of the probability density function.
``rvs(df, scale, size=1, random_state=None)``
    Draw random samples from a Wishart distribution.
``entropy()``
    Compute the differential entropy of the Wishart distribution.

Parameters
----------
x : array_like
    Quantiles, with the last axis of `x` denoting the components.
df : int
    Degrees of freedom, must be greater than or equal to dimension of the
    scale matrix
scale : array_like
    Symmetric positive definite scale matrix of the distribution
random_state : None or int or np.random.RandomState instance, optional
    If int or RandomState, use it for drawing the random variates.
    If None (or np.random), the global np.random state is used.
    Default is None.

Alternatively, the object may be called (as a function) to fix the degrees
of freedom and scale parameters, returning a 'frozen' Wishart random
variable:

rv = wishart(df=1, scale=1)
    - Frozen object with the same methods but holding the given
      degrees of freedom and scale fixed.

See Also
--------
invwishart, chi2

Notes
-----


The scale matrix `scale` must be a symmetric positive definite
matrix. Singular matrices, including the symmetric positive semi-definite
case, are not supported.

The Wishart distribution is often denoted

.. math::

    W_p(\nu, \Sigma)

where :math:`\nu` is the degrees of freedom and :math:`\Sigma` is the
:math:`p \times p` scale matrix.

The probability density function for `wishart` has support over positive
definite matrices :math:`S`; if :math:`S \sim W_p(\nu, \Sigma)`, then
its PDF is given by:

.. math::

    f(S) = \frac{ |S|^{\frac{\nu - p - 1}{2}}}{2^{ \frac{\nu p}{2} }
           |\Sigma|^\frac{\nu}{2} \Gamma_p \left ( \frac{\nu}{2} \right )}
           \exp\left( -tr(\Sigma^{-1} S) / 2 \right)

If :math:`S \sim W_p(\nu, \Sigma)` (Wishart) then
:math:`S^{-1} \sim W_p^{-1}(\nu, \Sigma^{-1})` (inverse Wishart).

If the scale matrix is 1-dimensional and equal to one, then the Wishart
distribution :math:`W_1(\nu, 1)` collapses to the :math:`\chi^2(\nu)`
distribution.

.. versionadded:: 0.16.0

References
----------
.. [1] M.L. Eaton, 'Multivariate Statistics: A Vector Space Approach',
       Wiley, 1983.
.. [2] W.B. Smith and R.R. Hocking, 'Algorithm AS 53: Wishart Variate
       Generator', Applied Statistics, vol. 21, pp. 341-345, 1972.

Examples
--------
>>> import matplotlib.pyplot as plt
>>> from scipy.stats import wishart, chi2
>>> x = np.linspace(1e-5, 8, 100)
>>> w = wishart.pdf(x, df=3, scale=1); w[:5]
array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])
>>> c = chi2.pdf(x, 3); c[:5]
array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])
>>> plt.plot(x, w)

The input quantiles can be any shape of array, as long as the last
axis labels the components.
*)

val wrapcauchy : ?loc:float -> ?scale:float -> c:Py.Object.t -> unit -> [`Object|`Rv_continuous|`Rv_generic|`Wrapcauchy_gen] Np.Obj.t
(**
A wrapped Cauchy continuous random variable.

As an instance of the `rv_continuous` class, `wrapcauchy` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(c, loc=0, scale=1, size=1, random_state=None)
    Random variates.
pdf(x, c, loc=0, scale=1)
    Probability density function.
logpdf(x, c, loc=0, scale=1)
    Log of the probability density function.
cdf(x, c, loc=0, scale=1)
    Cumulative distribution function.
logcdf(x, c, loc=0, scale=1)
    Log of the cumulative distribution function.
sf(x, c, loc=0, scale=1)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(x, c, loc=0, scale=1)
    Log of the survival function.
ppf(q, c, loc=0, scale=1)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, c, loc=0, scale=1)
    Inverse survival function (inverse of ``sf``).
moment(n, c, loc=0, scale=1)
    Non-central moment of order n
stats(c, loc=0, scale=1, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(c, loc=0, scale=1)
    (Differential) entropy of the RV.
fit(data, c, loc=0, scale=1)
    Parameter estimates for generic data.
expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)
    Expected value of a function (of one argument) with respect to the distribution.
median(c, loc=0, scale=1)
    Median of the distribution.
mean(c, loc=0, scale=1)
    Mean of the distribution.
var(c, loc=0, scale=1)
    Variance of the distribution.
std(c, loc=0, scale=1)
    Standard deviation of the distribution.
interval(alpha, c, loc=0, scale=1)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability density function for `wrapcauchy` is:

.. math::

    f(x, c) = \frac{1-c^2}{2\pi (1+c^2 - 2c \cos(x))}

for :math:`0 \le x \le 2\pi`, :math:`0 < c < 1`.

`wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.

The probability density above is defined in the 'standardized' form. To shift
and/or scale the distribution use the ``loc`` and ``scale`` parameters.
Specifically, ``wrapcauchy.pdf(x, c, loc, scale)`` is identically
equivalent to ``wrapcauchy.pdf(y, c) / scale`` with
``y = (x - loc) / scale``.

Examples
--------
>>> from scipy.stats import wrapcauchy
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> c = 0.0311
>>> mean, var, skew, kurt = wrapcauchy.stats(c, moments='mvsk')

Display the probability density function (``pdf``):

>>> x = np.linspace(wrapcauchy.ppf(0.01, c),
...                 wrapcauchy.ppf(0.99, c), 100)
>>> ax.plot(x, wrapcauchy.pdf(x, c),
...        'r-', lw=5, alpha=0.6, label='wrapcauchy pdf')

Alternatively, the distribution object can be called (as a function)
to fix the shape, location and scale parameters. This returns a 'frozen'
RV object holding the given parameters fixed.

Freeze the distribution and display the frozen ``pdf``:

>>> rv = wrapcauchy(c)
>>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')

Check accuracy of ``cdf`` and ``ppf``:

>>> vals = wrapcauchy.ppf([0.001, 0.5, 0.999], c)
>>> np.allclose([0.001, 0.5, 0.999], wrapcauchy.cdf(vals, c))
True

Generate random numbers:

>>> r = wrapcauchy.rvs(c, size=1000)

And compare the histogram:

>>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()
*)

val yeojohnson : ?lmbda:float -> x:[>`Ndarray] Np.Obj.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * float)
(**
Return a dataset transformed by a Yeo-Johnson power transformation.

Parameters
----------
x : ndarray
    Input array.  Should be 1-dimensional.
lmbda : float, optional
    If ``lmbda`` is ``None``, find the lambda that maximizes the
    log-likelihood function and return it as the second output argument.
    Otherwise the transformation is done for the given value.

Returns
-------
yeojohnson: ndarray
    Yeo-Johnson power transformed array.
maxlog : float, optional
    If the `lmbda` parameter is None, the second returned argument is
    the lambda that maximizes the log-likelihood function.

See Also
--------
probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox

Notes
-----
The Yeo-Johnson transform is given by::

    y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0
        log(x + 1),                                  for x >= 0, lmbda = 0
        -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2
        -log(-x + 1),                                for x < 0, lmbda = 2

Unlike `boxcox`, `yeojohnson` does not require the input data to be
positive.

.. versionadded:: 1.2.0


References
----------
I. Yeo and R.A. Johnson, 'A New Family of Power Transformations to
Improve Normality or Symmetry', Biometrika 87.4 (2000):


Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

We generate some random variates from a non-normal distribution and make a
probability plot for it, to show it is non-normal in the tails:

>>> fig = plt.figure()
>>> ax1 = fig.add_subplot(211)
>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)
>>> ax1.set_xlabel('')
>>> ax1.set_title('Probplot against normal distribution')

We now use `yeojohnson` to transform the data so it's closest to normal:

>>> ax2 = fig.add_subplot(212)
>>> xt, lmbda = stats.yeojohnson(x)
>>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)
>>> ax2.set_title('Probplot after Yeo-Johnson transformation')

>>> plt.show()
*)

val yeojohnson_llf : lmb:[`F of float | `I of int | `Bool of bool | `S of string] -> data:[>`Ndarray] Np.Obj.t -> unit -> float
(**
The yeojohnson log-likelihood function.

Parameters
----------
lmb : scalar
    Parameter for Yeo-Johnson transformation. See `yeojohnson` for
    details.
data : array_like
    Data to calculate Yeo-Johnson log-likelihood for. If `data` is
    multi-dimensional, the log-likelihood is calculated along the first
    axis.

Returns
-------
llf : float
    Yeo-Johnson log-likelihood of `data` given `lmb`.

See Also
--------
yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax

Notes
-----
The Yeo-Johnson log-likelihood function is defined here as

.. math::

    llf = N/2 \log(\hat{\sigma}^2) + (\lambda - 1)
          \sum_i \text{ sign }(x_i)\log(|x_i| + 1)

where :math:`\hat{\sigma}^2` is estimated variance of the the Yeo-Johnson
transformed input data ``x``.

.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes
>>> np.random.seed(1245)

Generate some random variates and calculate Yeo-Johnson log-likelihood
values for them for a range of ``lmbda`` values:

>>> x = stats.loggamma.rvs(5, loc=10, size=1000)
>>> lmbdas = np.linspace(-2, 10)
>>> llf = np.zeros(lmbdas.shape, dtype=float)
>>> for ii, lmbda in enumerate(lmbdas):
...     llf[ii] = stats.yeojohnson_llf(lmbda, x)

Also find the optimal lmbda value with `yeojohnson`:

>>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)

Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a
horizontal line to check that that's really the optimum:

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(lmbdas, llf, 'b.-')
>>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')
>>> ax.set_xlabel('lmbda parameter')
>>> ax.set_ylabel('Yeo-Johnson log-likelihood')

Now add some probability plots to show that where the log-likelihood is
maximized the data transformed with `yeojohnson` looks closest to normal:

>>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'
>>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):
...     xt = stats.yeojohnson(x, lmbda=lmbda)
...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)
...     ax_inset = inset_axes(ax, width='20%', height='20%', loc=loc)
...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')
...     ax_inset.set_xticklabels([])
...     ax_inset.set_yticklabels([])
...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda)

>>> plt.show()
*)

val yeojohnson_normmax : ?brack:Py.Object.t -> x:[>`Ndarray] Np.Obj.t -> unit -> float
(**
Compute optimal Yeo-Johnson transform parameter.

Compute optimal Yeo-Johnson transform parameter for input data, using
maximum likelihood estimation.

Parameters
----------
x : array_like
    Input array.
brack : 2-tuple, optional
    The starting interval for a downhill bracket search with
    `optimize.brent`. Note that this is in most cases not critical; the
    final result is allowed to be outside this bracket.

Returns
-------
maxlog : float
    The optimal transform parameter found.

See Also
--------
yeojohnson, yeojohnson_llf, yeojohnson_normplot

Notes
-----
.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt
>>> np.random.seed(1234)  # make this example reproducible

Generate some data and determine optimal ``lmbda``

>>> x = stats.loggamma.rvs(5, size=30) + 5
>>> lmax = stats.yeojohnson_normmax(x)

>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)
>>> ax.axvline(lmax, color='r')

>>> plt.show()
*)

val yeojohnson_normplot : ?plot:Py.Object.t -> ?n:int -> x:[>`Ndarray] Np.Obj.t -> la:Py.Object.t -> lb:Py.Object.t -> unit -> ([`ArrayLike|`Ndarray|`Object] Np.Obj.t * [`ArrayLike|`Ndarray|`Object] Np.Obj.t)
(**
Compute parameters for a Yeo-Johnson normality plot, optionally show it.

A Yeo-Johnson normality plot shows graphically what the best
transformation parameter is to use in `yeojohnson` to obtain a
distribution that is close to normal.

Parameters
----------
x : array_like
    Input array.
la, lb : scalar
    The lower and upper bounds for the ``lmbda`` values to pass to
    `yeojohnson` for Yeo-Johnson transformations. These are also the
    limits of the horizontal axis of the plot if that is generated.
plot : object, optional
    If given, plots the quantiles and least squares fit.
    `plot` is an object that has to have methods 'plot' and 'text'.
    The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,
    or a custom object with the same methods.
    Default is None, which means that no plot is created.
N : int, optional
    Number of points on the horizontal axis (equally distributed from
    `la` to `lb`).

Returns
-------
lmbdas : ndarray
    The ``lmbda`` values for which a Yeo-Johnson transform was done.
ppcc : ndarray
    Probability Plot Correlelation Coefficient, as obtained from `probplot`
    when fitting the Box-Cox transformed input `x` against a normal
    distribution.

See Also
--------
probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max

Notes
-----
Even if `plot` is given, the figure is not shown or saved by
`boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``
should be used after calling `probplot`.

.. versionadded:: 1.2.0

Examples
--------
>>> from scipy import stats
>>> import matplotlib.pyplot as plt

Generate some non-normally distributed data, and create a Yeo-Johnson plot:

>>> x = stats.loggamma.rvs(5, size=500) + 5
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)

Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in
the same plot:

>>> _, maxlog = stats.yeojohnson(x)
>>> ax.axvline(maxlog, color='r')

>>> plt.show()
*)

val yulesimon : ?loc:float -> alpha:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Yulesimon_gen] Np.Obj.t
(**
A Yule-Simon discrete random variable.

As an instance of the `rv_discrete` class, `yulesimon` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(alpha, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, alpha, loc=0)
    Probability mass function.
logpmf(k, alpha, loc=0)
    Log of the probability mass function.
cdf(k, alpha, loc=0)
    Cumulative distribution function.
logcdf(k, alpha, loc=0)
    Log of the cumulative distribution function.
sf(k, alpha, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, alpha, loc=0)
    Log of the survival function.
ppf(q, alpha, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, alpha, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(alpha, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(alpha, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(alpha,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(alpha, loc=0)
    Median of the distribution.
mean(alpha, loc=0)
    Mean of the distribution.
var(alpha, loc=0)
    Variance of the distribution.
std(alpha, loc=0)
    Standard deviation of the distribution.
interval(alpha, alpha, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----

The probability mass function for the `yulesimon` is:

.. math::

    f(k) =  \alpha B(k, \alpha+1)

for :math:`k=1,2,3,...`, where :math:`\alpha>0`.
Here :math:`B` refers to the `scipy.special.beta` function.

The sampling of random variates is based on pg 553, Section 6.3 of [1]_.
Our notation maps to the referenced logic via :math:`\alpha=a-1`.

For details see the wikipedia entry [2]_.

References
----------
.. [1] Devroye, Luc. 'Non-uniform Random Variate Generation',
     (1986) Springer, New York.

.. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``yulesimon.pmf(k, alpha, loc)`` is identically
equivalent to ``yulesimon.pmf(k - loc, alpha)``.

Examples
--------
>>> from scipy.stats import yulesimon
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> alpha = 11
>>> mean, var, skew, kurt = yulesimon.stats(alpha, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(yulesimon.ppf(0.01, alpha),
...               yulesimon.ppf(0.99, alpha))
>>> ax.plot(x, yulesimon.pmf(x, alpha), 'bo', ms=8, label='yulesimon pmf')
>>> ax.vlines(x, 0, yulesimon.pmf(x, alpha), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = yulesimon(alpha)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = yulesimon.cdf(x, alpha)
>>> np.allclose(x, yulesimon.ppf(prob, alpha))
True

Generate random numbers:

>>> r = yulesimon.rvs(alpha, size=1000)
*)

val zipf : ?loc:float -> a:Py.Object.t -> unit -> [`Object|`Rv_discrete|`Rv_generic|`Zipf_gen] Np.Obj.t
(**
A Zipf discrete random variable.

As an instance of the `rv_discrete` class, `zipf` object inherits from it
a collection of generic methods (see below for the full list),
and completes them with details specific for this particular distribution.

Methods
-------
rvs(a, loc=0, size=1, random_state=None)
    Random variates.
pmf(k, a, loc=0)
    Probability mass function.
logpmf(k, a, loc=0)
    Log of the probability mass function.
cdf(k, a, loc=0)
    Cumulative distribution function.
logcdf(k, a, loc=0)
    Log of the cumulative distribution function.
sf(k, a, loc=0)
    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).
logsf(k, a, loc=0)
    Log of the survival function.
ppf(q, a, loc=0)
    Percent point function (inverse of ``cdf`` --- percentiles).
isf(q, a, loc=0)
    Inverse survival function (inverse of ``sf``).
stats(a, loc=0, moments='mv')
    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').
entropy(a, loc=0)
    (Differential) entropy of the RV.
expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)
    Expected value of a function (of one argument) with respect to the distribution.
median(a, loc=0)
    Median of the distribution.
mean(a, loc=0)
    Mean of the distribution.
var(a, loc=0)
    Variance of the distribution.
std(a, loc=0)
    Standard deviation of the distribution.
interval(alpha, a, loc=0)
    Endpoints of the range that contains alpha percent of the distribution

Notes
-----
The probability mass function for `zipf` is:

.. math::

    f(k, a) = \frac{1}{\zeta(a) k^a}

for :math:`k \ge 1`.

`zipf` takes :math:`a` as shape parameter. :math:`\zeta` is the
Riemann zeta function (`scipy.special.zeta`)

The probability mass function above is defined in the 'standardized' form.
To shift distribution use the ``loc`` parameter.
Specifically, ``zipf.pmf(k, a, loc)`` is identically
equivalent to ``zipf.pmf(k - loc, a)``.

Examples
--------
>>> from scipy.stats import zipf
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(1, 1)

Calculate a few first moments:

>>> a = 6.5
>>> mean, var, skew, kurt = zipf.stats(a, moments='mvsk')

Display the probability mass function (``pmf``):

>>> x = np.arange(zipf.ppf(0.01, a),
...               zipf.ppf(0.99, a))
>>> ax.plot(x, zipf.pmf(x, a), 'bo', ms=8, label='zipf pmf')
>>> ax.vlines(x, 0, zipf.pmf(x, a), colors='b', lw=5, alpha=0.5)

Alternatively, the distribution object can be called (as a function)
to fix the shape and location. This returns a 'frozen' RV object holding
the given parameters fixed.

Freeze the distribution and display the frozen ``pmf``:

>>> rv = zipf(a)
>>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,
...         label='frozen pmf')
>>> ax.legend(loc='best', frameon=False)
>>> plt.show()

Check accuracy of ``cdf`` and ``ppf``:

>>> prob = zipf.cdf(x, a)
>>> np.allclose(x, zipf.ppf(prob, a))
True

Generate random numbers:

>>> r = zipf.rvs(a, size=1000)
*)

val zmap : ?axis:[`I of int | `None] -> ?ddof:int -> scores:[>`Ndarray] Np.Obj.t -> compare:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Calculate the relative z-scores.

Return an array of z-scores, i.e., scores that are standardized to
zero mean and unit variance, where mean and variance are calculated
from the comparison array.

Parameters
----------
scores : array_like
    The input for which z-scores are calculated.
compare : array_like
    The input from which the mean and standard deviation of the
    normalization are taken; assumed to have the same dimension as
    `scores`.
axis : int or None, optional
    Axis over which mean and variance of `compare` are calculated.
    Default is 0. If None, compute over the whole array `scores`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.

Returns
-------
zscore : array_like
    Z-scores, in the same shape as `scores`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> from scipy.stats import zmap
>>> a = [0.5, 2.0, 2.5, 3]
>>> b = [0, 1, 2, 3, 4]
>>> zmap(a, b)
array([-1.06066017,  0.        ,  0.35355339,  0.70710678])
*)

val zscore : ?axis:[`I of int | `None] -> ?ddof:int -> ?nan_policy:[`Propagate | `Raise | `Omit] -> a:[>`Ndarray] Np.Obj.t -> unit -> [`ArrayLike|`Ndarray|`Object] Np.Obj.t
(**
Compute the z score.

Compute the z score of each value in the sample, relative to the
sample mean and standard deviation.

Parameters
----------
a : array_like
    An array like object containing the sample data.
axis : int or None, optional
    Axis along which to operate. Default is 0. If None, compute over
    the whole array `a`.
ddof : int, optional
    Degrees of freedom correction in the calculation of the
    standard deviation. Default is 0.
nan_policy : {'propagate', 'raise', 'omit'}, optional
    Defines how to handle when input contains nan. 'propagate' returns nan,
    'raise' throws an error, 'omit' performs the calculations ignoring nan
    values. Default is 'propagate'.

Returns
-------
zscore : array_like
    The z-scores, standardized by mean and standard deviation of
    input array `a`.

Notes
-----
This function preserves ndarray subclasses, and works also with
matrices and masked arrays (it uses `asanyarray` instead of
`asarray` for parameters).

Examples
--------
>>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,
...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])
>>> from scipy import stats
>>> stats.zscore(a)
array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,
        0.6748, -1.1488, -1.3324])

Computing along a specified axis, using n-1 degrees of freedom
(``ddof=1``) to calculate the standard deviation:

>>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],
...               [ 0.7149,  0.0775,  0.6072,  0.9656],
...               [ 0.6341,  0.1403,  0.9759,  0.4064],
...               [ 0.5918,  0.6948,  0.904 ,  0.3721],
...               [ 0.0921,  0.2481,  0.1188,  0.1366]])
>>> stats.zscore(b, axis=1, ddof=1)
array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],
       [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],
       [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],
       [-0.22095197,  0.24468594,  1.19042819, -1.21416216],
       [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])
*)

